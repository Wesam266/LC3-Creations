{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import nltk\n",
    "import time\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import urllib2\n",
    "import unicodedata\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tqdm\n",
    "\n",
    "wordnet_tags = ['n', 'v', 'a', 's', 'r']\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from http://brandonrose.org/clustering\n",
    "\n",
    "\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmer = WordNetLemmatizer()\n",
    "wordnet_tags = ['n', 'v', 'a', 's', 'r']\n",
    "\n",
    "def tokenize_and_stem_n_lem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    ''' \n",
    "    Old code gave seperate lists for lem and stem; I lemmed, then just stemmed the result\n",
    "    #stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    #lems = [lemmer.lemmatize(l) for l in filtered_tokens]\n",
    "    '''\n",
    "    stems = [stemmer.stem(lemmer.lemmatize(l)) for l in filtered_tokens]\n",
    "    \n",
    "    #return stems,lems\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#strip any proper names from a text...unfortunately right now this is yanking the first word from a sentence too.\n",
    "import string\n",
    "def strip_proppers(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word.islower()]\n",
    "    return \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#strip any proper nouns (NNP) or plural proper nouns (NNPS) from a text\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def strip_proppers_POS(text):\n",
    "    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n",
    "    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
    "    return non_propernouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path        = os.path.abspath(os.getcwd())\n",
    "TESTDIR     = os.path.normpath(os.path.join(os.path.expanduser(\"~\"),\"projects\",\"LC3-Creations\", \"examples\",\"KDDsample\"))\n",
    "\n",
    "corpus = {}\n",
    "%time\n",
    "start_time = time.time()\n",
    "for dirName, subdirList, fileList in walk(TESTDIR):    \n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            a = unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore')\n",
    "            document = unicodedata.normalize('NFKD', a).encode('ascii','ignore')\n",
    "\n",
    "            if len(document)<300:\n",
    "                pass\n",
    "            else:\n",
    "\n",
    "                # The entire document\n",
    "                body = re.sub('[\\s]',\" \",document)\n",
    "\n",
    "                # Getting title\n",
    "                title = re.findall(\"^[^\\\\n\\\\n]+\",document)[0]\n",
    "\n",
    "                # Getting the abstract\n",
    "                try:\n",
    "                    abstract = re.findall (r'(Abstract|ABSTRACT)([^]]*)\\n',document[:2000])\n",
    "                except IndexError:\n",
    "                    abstract = re.findall (r'(Abstract|ABSTRACT)([^]]*)\\n',document[:2000])[0]\n",
    "                else:\n",
    "                    abstract = abstract = re.findall (r'(Abstract|ABSTRACT)([^]]*)\\n',document[:2000])[0][1]\n",
    "\n",
    "                if isinstance(abstract, tuple):\n",
    "                    abstract = re.sub('[\\s]',\" \",abstract[1])\n",
    "                elif isinstance(abstract,list):\n",
    "                    abstract = re.sub('[\\s]',\" \",abstract[1])\n",
    "                elif isinstance(abstract,str):\n",
    "                    abstract = re.sub('[\\s]',\" \", abstract)\n",
    "\n",
    "\n",
    "                # Extracts section with names and email addresses only\n",
    "                section  = re.findall (r'\\n\\n([^]]*)\\n\\n(Abstract|ABSTRACT)',document[:2000])\n",
    "\n",
    "                type(section[0][0])\n",
    "\n",
    "                if isinstance(section, list):\n",
    "                    section = re.sub('[\\s]',\" \",section[0][0])\n",
    "                else:\n",
    "                    section = re.sub('[\\s]',\" \",section)\n",
    "\n",
    "\n",
    "                # Code to extract entities from top section of pdf and store a relationship tree\n",
    "                tagged = nltk.pos_tag(nltk.word_tokenize(section))\n",
    "                entities = nltk.chunk.ne_chunk(tagged)\n",
    "\n",
    "                # Another entity extractor\n",
    "                st = StanfordNERTagger('/Users/linwood/stanford-corenlp-full-2015-04-20/classifiers/english.conll.4class.distsim.crf.ser.gz',\n",
    "                       '/Users/linwood/stanford-corenlp-full-2015-04-20/stanford-corenlp-3.5.2.jar',\n",
    "                       encoding='utf-8')\n",
    "                tokenized_text = word_tokenize(section)\n",
    "                stanentities = st.tag(tokenized_text)\n",
    "\n",
    "                # Calls function to lemmatize and stem the document; stores the result\n",
    "                tokenize_and_stem_n_lem(abstract);\n",
    "\n",
    "                '''\n",
    "                This gives seperate lists for lem and stem; replacement code stores combin\n",
    "                # Creates the json document format to store the files\n",
    "                corpus[str(fileName)]={}\n",
    "                corpus[str(fileName)]={'Title':title,'Abstract':abstract,'Entities':entities, \n",
    "                                       \"Stanford ER\":stanentities, \"Stems\": tokenize_and_stem_n_lem(abstract)[0], \n",
    "                                      \"Lems\": tokenize_and_stem_n_lem(abstract)[1]}'''\n",
    "\n",
    "                # Creates the json document format to store the files\n",
    "                corpus[str(fileName)]={}\n",
    "                corpus[str(fileName)]={'Title':title,'body':body,'Abstract':abstract,'Entities':entities, \n",
    "                                       \"Stanford ER\":stanentities, \"Stems\": tokenize_and_stem_n_lem(abstract)}\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1446428576.5'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
