{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import urllib2\n",
    "import unicodedata\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "wordnet_tags = ['n', 'v', 'a', 's', 'r']\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main code, built from testing, is in the cell immediately below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from http://brandonrose.org/clustering\n",
    "\n",
    "\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmer = WordNetLemmatizer()\n",
    "wordnet_tags = ['n', 'v', 'a', 's', 'r']\n",
    "\n",
    "def tokenize_and_stem_n_lem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    ''' \n",
    "    Old code gave seperate lists for lem and stem; I lemmed, then just stemmed the result\n",
    "    #stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    #lems = [lemmer.lemmatize(l) for l in filtered_tokens]\n",
    "    '''\n",
    "    stems = [stemmer.stem(lemmer.lemmatize(l)) for l in filtered_tokens]\n",
    "    \n",
    "    #return stems,lems\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['pdf2txt.py', '/Users/linwood/projects/LC3-Creations/examples/KDDsample/p1.pdf']' returned non-zero exit status 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-360-4b3c8ebe85ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfileName\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfileName\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfileName\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pdf2txt.py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTESTDIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NFKD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['pdf2txt.py', '/Users/linwood/projects/LC3-Creations/examples/KDDsample/p1.pdf']' returned non-zero exit status 1"
     ]
    }
   ],
   "source": [
    "path        = os.path.abspath(os.getcwd())\n",
    "TESTDIR     = os.path.normpath(os.path.join(os.path.expanduser(\"~\"),\"projects\",\"LC3-Creations\", \"examples\",\"KDDsample\"))\n",
    "\n",
    "corpus = {}\n",
    "\n",
    "start_time = time.time()\n",
    "for dirName, subdirList, fileList in walk(TESTDIR):\n",
    "        for fileName in fileList:\n",
    "            if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "                a = unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore')\n",
    "                document = unicodedata.normalize('NFKD', a).encode('ascii','ignore')\n",
    "                \n",
    "                if len(document)<300:\n",
    "                    pass\n",
    "                else:\n",
    "                    \n",
    "                    # The entire document\n",
    "                    body = re.sub('[\\s]',\" \",document)\n",
    "                    \n",
    "                    # Getting title\n",
    "                    title = re.findall(\"^[^\\\\n\\\\n]+\",document)[0]\n",
    "\n",
    "                    # Getting the abstract\n",
    "                    try:\n",
    "                        abstract = re.findall (r'(Abstract|ABSTRACT)([^]]*)\\n',document[:2000])\n",
    "                    except IndexError:\n",
    "                        abstract = re.findall (r'(Abstract|ABSTRACT)([^]]*)\\n',document[:2000])[0]\n",
    "                    else:\n",
    "                        abstract = abstract = re.findall (r'(Abstract|ABSTRACT)([^]]*)\\n',document[:2000])[0][1]\n",
    "                    \n",
    "                    if isinstance(abstract, tuple):\n",
    "                        abstract = re.sub('[\\s]',\" \",abstract[1])\n",
    "                    elif isinstance(abstract,list):\n",
    "                        abstract = re.sub('[\\s]',\" \",abstract[1])\n",
    "                    elif isinstance(abstract,str):\n",
    "                        abstract = re.sub('[\\s]',\" \", abstract)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # Extracts section with names and email addresses only\n",
    "                    section  = re.findall (r'\\n\\n([^]]*)\\n\\n(Abstract|ABSTRACT)',document[:2000])\n",
    "\n",
    "                    type(section[0][0])\n",
    "\n",
    "                    if isinstance(section, list):\n",
    "                        section = re.sub('[\\s]',\" \",section[0][0])\n",
    "                    else:\n",
    "                        section = re.sub('[\\s]',\" \",section)\n",
    "\n",
    "\n",
    "                    # Code to extract entities from top section of pdf and store a relationship tree\n",
    "                    tagged = nltk.pos_tag(nltk.word_tokenize(section))\n",
    "                    entities = nltk.chunk.ne_chunk(tagged)\n",
    "\n",
    "                    # Another entity extractor\n",
    "                    st = StanfordNERTagger('/Users/linwood/stanford-corenlp-full-2015-04-20/classifiers/english.conll.4class.distsim.crf.ser.gz',\n",
    "                           '/Users/linwood/stanford-corenlp-full-2015-04-20/stanford-corenlp-3.5.2.jar',\n",
    "                           encoding='utf-8')\n",
    "                    tokenized_text = word_tokenize(section)\n",
    "                    stanentities = st.tag(tokenized_text)\n",
    "\n",
    "                    # Calls function to lemmatize and stem the document; stores the result\n",
    "                    tokenize_and_stem_n_lem(abstract);\n",
    "\n",
    "                    '''\n",
    "                    This gives seperate lists for lem and stem; replacement code stores combin\n",
    "                    # Creates the json document format to store the files\n",
    "                    corpus[str(fileName)]={}\n",
    "                    corpus[str(fileName)]={'Title':title,'Abstract':abstract,'Entities':entities, \n",
    "                                           \"Stanford ER\":stanentities, \"Stems\": tokenize_and_stem_n_lem(abstract)[0], \n",
    "                                          \"Lems\": tokenize_and_stem_n_lem(abstract)[1]}'''\n",
    "\n",
    "                    # Creates the json document format to store the files\n",
    "                    corpus[str(fileName)]={}\n",
    "                    corpus[str(fileName)]={'Title':title,'body'=body,'Abstract':abstract,'Entities':entities,\n",
    "                                           \"Stanford ER\":stanentities, \"Stems\": tokenize_and_stem_n_lem(abstract)}\n",
    "                    print \".\","
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder to see the output of the code above; make sure it does what I want; this will be converted and stored as json document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the semicolon to see the output\n",
    "len(corpus.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing/building individual compents happens below; then I paste it above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[i['Abstract'] for i in corpus.values()[:]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44707\n"
     ]
    }
   ],
   "source": [
    "f = unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(os.path.expanduser(\"~\"),\"projects\",\"LC3-Creations\", \"examples\",\"KDDsample\",\"p627.pdf\")))]),errors='ignore')\n",
    "fd = unicodedata.normalize('NFKD', f).encode('ascii','ignore')\n",
    "print len(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reducing',\n",
       " 'the',\n",
       " 'Unlabeled',\n",
       " 'Sample',\n",
       " 'Complexity',\n",
       " 'of',\n",
       " 'Semi',\n",
       " 'Supervised',\n",
       " 'Multi',\n",
       " 'view',\n",
       " 'Learning',\n",
       " 'Chao',\n",
       " 'Lan',\n",
       " 'clan',\n",
       " 'ittc',\n",
       " 'ku',\n",
       " 'edu',\n",
       " 'Jun',\n",
       " 'Huan',\n",
       " 'jhuan',\n",
       " 'ittc',\n",
       " 'ku',\n",
       " 'edu',\n",
       " 'School',\n",
       " 'of',\n",
       " 'Engineering',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Kansas',\n",
       " 'Lawrence',\n",
       " 'KS',\n",
       " '66047',\n",
       " 'USA',\n",
       " 'ABSTRACT',\n",
       " 'In',\n",
       " 'semi',\n",
       " 'supervised',\n",
       " 'multi',\n",
       " 'view',\n",
       " 'learning',\n",
       " 'unlabeled',\n",
       " 'sample',\n",
       " 'com',\n",
       " 'plexity',\n",
       " 'u',\n",
       " 's',\n",
       " 'c',\n",
       " 'species',\n",
       " 'the',\n",
       " 'size',\n",
       " 'of',\n",
       " 'unlabeled',\n",
       " 'training',\n",
       " 'sample',\n",
       " 'that',\n",
       " 'guarantees',\n",
       " 'a',\n",
       " 'desired',\n",
       " 'learning',\n",
       " 'error',\n",
       " 'In',\n",
       " 'this',\n",
       " 'paper',\n",
       " 'we',\n",
       " 'improve',\n",
       " 'the',\n",
       " 'state',\n",
       " 'of',\n",
       " 'art',\n",
       " 'u',\n",
       " 's',\n",
       " 'c',\n",
       " 'from',\n",
       " 'O',\n",
       " '1',\n",
       " 'to',\n",
       " 'O',\n",
       " 'log',\n",
       " '1',\n",
       " 'for',\n",
       " 'small',\n",
       " 'error',\n",
       " 'under',\n",
       " 'mild',\n",
       " 'conditions',\n",
       " 'To',\n",
       " 'obtain',\n",
       " 'the',\n",
       " 'improved',\n",
       " 'result',\n",
       " 'as',\n",
       " 'a',\n",
       " 'primary',\n",
       " 'step',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'a',\n",
       " 'connection',\n",
       " 'between',\n",
       " 'the',\n",
       " 'generalization',\n",
       " 'error',\n",
       " 'of',\n",
       " 'a',\n",
       " 'classier',\n",
       " 'and',\n",
       " 'its',\n",
       " 'incompatibility',\n",
       " 'which',\n",
       " 'measures',\n",
       " 'the',\n",
       " 'tness',\n",
       " 'between',\n",
       " 'the',\n",
       " 'classier',\n",
       " 'and',\n",
       " 'the',\n",
       " 'sample',\n",
       " 'distribution',\n",
       " 'We',\n",
       " 'then',\n",
       " 'prove',\n",
       " 'that',\n",
       " 'with',\n",
       " 'a',\n",
       " 'suciently',\n",
       " 'large',\n",
       " 'unlabeled',\n",
       " 'sample',\n",
       " 'one',\n",
       " 'is',\n",
       " 'able',\n",
       " 'to',\n",
       " 'nd',\n",
       " 'classiers',\n",
       " 'with',\n",
       " 'low',\n",
       " 'incompatibility',\n",
       " 'Combining',\n",
       " 'the',\n",
       " 'two',\n",
       " 'observations',\n",
       " 'we',\n",
       " 'man',\n",
       " 'age',\n",
       " 'to',\n",
       " 'prove',\n",
       " 'a',\n",
       " 'probably',\n",
       " 'approximately',\n",
       " 'correct',\n",
       " 'PAC',\n",
       " 'style',\n",
       " 'learning',\n",
       " 'bound',\n",
       " 'for',\n",
       " 'semi',\n",
       " 'supervised',\n",
       " 'multi',\n",
       " 'view',\n",
       " 'learning',\n",
       " 'We',\n",
       " 'empirically',\n",
       " 'veried',\n",
       " 'our',\n",
       " 'theory',\n",
       " 'by',\n",
       " 'designing',\n",
       " 'two',\n",
       " 'proof',\n",
       " 'of',\n",
       " 'concept',\n",
       " 'algorithms',\n",
       " 'one',\n",
       " 'based',\n",
       " 'on',\n",
       " 'active',\n",
       " 'view',\n",
       " 'sensing',\n",
       " 'and',\n",
       " 'the',\n",
       " 'other',\n",
       " 'based',\n",
       " 'on',\n",
       " 'online',\n",
       " 'co',\n",
       " 'regularization',\n",
       " 'with',\n",
       " 'real',\n",
       " 'world',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'Keywords',\n",
       " 'Sample',\n",
       " 'Complexity',\n",
       " 'Multi',\n",
       " 'View',\n",
       " 'Learning',\n",
       " 'Semi',\n",
       " 'Supervised',\n",
       " 'Learning',\n",
       " '1',\n",
       " 'INTRODUCTION',\n",
       " 'In',\n",
       " 'semi',\n",
       " 'supervised',\n",
       " 'learning',\n",
       " '30',\n",
       " 'a',\n",
       " 'classier',\n",
       " 'is',\n",
       " 'trained',\n",
       " 'with',\n",
       " 'both',\n",
       " 'a',\n",
       " 'labeled',\n",
       " 'sample',\n",
       " 'and',\n",
       " 'an',\n",
       " 'unlabeled',\n",
       " 'sample',\n",
       " 'where',\n",
       " 'each',\n",
       " 'sample',\n",
       " 'is',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'data',\n",
       " 'These',\n",
       " 'two',\n",
       " 'types',\n",
       " 'of',\n",
       " 'samples',\n",
       " 'dier',\n",
       " 'in',\n",
       " 'whether',\n",
       " 'the',\n",
       " 'true',\n",
       " 'label',\n",
       " 'set',\n",
       " 'is',\n",
       " 'known',\n",
       " 'during',\n",
       " 'training',\n",
       " 'for',\n",
       " 'la',\n",
       " 'beled',\n",
       " 'sample',\n",
       " 'the',\n",
       " 'set',\n",
       " 'is',\n",
       " 'known',\n",
       " 'whereas',\n",
       " 'for',\n",
       " 'unlabeled',\n",
       " 'sample',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'Sample',\n",
       " 'complexity',\n",
       " '5',\n",
       " 'is',\n",
       " 'a',\n",
       " 'common',\n",
       " 'measurement',\n",
       " 'on',\n",
       " 'the',\n",
       " 'eciency',\n",
       " 'of',\n",
       " 'training',\n",
       " 'samples',\n",
       " 'It',\n",
       " 'species',\n",
       " 'the',\n",
       " 'size',\n",
       " 'of',\n",
       " 'a',\n",
       " 'training',\n",
       " 'sample',\n",
       " 'that',\n",
       " 'guarantees',\n",
       " 'a',\n",
       " 'desired',\n",
       " 'learning',\n",
       " 'error',\n",
       " 'In',\n",
       " 'this',\n",
       " 'paper',\n",
       " 'such',\n",
       " 'complexities',\n",
       " 'of',\n",
       " 'labeled',\n",
       " 'and',\n",
       " 'unlabeled',\n",
       " 'samples',\n",
       " 'are',\n",
       " 'termed',\n",
       " 'as',\n",
       " 'labeled',\n",
       " 'sample',\n",
       " 'complexity',\n",
       " 'cid',\n",
       " '96',\n",
       " 's',\n",
       " 'c',\n",
       " '1',\n",
       " 'and',\n",
       " 'unlabeled',\n",
       " 'sample',\n",
       " 'complexity',\n",
       " 'u',\n",
       " 's',\n",
       " 'c',\n",
       " 'respectively',\n",
       " '1In',\n",
       " 'active',\n",
       " 'learning',\n",
       " 'cid',\n",
       " '96',\n",
       " 's',\n",
       " 'c',\n",
       " 'is',\n",
       " 'also',\n",
       " 'called',\n",
       " 'label',\n",
       " 'complexity',\n",
       " '19',\n",
       " 'or',\n",
       " 'abbreviated',\n",
       " 'as',\n",
       " 'sample',\n",
       " 'complexity',\n",
       " '7',\n",
       " 'Permission',\n",
       " 'to',\n",
       " 'make',\n",
       " 'digital',\n",
       " 'or',\n",
       " 'hard',\n",
       " 'copies',\n",
       " 'of',\n",
       " 'all',\n",
       " 'or',\n",
       " 'part',\n",
       " 'of',\n",
       " 'this',\n",
       " 'work',\n",
       " 'for',\n",
       " 'personal',\n",
       " 'or',\n",
       " 'classroom',\n",
       " 'use',\n",
       " 'is',\n",
       " 'granted',\n",
       " 'without',\n",
       " 'fee',\n",
       " 'provided',\n",
       " 'that',\n",
       " 'copies',\n",
       " 'are',\n",
       " 'not',\n",
       " 'made',\n",
       " 'or',\n",
       " 'distributed',\n",
       " 'for',\n",
       " 'prot',\n",
       " 'or',\n",
       " 'commercial',\n",
       " 'advantage',\n",
       " 'and',\n",
       " 'that',\n",
       " 'copies',\n",
       " 'bear',\n",
       " 'this',\n",
       " 'notice',\n",
       " 'and',\n",
       " 'the',\n",
       " 'full',\n",
       " 'cita',\n",
       " 'tion',\n",
       " 'on',\n",
       " 'the',\n",
       " 'rst',\n",
       " 'page',\n",
       " 'Copyrights',\n",
       " 'for',\n",
       " 'components',\n",
       " 'of',\n",
       " 'this',\n",
       " 'work',\n",
       " 'owned',\n",
       " 'by',\n",
       " 'others',\n",
       " 'than',\n",
       " 'ACM',\n",
       " 'must',\n",
       " 'be',\n",
       " 'honored',\n",
       " 'Abstracting',\n",
       " 'with',\n",
       " 'credit',\n",
       " 'is',\n",
       " 'permitted',\n",
       " 'To',\n",
       " 'copy',\n",
       " 'otherwise',\n",
       " 'or',\n",
       " 're',\n",
       " 'publish',\n",
       " 'to',\n",
       " 'post',\n",
       " 'on',\n",
       " 'servers',\n",
       " 'or',\n",
       " 'to',\n",
       " 'redistribute',\n",
       " 'to',\n",
       " 'lists',\n",
       " 'requires',\n",
       " 'prior',\n",
       " 'specic',\n",
       " 'permission',\n",
       " 'and',\n",
       " 'or',\n",
       " 'a',\n",
       " 'fee',\n",
       " 'Request',\n",
       " 'permissions',\n",
       " 'from',\n",
       " 'Permissions',\n",
       " 'acm',\n",
       " 'org',\n",
       " 'KDD15',\n",
       " 'August',\n",
       " '10',\n",
       " '13',\n",
       " '2015',\n",
       " 'Sydney',\n",
       " 'NSW',\n",
       " 'Australia',\n",
       " 'c',\n",
       " 'cid',\n",
       " '13',\n",
       " '2015',\n",
       " 'ACM',\n",
       " 'ISBN',\n",
       " '978',\n",
       " '1',\n",
       " '4503',\n",
       " '3664',\n",
       " '2',\n",
       " '15',\n",
       " '08',\n",
       " '15',\n",
       " '00',\n",
       " 'DOI',\n",
       " 'http',\n",
       " 'dx',\n",
       " 'doi',\n",
       " 'org',\n",
       " '10',\n",
       " '1145',\n",
       " '2783258',\n",
       " '2783409',\n",
       " 'Semi',\n",
       " 'supervised',\n",
       " 'multi',\n",
       " 'view',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'a',\n",
       " 'popular',\n",
       " 'learning',\n",
       " 'scheme',\n",
       " 'when',\n",
       " 'data',\n",
       " 'can',\n",
       " 'be',\n",
       " 'represented',\n",
       " 'by',\n",
       " 'multiple',\n",
       " 'views',\n",
       " 'such',\n",
       " 'as',\n",
       " 'in',\n",
       " 'web',\n",
       " 'classication',\n",
       " '10',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '3',\n",
       " 'computer',\n",
       " 'vision',\n",
       " '13',\n",
       " 'medical',\n",
       " 'diagnosis',\n",
       " '29',\n",
       " 'and',\n",
       " 'chemical',\n",
       " 'classication',\n",
       " '14',\n",
       " 'A',\n",
       " 'key',\n",
       " 'notion',\n",
       " 'in',\n",
       " 'this',\n",
       " 'learning',\n",
       " 'scheme',\n",
       " 'is',\n",
       " 'the',\n",
       " 'view',\n",
       " 'incompatibility',\n",
       " 'of',\n",
       " 'a',\n",
       " 'classier',\n",
       " 'that',\n",
       " 'is',\n",
       " 'the',\n",
       " 'fraction',\n",
       " 'of',\n",
       " 'data',\n",
       " 'over',\n",
       " 'the',\n",
       " 'sample',\n",
       " 'space',\n",
       " 'whose',\n",
       " 'views',\n",
       " 'are',\n",
       " 'disagreed',\n",
       " 'by',\n",
       " 'the',\n",
       " 'classier',\n",
       " 'in',\n",
       " 'predictions',\n",
       " '5',\n",
       " 'The',\n",
       " 'notion',\n",
       " 'is',\n",
       " 'also',\n",
       " 'called',\n",
       " 'view',\n",
       " 'disagreement',\n",
       " 'rate',\n",
       " 'in',\n",
       " 'some',\n",
       " 'discussions',\n",
       " 'such',\n",
       " 'as',\n",
       " '16',\n",
       " 'A',\n",
       " 'typical',\n",
       " 'multi',\n",
       " 'view',\n",
       " 'algorithm',\n",
       " 'e',\n",
       " 'g',\n",
       " '10',\n",
       " '24',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'unlabeled',\n",
       " 'sample',\n",
       " 'to',\n",
       " 'estimate',\n",
       " 'the',\n",
       " 'view',\n",
       " 'incompatibility',\n",
       " 'of',\n",
       " 'classiers',\n",
       " 'and',\n",
       " 'returns',\n",
       " 'those',\n",
       " 'with',\n",
       " 'low',\n",
       " 'view',\n",
       " 'incompatibility',\n",
       " 'A',\n",
       " 'recent',\n",
       " 'survey',\n",
       " 'on',\n",
       " 'multi',\n",
       " 'view',\n",
       " 'learning',\n",
       " 'can',\n",
       " 'be',\n",
       " 'found',\n",
       " 'in',\n",
       " '28',\n",
       " 'As',\n",
       " 'noted',\n",
       " 'by',\n",
       " 'many',\n",
       " 'previous',\n",
       " 'studies',\n",
       " 'semi',\n",
       " 'supervised',\n",
       " 'multi',\n",
       " 'view',\n",
       " 'learning',\n",
       " 'has',\n",
       " 'delivered',\n",
       " 'an',\n",
       " 'eective',\n",
       " 'scheme',\n",
       " 'of',\n",
       " 'training',\n",
       " 'with',\n",
       " 'the',\n",
       " 'unlabeled',\n",
       " 'sample',\n",
       " 'A',\n",
       " 'natural',\n",
       " 'follow',\n",
       " 'up',\n",
       " 'question',\n",
       " 'is',\n",
       " 'whether',\n",
       " 'such',\n",
       " 'scheme',\n",
       " 'can',\n",
       " 'be',\n",
       " 'more',\n",
       " 'ecient',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'u',\n",
       " 's',\n",
       " 'c',\n",
       " 'We',\n",
       " 'have',\n",
       " 'witnessed',\n",
       " 'signicant',\n",
       " 'progress',\n",
       " 'on',\n",
       " 'improv',\n",
       " 'ing',\n",
       " 'the',\n",
       " 'eciency',\n",
       " 'of',\n",
       " 'cid',\n",
       " '96',\n",
       " 's',\n",
       " 'c',\n",
       " 'i',\n",
       " 'e',\n",
       " 'it',\n",
       " 'is',\n",
       " 'improved',\n",
       " 'from',\n",
       " 'O',\n",
       " '1',\n",
       " 'to',\n",
       " 'O',\n",
       " 'log',\n",
       " '1',\n",
       " 'in',\n",
       " 'active',\n",
       " 'multi',\n",
       " 'view',\n",
       " 'learning',\n",
       " '27',\n",
       " 'and',\n",
       " 'even',\n",
       " 'to',\n",
       " 'O',\n",
       " 'log1',\n",
       " '1',\n",
       " 'in',\n",
       " 'semi',\n",
       " 'supervised',\n",
       " 'multi',\n",
       " 'view',\n",
       " 'learning',\n",
       " '5',\n",
       " 'which',\n",
       " 'aims',\n",
       " 'at',\n",
       " 'trading',\n",
       " 'u',\n",
       " 's',\n",
       " 'c',\n",
       " 'for',\n",
       " 'an',\n",
       " 'ecient',\n",
       " 'cid',\n",
       " '96',\n",
       " 's',\n",
       " 'c',\n",
       " 'We',\n",
       " 'notice',\n",
       " 'that',\n",
       " 'limited',\n",
       " 'progress',\n",
       " 'has',\n",
       " 'been',\n",
       " 'made',\n",
       " 'on',\n",
       " 'u',\n",
       " 's',\n",
       " 'c',\n",
       " 'and',\n",
       " 'the',\n",
       " 'state',\n",
       " 'of',\n",
       " 'the',\n",
       " 'art',\n",
       " 'result',\n",
       " 'is',\n",
       " 'O',\n",
       " '1',\n",
       " '5',\n",
       " 'Improving',\n",
       " 'u',\n",
       " 's',\n",
       " 'c',\n",
       " 'would',\n",
       " 'not',\n",
       " 'only',\n",
       " 'advance',\n",
       " 'our',\n",
       " 'understanding',\n",
       " 'on',\n",
       " 'unlabeled',\n",
       " 'data',\n",
       " 'which',\n",
       " 'is',\n",
       " 'commonly',\n",
       " 'demanded',\n",
       " 'in',\n",
       " 'large',\n",
       " 'quantity',\n",
       " 'but',\n",
       " 'also',\n",
       " 'help',\n",
       " 'to',\n",
       " 'lift',\n",
       " 'the',\n",
       " 'computational',\n",
       " 'burden',\n",
       " 'induced',\n",
       " 'by',\n",
       " 'these',\n",
       " 'data',\n",
       " 'which',\n",
       " 'usually',\n",
       " 'dominate',\n",
       " 'the',\n",
       " 'training',\n",
       " 'time',\n",
       " 'e',\n",
       " 'g',\n",
       " '18',\n",
       " '23',\n",
       " 'In',\n",
       " 'this',\n",
       " 'paper',\n",
       " 'we',\n",
       " 'improve',\n",
       " 'the',\n",
       " 'state',\n",
       " 'of',\n",
       " 'art',\n",
       " 'u',\n",
       " 's',\n",
       " 'c',\n",
       " 'of',\n",
       " 'semi',\n",
       " 'supervised',\n",
       " 'multi',\n",
       " 'view',\n",
       " 'learning',\n",
       " 'from',\n",
       " 'O',\n",
       " '1',\n",
       " 'to',\n",
       " 'O',\n",
       " 'log',\n",
       " '1',\n",
       " 'under',\n",
       " 'mild',\n",
       " 'conditions',\n",
       " 'without',\n",
       " 'trading',\n",
       " 'the',\n",
       " 'associated',\n",
       " 'cid',\n",
       " '96',\n",
       " 's',\n",
       " 'c',\n",
       " 'We',\n",
       " 'begin',\n",
       " 'our',\n",
       " 'study',\n",
       " 'by',\n",
       " 'proving',\n",
       " 'a',\n",
       " 'connection',\n",
       " 'between',\n",
       " 'the',\n",
       " 'generalization',\n",
       " 'error',\n",
       " 'of',\n",
       " 'a',\n",
       " 'classier',\n",
       " 'and',\n",
       " 'its',\n",
       " 'incompatibility',\n",
       " 'under',\n",
       " 'a',\n",
       " 'general',\n",
       " 'expanding',\n",
       " 'condition',\n",
       " 'of',\n",
       " 'sample',\n",
       " 'distribu',\n",
       " 'tion',\n",
       " '27',\n",
       " 'This',\n",
       " 'connection',\n",
       " 'induces',\n",
       " 'a',\n",
       " 'transformation',\n",
       " 'from',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'removing',\n",
       " 'high',\n",
       " 'error',\n",
       " 'classiers',\n",
       " 'to',\n",
       " 'the',\n",
       " 'prob',\n",
       " 'lem',\n",
       " 'of',\n",
       " 'removing',\n",
       " 'high',\n",
       " 'incompatibility',\n",
       " 'classiers',\n",
       " 'We',\n",
       " 'then',\n",
       " 'prove',\n",
       " 'with',\n",
       " 'high',\n",
       " 'probability',\n",
       " 'an',\n",
       " 'unlabeled',\n",
       " 'sample',\n",
       " 'of',\n",
       " 'size',\n",
       " 'O',\n",
       " 'log',\n",
       " '1',\n",
       " 'suces',\n",
       " 'to',\n",
       " 'nd',\n",
       " 'a',\n",
       " 'classier',\n",
       " 'whose',\n",
       " 'error',\n",
       " 'is',\n",
       " 'bounded',\n",
       " 'by',\n",
       " 'Here',\n",
       " 'is',\n",
       " 'the',\n",
       " 'incompatible',\n",
       " 'coecient',\n",
       " 'developed',\n",
       " 'to',\n",
       " 'characterize',\n",
       " 'u',\n",
       " 's',\n",
       " 'c',\n",
       " 'and',\n",
       " 'as',\n",
       " 'part',\n",
       " 'of',\n",
       " 'our',\n",
       " 'proof',\n",
       " 'we',\n",
       " 'showed',\n",
       " 'it',\n",
       " 'is',\n",
       " 'bounded',\n",
       " 'under',\n",
       " 'mild',\n",
       " 'conditions',\n",
       " 'and',\n",
       " 'does',\n",
       " 'not',\n",
       " 'depend',\n",
       " 'on',\n",
       " 'Joining',\n",
       " 'all',\n",
       " 'results',\n",
       " 'we',\n",
       " 'conclude',\n",
       " 'the',\n",
       " 'improved',\n",
       " 'u',\n",
       " 's',\n",
       " 'c',\n",
       " 'Detailed',\n",
       " 'proofs',\n",
       " 'will',\n",
       " 'be',\n",
       " 'presented',\n",
       " 'in',\n",
       " 'Section',\n",
       " '3',\n",
       " 'Our',\n",
       " 'result',\n",
       " 'highlights',\n",
       " 'the',\n",
       " 'importance',\n",
       " 'of',\n",
       " 'incompatible',\n",
       " 're',\n",
       " 'gion',\n",
       " 'for',\n",
       " 'the',\n",
       " 'better',\n",
       " 'utilization',\n",
       " 'of',\n",
       " 'unlabeled',\n",
       " 'data',\n",
       " 'Incompat',\n",
       " 'ible',\n",
       " 'region',\n",
       " 'is',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " 'data',\n",
       " 'whose',\n",
       " 'views',\n",
       " 'are',\n",
       " 'disagreed',\n",
       " 'by',\n",
       " 'some',\n",
       " 'classier',\n",
       " 's',\n",
       " 'in',\n",
       " 'the',\n",
       " 'search',\n",
       " 'space',\n",
       " 'Algorithmic',\n",
       " 'issues',\n",
       " 're',\n",
       " '627',\n",
       " 'garding',\n",
       " 'the',\n",
       " 'estimation',\n",
       " 'of',\n",
       " 'this',\n",
       " 'region',\n",
       " 'is',\n",
       " 'discussed',\n",
       " 'in',\n",
       " 'Section',\n",
       " '4',\n",
       " 'In',\n",
       " 'Section',\n",
       " '5',\n",
       " 'we',\n",
       " 'empirically',\n",
       " 'verify',\n",
       " 'this',\n",
       " 'theoretic',\n",
       " 'insight',\n",
       " 'by',\n",
       " 'designing',\n",
       " 'two',\n",
       " 'proof',\n",
       " 'of',\n",
       " 'concept',\n",
       " 'multi',\n",
       " 'view',\n",
       " 'learning',\n",
       " 'algo',\n",
       " 'rithms',\n",
       " 'One',\n",
       " 'is',\n",
       " 'based',\n",
       " 'on',\n",
       " 'active',\n",
       " 'view',\n",
       " 'sensing',\n",
       " '29',\n",
       " 'which',\n",
       " 'aims',\n",
       " 'at',\n",
       " 'actively',\n",
       " 'collecting',\n",
       " 'missing',\n",
       " ...]"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#re.findall(r'?<=(Abstract|ABSTRACT)(.*)',fd[:2000])\n",
    "re.findall('[\\w]+', fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-575-facff69b7552>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-575-facff69b7552>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    except IndexError:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    abstr = re.findall((r'(Abstract|ABSTRACT)([^]]*)',fd[:2000])\n",
    "except IndexError:\n",
    "    abstr = re.findall (r'(Abstract|ABSTRACT)([^]]*)',fd[:2000])[0]\n",
    "except IndexError:\n",
    "    abstr = re.findall (r'(Abstract|ABSTRACT)([^]]*)',fd[:2000])[0][1]\n",
    "else :\n",
    "    abstr = \"We are unable to parse the abstract of %s, with title %s; Check file for abstract\" % (fileName,title)\n",
    "\n",
    "if isinstance(abstract, tuple):\n",
    "    abstr = re.sub('[\\s]',\" \",abstr[1])\n",
    "elif isinstance(abstract,list):\n",
    "    abstr = re.sub('[\\s]',\" \",abstr[1])\n",
    "elif isinstance(abstract,str):\n",
    "    abstr = re.sub('[\\s]',\" \", abstr)\n",
    "\n",
    "else:\n",
    "    abstr = \"We are unable to parse the abstract of %s, with title %s; Check file for abstract\" % (fileName,title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-343-3ff42bdc7673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mabstract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mabstract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mr'(Abstract|ABSTRACT)([^]]*)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    abstract = re.findall (r'\\n\\n(Abstract|ABSTRACT)([^]]*)\\n\\n',document[:2000])\n",
    "except IndexError:\n",
    "    abstract = re.findall (r'(Abstract|ABSTRACT)([^]]*)',document[:2000])[0][1]\n",
    "except IndexError:\n",
    "    abstract = re.findall()\n",
    "else:\n",
    "    abstract = re.findall (r'(Abstract|ABSTRACT)([^]]*)',document[:2000])[0]\n",
    "    \n",
    "if isinstance(abstract, tuple):\n",
    "    abstract = re.sub('[\\s]',\" \",abstract[1])\n",
    "elif isinstance(abstract,list):\n",
    "    abstract = re.sub('[\\s]',\" \",abstract[0])\n",
    "elif isinstance(abstract,str):\n",
    "    abstract = re.sub('[\\s]',\" \", abstract)\n",
    "else:\n",
    "    abstract = re.sub('[\\s]',\" \",abstract)\n",
    "        \n",
    "abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using TF-IDF to Determine Word Relevance in Document Queries   Juan Ramos  Department of Computer Science, Rutgers University, 23515 BPO Way, Piscataway, NJ, 08855       JURAMOS@EDEN.RUTGERS.EDU   Abstract   In this paper, we examine the results of applying  Term  Frequency  Inverse  Document  Frequency  (TF-IDF) to determine what words in a corpus of  documents might be more favorable to use in a  query.    As  the  term implies, TF-IDF  calculates  values for each word in a document through an  inverse proportion of the frequency of the word  in  a  particular  document  to  the  percentage  of  documents  the  word  appears  in.    Words  with  high  TF-IDF  numbers  strong  relationship  with  the  document  they  appear  in,  suggesting that if that word were to appear in a  query, the document could  be  of interest to the  user.    We  provide  evidence  that  this  simple  algorithm  efficiently  categorizes  relevant  words  that can enhance query retrieval.   imply   a   1.  Introduction  Before  proceeding  in  depth  into  our  experiments,  it  is  useful  to  describe  the  nature  of  the  query  retrieval  problem  for  a  corpus  of  documents  and  the  different  approaches used to solve it, including TF-IDF.   1.1  Query Retrieval Problem  The task of retrieving data from a user-defined query has  become so common and natural in recent years that some  might  not  give  it  a  second  thought.    However,  this  growing  use  of  query  retrieval  warrants  continued  research and enhancements to generate better solutions to  the problem.  Informally, query retrieval can be described as the task of  searching  a  collection  of  data,  be  that  text  documents,  databases,  networks,  etc.,  for  specific  instances  of  that  data.    First,  we  will  limit  ourselves  to  searching  a  collection  of  English  documents.    The  refined  problem  then  becomes  the  task  of  searching  this  corpus  for  documents  that  the  query  retrieval  system  considers  relevant to what the user entered as the query.  Let us describe this problem more formally.  We have a  set of documents D, with the user entering a query q = w1,  w2, (cid:133), wn for a sequence of words wi.  Then we wish to   return  a  subset  D*  of  D  such  that  for  each  d    D*,  we  maximize the following probability:     P(d | q, D) (1)      framework   (Berger  &  Lafferty,  1999).    As  the  above  notation  suggests,  numerous  approaches  to  this  problem  involve  probability  and  statistics,  while  others  propose  vector- based models to enhance the retrieval.  1.2  Algorithms for Ad-Hoc Retrieval  Let  us  briefly  examine  other  approaches  used  for  responding  to  queries.    Intuitively,  given  the  formal  notation we present for the problem, the use of statistical  methods  has  proven  both  popular  and  efficient  in  responding to the problem.  (Berger & Lafferty, 1999) for  example,  propose  a  probabilistic  that  incorporates the user(cid:146)s mindset at the time the query was  entered  to  enhance  their  approximations.    They  suggest  that the user has a specific information need G, which is   approximated  as  a  sequence  of  words  q  in  the  actual  query.  By accounting for this noisy transformation of G  into  q  and  applying  Bayes(cid:146)  Law  to  equation  (1),  they  show  good  results  on  returning  appropriate  documents  given q.  Vector-based methods for performing query retrieval also  show  good  promise.    (Berry,  Dumais  &  O(cid:146)Brien, 1994)  suggest performing query retrieval using a popular matrix  algorithm  called  Latent  Semantic  Indexing  (LSI).    In  essence,  the  algorithm  creates  a  reduced-dimensional  vector space that captures an n-dimensional representation  of  a  set  of  documents.    When  a  query  is  entered,  its  numerical representation is compared the cosine-distance  of  other  documents  in  the  document  space,  and  the  algorithm returns documents where this distance is small.   The authors(cid:146) experimental results show that this algorithm  is  highly  effective  in  query  retrieval,  even  when  the  problem  entails  performing  information  retrieval  over  documents  written  in  different  languages  (Littman  &  Keim 1997).  If certain criteria are met, they suggest that  the  LSI  approach  can  be  extended  to  more  than  two  languages.  The  procedure  we  examine  with  more  detail  is  Term  Frequency Inverse Document Frequency (TF-IDF).  This  weighing  scheme  can  be  categorized  as  a  statistical       procedure, though its immediate results are deterministic  in  nature.    Though  TF-IDF  is  a relatively  old  weighing  scheme,  it  is  simple  and  effective,  making  it  a  popular  starting point for other, more recent algorithms (Salton &  Buckley,  1988).    In  this  paper,  we  will  examine  the  behavior of TF-IDF over a set of English documents from  the  LDC(cid:146)s  United  Nations  Parallel  Text  Corpus.    The  purpose  of  this  paper  is  to  examine  the  behavior,  strengths, and weaknesses of TF-IDF as a starting point  for future algorithms.   2.  An Overview of TF-IDF  We will now examine the structure and implementation of  TF-IDF for a set of documents.  We will first introduce  the  mathematical  background  of  the  algorithm  and  examine its behavior relative to each variable.  We then  present the algorithm as we implemented it.   2.1  Mathematical Framework  We  will  give  a  quick  informal  explanation  of  TF-IDF  before  proceeding.    Essentially,  TF-IDF  works  by  determining the relative frequency of words in a specific  document compared to the inverse proportion of that word  over  the  entire  document  corpus.    Intuitively,  this  calculation determines how relevant a given word is in a  particular document.  Words that are common in a single  or  a  small group  of  documents  tend to  have  higher  TF- IDF  numbers  than  common  words  such  as  articles  and  prepositions.  The formal procedure for implementing TF-IDF has some  minor differences over all its applications, but the overall  approach works as follows.  Given a document collection  D,  a  word  w,  and  an  individual  document  d    D,  we  calculate     wd = fw, d * log (|D|/fw, D)  (2),      where fw, d equals the number of times w appears in d, |D|  is the size of the corpus, and fw,  D equals the number of  documents in which w appears in D (Salton & Buckley,  1988,  Berger,  et  al,  2000).    There  are  a  few  different  situation that can occur here for each word, depending on  the values of  fw,  d, |D|, and fw,  D, the most prominent of  which we(cid:146)ll examine.  Assume  that  |D|  ~  fw,  D,  i.e.  the  size  of  the  corpus  is  approximately  equal  to  the  frequency  of  w  over  D.    If        1 < log (|D|/ fw, D) < c for some very small constant c, then  wd will be smaller than fw, d but still positive.  This implies  that w is relatively common over the entire corpus but still  holds some importance throughout D.  For example, this  could  be  the  case  if  TF-IDF  would  examine  the  word  (cid:145)Jesus(cid:146) over the New Testament.  More relevant to us, this  result  would  be  expected  of  the  word  (cid:145)United(cid:146)  in  the  corpus  of  United  Nations  documents.    This  is  also  the  case  for  extremely  common  words  such  as  articles,      pronouns, and prepositions, which by themselves hold no  relevant  meaning  in  a  query  (unless  the  user  explicitly  wants documents containing such common words).  Such  common  words  thus  receive  a  very  low  TF-IDF  score,  rendering them essentially negligible in the search.  Finally,  suppose  fw,  d  is  large  and  fw,  D  is  small.    Then       log (|D|/ fw, D) will be rather large, and so wd will likewise  be large.  This is the case we(cid:146)re most interested in, since  words with high wd imply that w is an important word in d  but not common in D.  This w term is said to have a large  discriminatory  power. Therefore,  when  a  query  contains  this w, returning a document d where wd is large will very  likely satisfy the user.   2.2  Encoding TF-IDF  The code for TF-IDF is elegant in its simplicity.  Given a  query q composed of a set of words wi, we calculate wi, d  for  each  wi  for  every  document  d    D.    In  the  simplest  way, this can be done by running through the document  collection  and  keeping  a  running  sum  of  fw,  d  and  fw,  D.   Once done, we can easily calculate wi  d according to the  mathematical  framework  presented  before.    Once  all       wi, d(cid:146)s are found, we return a set D* containing documents  d such that we maximize the following equation:   i wi, d (3).   Either the user or the system can arbitrarily determine the  size of D* prior to initiating the query.  Also, documents  are returned in a decreasing order according to equation  (3).  This  is  the  traditional  method  of  implementing TF-IDF.   We  will  discuss  extensions  of  this  algorithm  in  later  sections, along with an analysis of TF-IDF according to  our own results.   3.  Experiment   3.1  Data Collection and Formatting  We tested our TF-IDF implementation on a collection of  1400 documents from the LDC(cid:146)s United Nations Parallel  Text Corpus.  These documents were gathered arbitrarily  from  a  larger  collection  of  documents  from  the  UN(cid:146)s  1988  database.    The  documents  were  encoded  with  the  SGML  text  format,  so  we  decided  to  leave  in  the  formatting tags to account for noisy data and to test the  robustness  of  TF-IDF.    We  simulated  more  noise  by  enforcing case-sensitivity.  Due to certain constraints, we  had  to  limit  the  number  of  queries  used  to  perform  information  retrieval  on  to  86.    We  calculate  TF-IDF  weights for these queries according to equation (3), and  then  return  the  first  100  documents  that  maximize  equation  (3).    The returned  documents  were  returned  in  descending  order,  with  documents  with  higher  weight  sums  appearing  first.    To  compare  our  results,  we  also  performed  in  parallel  the  brute  force  (and  rather  na(cid:239)ve)       method  of  performing  query  retrieval  based  only  on  the  term  fw,  d.    Naturally,  this  latter  method  is  intuitively  flawed for the larger problem of query retrieval, since this  approach  would  simply  return  documents  where  non- relevant  words  appear  most  (i.e.  long  documents  with  plenty of articles and prepositions that might not have any  relevance  to  the  query).    We  will  provide  evidence  that  TF-IDF,  though relatively  simple,  is  a  big improvement  over this na(cid:239)ve approach.   3.2  Experimental Results  Return Pos.  Document #  Sum fw, d  1  2  3  4  5  6  7  8   64  879  1037  324  710  161  1175  402   139  136  121  107  98  93  87  86   Sum wd  1.83  4.52  2.08  0.91  7.22  3.95  0.24  5.13   Table 1. First eight documents with highest fw, d returned by our  na(cid:239)ve  algorithm  for  query  =  (cid:147)the  trafficking  of  drugs  in  Colombia(cid:148).  The high fw,  d comes mostly from long documents  with plenty articles and prepositions.  These documents had very  low wd scores and are mostly useless to the query.  As  expected,  the  na(cid:239)ve,  brute  force  approach  of  simply  returning  documents  with  high  sum  of  fw,  d  given  each  query  word  was  very  inaccurate.    Of  the  top  eight  documents  shown  in  Table  1,  none  are  relevant  to  the  given query.  This pattern is evident throughout the whole  list of 344 words, which is summarized in Figure 1.   Naive Query Retrieval     y r e u Q o t   e c n a v e e R  l  12  10  8  6  4  2  0  -2  1 6  1 1  6 1  1 2  6 2  1 3  6 3  1 4  6 4  1 5  6 5  1 6  6 6  1 7  6 7  1 8  6 8  1 9  6 9  Documents  Figure 1.  Results of running na(cid:239)ve query retrieval on our data.  Notice  the  algorithm  does  not  consider  wd,  but  rather  returns  documents  based  solely  on  fw,d.    Relevant  documents  are  scattered sporadically, so simply returning the top documents as  done by this algorithm returns irrelevant documents.      Let us now consider the results from running TF-IDF on  our  data.    In  this  case,  documents  at  the  top  of  the  list  have  a  high  sum  of  wd,  so  a  query  containing  w  would  likely receive document d as a return value.  Return Pos.  Document #  Sum fw, d  1  2  3  4  5  6  7  8   Sum wd  28.09  26.73  23.96  19.16  16.5  15.42  12.34  10.79   788  426  881  253  1007  362  520  23   24  72  56  43  37  29  33  58    Table 2. First eight documents with highest wd returned by TF- IDF for query = (cid:147)the trafficking of drugs in Colombia(cid:148).  The top  document,  entitled  (cid:147)International  Campaign  Against  Traffic  in  Drugs(cid:148), is intuitively relevant to the query.   As  Table  2  shows,  retrieval  with  TF-IDF  returned  documents highly correlate to the given query.  The top  two  documents  make  frequent  use  of  the  non-article  words  in  the  query;  words  that  are  not  that  frequent  in  other documents.  This gives a high sum of wd, which in  turn  gives  a  high  relevance  to  the  document.    Figure  2  shows this continuing pattern for our data.   Query Results with TF-IDF     y r e u Q o t   t e c n a v e l e R  120  100  80  60  40  20  0  1 7  3 1  9 1  5 2  1 3  7 3  3 4  9 4  5 5  1 6  7 6  3 7  9 7  5 8  1 9  7 9  Documents  Figure  2.  Results  of  retrieval  with  TF-IDF  on  our  data.    High  values of wd are concentrated on the beginning of the graph, so  basing  query  retrieval  on the  top  words  here  will  likely  return  relevant documents.    The two  extra  graphs indicate  upper and  lower bounds found by the retrieval engine.  Clearly,  TF-IDF  is  much  more  powerful  than  its  na(cid:239)ve  counterpart.  When examining the words in the query, we  see  that TF-IDF  can  find  documents  that  make  frequent    words  with  their  lexical  derivations  and  synonyms.   Future research might also include employing TF-IDF to  performing  searches  in  documents  written  in a  different  language than the query.  Enhancing the already powerful  TF-IDF  algorithm  would  increase  the  success  of  query  retrieval systems, which have quickly risen to become a  key element of present global information exchange.   References  Berger, A & Lafferty, J. (1999).  Information Retrieval as  Statistical Translation. In Proceedings of the 22nd ACM  Conference  on  Research  and  Development  in  Information Retrieval (SIGIR(cid:146)99), 222-229.   Berger,  A  et  al  (2000).    Bridging  the  Lexical  Chasm:  Statistical Approaches to Answer Finding.  In Proc. Int.  Conf.  Research  and  Development  in  Information  Retrieval, 192-199.   Berry,  Michael  W.  et al.  (1995).   Using  Linear  Algebra  for  Intelligent  Information  Retrieval.    SIAM  Review,  37(4):177-196.   Brown, Peter F. et al. (1990).  A Statistical Approach to  Machine  Translation.    In  Computational  Linguistics  16(2): 79-85.   Littman, M., & Keim, G. (1997).  Cross-Language Text  Retrieval with Three Languages.  In CS-1997-16, Duke  University.   Oren, Nir.  (2002). Reexamining tf.idf based information  retrieval with Genetic Programming.  In Proceedings of  SAICSIT 2002, 1-10.   Salton,  G.  &  Buckley,  C.  (1988).  Term-weighing  approache sin automatic text retrieval.  In Information  Processing & Management, 24(5): 513-523.               use of said words and determine if they are relevant in the  document.  The discriminatory power of TF-IDF allows  the  retrieval  engine  to  quickly  find  relevant  documents  that are likely to satisfy the user.   4.  Conclusions   relevant   information  on   4.1  Advantages and Limitations  We  have  seen  that  TF-IDF  is  an  efficient  and  simple  algorithm  for  matching  words  in  a  query  to  documents  that are relevant to that query.  From the data collected,  we  see  that  TF-IDF  returns  documents  that  are  highly  relevant to a particular query.  If a user were to input a  query for a particular topic, TF-IDF can find documents  that  contain  the  query.   Furthermore, encoding TF-IDF is straightforward, making  it  ideal  for  forming  the  basis  for  more  complicated  algorithms  and  query  retrieval  systems  (Berger  et  al,  2000).  Despite its strength, TF-IDF has its limitations.  In terms  of synonyms, notice that TF-IDF does not make the jump  to the relationship between words.  Going back to (Berger  & Lafferty, 1999), if the user wanted to find information  about, say, the word (cid:145)priest(cid:146), TF-IDF would not consider  documents that might be relevant to the query but instead  use the word (cid:145)reverend(cid:146).  In our own experiment, TF-IDF  could not equate the word (cid:145)drug(cid:146) with its plural (cid:145)drugs(cid:146),  categorizing  each  instead  as  separate  words  and  slightly  decreasing  the  word(cid:146)s  wd  value.    For  large  document  collections, this could present an escalating problem.     This  algorithm  gradient  descent   4.2  Further Research  Since  TF-IDF  is  merely  a  staple  benchmark,  numerous  algorithms have surfaced that take the program to the next  level.  (Berger et al, 2000) propose a number of these in a  single paper, including a version of TF-IDF that they call  incorporates  Adaptive  TF-IDF.  hillclimbing  and  to  enhance  performance.    They  also  propose  an  algorithm  for  performing  TF-IDF  in  a  cross-language  retrieval  setting  by  applying  statistical  translation  to  the  benchmark  TF- IDF.  Genetic  algorithms  have  also  been  used  to  evolve  programs that can match or beat TF-IDF schemes.  (Oren,  2002)  employs  this  method  to  evolve  a  large  colony  of  individuals.  ideas  of  genetic  programming,  mutation,  crossover  and  copying,  the  author  of  the  paper  was  able  to  evolve  programs  that  performed  slightly  better  than  the  common  TF-IDF  weighing scheme.  Though the author felt the results were  not  considered  significant,  the  paper  shows  that  there  is  still interest in enhancing the simple TF-IDF scheme.  Examining our data, the easiest way for us to enhance TF- IDF  would  be  to  disregard  case-sensitivity  and  equate   the  main     Using       '"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = re.sub('[\\s]',\" \",document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall (r'\\n \\n([^]]*\\n)',a[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 901 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print 'there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p507.pdf',\n",
       " 'p627.pdf',\n",
       " 'p895.pdf',\n",
       " 'p597.pdf',\n",
       " 'p785.pdf',\n",
       " 'p825.pdf',\n",
       " 'p695.pdf',\n",
       " 'p289.pdf',\n",
       " 'p417.pdf',\n",
       " 'p447.pdf',\n",
       " 'p269.pdf',\n",
       " 'p915.pdf',\n",
       " 'p497.pdf',\n",
       " 'p765.pdf',\n",
       " 'p387.pdf',\n",
       " 'p19.pdf',\n",
       " 'p359.pdf',\n",
       " 'p557.pdf',\n",
       " 'p745.pdf',\n",
       " 'p339.pdf',\n",
       " 'p675.pdf',\n",
       " 'p617.pdf',\n",
       " 'p715.pdf',\n",
       " 'p259.pdf',\n",
       " 'p537.pdf',\n",
       " 'p865.pdf',\n",
       " 'p407.pdf',\n",
       " 'p477.pdf',\n",
       " 'p487.pdf',\n",
       " 'p329.pdf',\n",
       " 'p457.pdf',\n",
       " 'p607.pdf',\n",
       " 'p299.pdf',\n",
       " 'p855.pdf',\n",
       " 'p349.pdf',\n",
       " 'p1005.pdf',\n",
       " 'p437.pdf',\n",
       " 'p249.pdf',\n",
       " 'p517.pdf',\n",
       " 'p875.pdf',\n",
       " 'p587.pdf',\n",
       " 'p59.pdf',\n",
       " 'p905.pdf',\n",
       " 'p725.pdf',\n",
       " 'p379.pdf',\n",
       " 'p527.pdf',\n",
       " 'p755.pdf',\n",
       " 'p577.pdf',\n",
       " 'p805.pdf',\n",
       " 'p239.pdf',\n",
       " 'p685.pdf',\n",
       " 'p397.pdf',\n",
       " 'p845.pdf',\n",
       " 'p665.pdf',\n",
       " 'p427.pdf',\n",
       " 'p279.pdf',\n",
       " 'p645.pdf',\n",
       " 'p835.pdf',\n",
       " 'p467.pdf',\n",
       " 'p635.pdf',\n",
       " 'p29.pdf',\n",
       " 'p309.pdf',\n",
       " 'p655.pdf',\n",
       " 'p39.pdf',\n",
       " 'p705.pdf',\n",
       " 'p567.pdf',\n",
       " 'p219.pdf',\n",
       " 'p815.pdf',\n",
       " 'p547.pdf',\n",
       " 'p229.pdf',\n",
       " 'p885.pdf',\n",
       " 'p735.pdf',\n",
       " 'p369.pdf',\n",
       " 'p319.pdf',\n",
       " 'p775.pdf']"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# remove semicolon to see output\n",
    "corpus.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how to access values in document store and attributes of each document for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title', 'Abstract', 'Entities', 'Stems', 'Lems', 'Stanford ER']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['p59.pdf'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using TF-IDF to Determine Word Relevance in Document Queries \\n\\nJuan Ramos \\nDepartment of Computer Science, Rutgers University, 23515 BPO Way, Piscataway, NJ, 08855 \\n \\n \\n\\nJURAMOS@EDEN.RUTGERS.EDU \\n\\nAbstract \\n\\nIn this paper, we examine the results of applying \\nTerm  Frequency  Inverse  Document  Frequency \\n(TF-IDF) to determine what words in a corpus of \\ndocuments might be more favorable to use in a \\nquery.    As  the  term implies, TF-IDF  calculates \\nvalues for each word in a document through an \\ninverse proportion of the frequency of the word \\nin  a  particular  document  to  the  percentage  of \\ndocuments  the  word  appears  in.    Words  with \\nhigh  TF-IDF  numbers \\nstrong \\nrelationship  with  the  document  they  appear  in, \\nsuggesting that if that word were to appear in a \\nquery, the document could  be  of interest to the \\nuser.    We  provide  evidence  that  this  simple \\nalgorithm  efficiently  categorizes  relevant  words \\nthat can enhance query retrieval. \\n\\nimply \\n\\na \\n\\n1.  Introduction \\nBefore  proceeding  in  depth  into  our  experiments,  it  is \\nuseful  to  describe  the  nature  of  the  query  retrieval \\nproblem  for  a  corpus  of  documents  and  the  different \\napproaches used to solve it, including TF-IDF. \\n\\n1.1  Query Retrieval Problem \\nThe task of retrieving data from a user-defined query has \\nbecome so common and natural in recent years that some \\nmight  not  give  it  a  second  thought.    However,  this \\ngrowing  use  of  query  retrieval  warrants  continued \\nresearch and enhancements to generate better solutions to \\nthe problem. \\nInformally, query retrieval can be described as the task of \\nsearching  a  collection  of  data,  be  that  text  documents, \\ndatabases,  networks,  etc.,  for  specific  instances  of  that \\ndata.    First,  we  will  limit  ourselves  to  searching  a \\ncollection  of  English  documents.    The  refined  problem \\nthen  becomes  the  task  of  searching  this  corpus  for \\ndocuments  that  the  query  retrieval  system  considers \\nrelevant to what the user entered as the query. \\nLet us describe this problem more formally.  We have a \\nset of documents D, with the user entering a query q = w1, \\nw2, (cid:133), wn for a sequence of words wi.  Then we wish to \\n\\nreturn  a  subset  D*  of  D  such  that  for  each  d    D*,  we \\nmaximize the following probability: \\n \\n\\nP(d | q, D) (1) \\n\\n \\n\\nframework \\n\\n(Berger  &  Lafferty,  1999).    As  the  above  notation \\nsuggests,  numerous  approaches  to  this  proble'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert document from unicode to string \n",
    "\n",
    "import unicodedata\n",
    "document = unicodedata.normalize('NFKD', a).encode('ascii','ignore')\n",
    "document[:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   In this paper, we examine the results of applying  Term  Frequency  Inverse  Document  Frequency  (TF-IDF) to determine what words in a corpus of  documents might be more favorable to use in a  query.    As  the  term implies, TF-IDF  calculates  values for each word in a document through an  inverse proportion of the frequency of the word  in  a  particular  document  to  the  percentage  of  documents  the  word  appears  in.    Words  with  high  TF-IDF  numbers  strong  relationship  with  the  document  they  appear  in,  suggesting that if that word were to appear in a  query, the document could  be  of interest to the  user.    We  provide  evidence  that  this  simple  algorithm  efficiently  categorizes  relevant  words  that can enhance query retrieval.   imply   a   1.  Introduction  Before  proceeding  in  depth  into  our  experiments,  it  is  useful  to  describe  the  nature  of  the  query  retrieval  problem  for  a  corpus  of  documents  and  the  different  approaches used to solve it, including TF-IDF. '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting title\n",
    "title = re.findall(\"^[^\\\\n\\\\n]+\",document)[0]\n",
    "title\n",
    "\n",
    "# Getting the abstract\n",
    "abstract = re.findall (r'\\n\\n(Abstract|ABSTRACT)([^]]*)\\n\\n',document[:2000])[0]\n",
    "\n",
    "if isinstance(abstract, tuple):\n",
    "    abstract = re.sub('[\\s]',\" \",abstract[1])\n",
    "else:\n",
    "    abstract = re.sub('[\\s]',\" \",abstract)\n",
    "abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Juan Ramos  Department of Computer Science, Rutgers University, 23515 BPO Way, Piscataway, NJ, 08855       JURAMOS@EDEN.RUTGERS.EDU '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracts section with names and email addresses only\n",
    "\n",
    "section  = re.findall (r'\\n\\n([^]]*)\\n\\n(Abstract|ABSTRACT)',document[:2000])\n",
    "\n",
    "type(section[0][0])\n",
    "\n",
    "if isinstance(section, list):\n",
    "    section = re.sub('[\\s]',\" \",section[0][0])\n",
    "else:\n",
    "    section = re.sub('[\\s]',\" \",section)\n",
    "section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = re.findall (r'\\n\\n([^]]+)\\n\\n(Abstract|ABSTRACT)',document[:2000] )\n",
    "\n",
    "if isinstance(test, str):\n",
    "    print \"Wow\"\n",
    "elif isinstance (test,int):\n",
    "    print \"I'm surprised!\"\n",
    "\n",
    "elif  isinstance (test,list):\n",
    "    print \"We got it RIGHT!\"\n",
    "\n",
    "else:\n",
    "    print \"Nothing fits\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tree.collapse_unary of Tree('S', [('Relationships', 'NNS'), ('in', 'IN'), ('Multi-Label', 'NNP'), ('Learning', 'NNP'), Tree('PERSON', [('Christina', 'NNP'), ('Papagiannopoulou', 'NNP'), ('School', 'NNP')]), ('of', 'IN'), Tree('ORGANIZATION', [('Informatics', 'NNP'), ('Aristotle', 'NNP'), ('University', 'NNP')]), ('of', 'IN'), ('Thessaloniki', 'NNP'), ('Thessaloniki', 'NNP'), ('54124', 'CD'), (',', ','), Tree('GPE', [('Greece', 'NNP')]), ('cppapagi', 'NN'), ('@', ':'), ('gmail.com', 'JJ'), ('Grigorios', 'NNS'), Tree('PERSON', [('Tsoumakas', 'NNP'), ('School', 'NNP')]), ('of', 'IN'), Tree('ORGANIZATION', [('Informatics', 'NNP'), ('Aristotle', 'NNP'), ('University', 'NNP')]), ('of', 'IN'), ('Thessaloniki', 'NNP'), ('Thessaloniki', 'NNP'), ('54124', 'CD'), (',', ','), Tree('GPE', [('Greece', 'NNP')]), ('greg', 'NN'), ('@', ':'), ('csd.auth.gr', 'JJ'), Tree('PERSON', [('Ioannis', 'NNP')]), ('Tsamardinos1,2', 'NNP'), ('1Computer', 'CD'), ('Science', 'NNP'), ('Dept.', 'NNP'), (',', ','), Tree('GPE', [('Univ', 'NNP')]), ('.', '.'), ('of', 'IN'), Tree('GPE', [('Crete', 'NNP')]), (',', ','), Tree('GPE', [('Greece', 'NNP')]), ('2Institute', 'JJ'), ('of', 'IN'), Tree('ORGANIZATION', [('Computer', 'NNP'), ('Science', 'NNP')]), (',', ','), Tree('ORGANIZATION', [('FORTH', 'NNP'), ('Voutes', 'NNP'), ('Campus', 'NNP')]), (',', ','), ('700', 'CD'), ('13', 'CD'), ('Heraklion', 'NNP'), (',', ','), Tree('GPE', [('Crete', 'NNP')]), (',', ','), Tree('GPE', [('Greece', 'NNP')]), ('tsamard.it', 'NN'), ('@', ':'), ('gmail.com', 'JJ')])>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to extract entities from top section of pdf and store a relationship tree\n",
    "\n",
    "import nltk\n",
    "\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(section))\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "entities.collapse_unary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Juan', u'ORGANIZATION'), (u'Ramos', u'ORGANIZATION'), (u'Department', u'ORGANIZATION'), (u'of', u'ORGANIZATION'), (u'Computer', u'ORGANIZATION'), (u'Science', u'ORGANIZATION'), (u',', u'O'), (u'Rutgers', u'ORGANIZATION'), (u'University', u'ORGANIZATION'), (u',', u'O'), (u'23515', u'O'), (u'BPO', u'MISC'), (u'Way', u'MISC'), (u',', u'O'), (u'Piscataway', u'LOCATION'), (u',', u'O'), (u'NJ', u'LOCATION'), (u',', u'O'), (u'08855', u'O'), (u'JURAMOS', u'O'), (u'@', u'O'), (u'EDEN.RUTGERS.EDU', u'O')]\n"
     ]
    }
   ],
   "source": [
    "# Use another entity extractor\n",
    "\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "st = StanfordNERTagger('/Users/linwood/stanford-corenlp-full-2015-04-20/classifiers/english.conll.4class.distsim.crf.ser.gz',\n",
    "\t\t\t\t\t   '/Users/linwood/stanford-corenlp-full-2015-04-20/stanford-corenlp-3.5.2.jar',\n",
    "\t\t\t\t\t   encoding='utf-8')\n",
    "\n",
    "text = section\n",
    "tokenized_text = word_tokenize(section)\n",
    "classified_text = st.tag(tokenized_text)\n",
    "\n",
    "print(classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p99.pdf', 'p925.pdf']"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates the json document format to store the files\n",
    "\n",
    "corpus = {}\n",
    "corpus[str(fileName)]={}\n",
    "corpus[str(fileName)]={'Title':title,'Abstract':abstract,'Entities':entities}\n",
    "corpus['p99.pdf']={'Title':\"Test title\",'Abstract':\"test abstract langauge.  Just adding test to make it longer\", 'Entities':\"Linwood Creekmore\"}\n",
    "\n",
    "corpus.keys()\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing how to use the Stanford NER Tagger\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "st = StanfordNERTagger('/Users/linwood/stanford-corenlp-full-2015-04-20/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "\t\t\t\t\t   '/Users/linwood/stanford-corenlp-full-2015-04-20/stanford-ner-3.5.2.jar',\n",
    "\t\t\t\t\t   encoding='utf-8')\n",
    "\n",
    "st.tag(word_tokenize(section));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from http://brandonrose.org/clustering\n",
    "\n",
    "\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmer = WordNetLemmatizer()\n",
    "wordnet_tags = ['n', 'v', 'a', 's', 'r']\n",
    "\n",
    "def tokenize_and_stem_n_lem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stemmedlems = [lemmer.lemmatize(stemmer.stem(t)) for t in filtered_tokens]\n",
    "    lems = [lemmer.lemmatize(l) for l in filtered_tokens]\n",
    "    \n",
    "    return stemmedlems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating lists for abstracts, titles, and entities\n",
    "\n",
    "abstracts = [i['Abstract'] for i in corpus.values()[:]]\n",
    "titles = [i['Title'] for i in corpus.values()[:]]\n",
    "ents = [i['Entities'] for i in corpus.values()[:]]\n",
    "\n",
    "\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in abstracts:\n",
    "    allwords_stemmed = tokenize_and_stem_n_lem(i) #for some reason, had to add index in list here\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17146428  0.16828501  0.         ...,  0.14916638  0.          0.        ]\n",
      " [ 0.21066584  0.          0.         ...,  0.          0.2331633\n",
      "   0.22349326]\n",
      " [ 0.          0.          0.         ...,  0.17149221  0.          0.20913053]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.48691254  0.          0.        ]\n",
      " [ 0.11914226  0.          0.         ...,  0.10364852  0.13186572  0.        ]\n",
      " [ 0.          0.16023784  0.34641254 ...,  0.28406686  0.18070051  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, ngram_range=(1,3))\n",
    "print vectorizer.fit_transform(abstracts).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 16488 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print 'there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(abstracts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 139 ms, total: 1.26 s\n",
      "Wall time: 1.16 s\n",
      "(83, 56)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem_n_lem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(abstracts) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.00000000e+00   7.57285856e-01   8.55456992e-01 ...,   8.10228320e-01\n",
      "    7.92196685e-01   8.94651464e-01]\n",
      " [  7.57285856e-01   1.11022302e-16   6.51007760e-01 ...,   8.29244427e-01\n",
      "    7.73230612e-01   6.08082072e-01]\n",
      " [  8.55456992e-01   6.51007760e-01   0.00000000e+00 ...,   6.79580194e-01\n",
      "    7.92530515e-01   9.37545580e-01]\n",
      " ..., \n",
      " [  8.10228320e-01   8.29244427e-01   6.79580194e-01 ...,  -2.22044605e-16\n",
      "    4.55010779e-01   6.04193138e-01]\n",
      " [  7.92196685e-01   7.73230612e-01   7.92530515e-01 ...,   4.55010779e-01\n",
      "   -2.22044605e-16   8.35652381e-01]\n",
      " [  8.94651464e-01   6.08082072e-01   9.37545580e-01 ...,   6.04193138e-01\n",
      "    8.35652381e-01   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "print dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 76.9 ms, sys: 3.25 ms, total: 80.2 ms\n",
      "Wall time: 78.3 ms\n",
      "83\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>clusters</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Locally Densest Subgraph Discovery</td>\n",
       "      <td>Mining dense subgraphs from a large graph is ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Collective Opinion Spam Detection:</td>\n",
       "      <td>Online reviews capture the testimonials of re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep Computational Phenotyping</td>\n",
       "      <td>We apply deep learning to the problem of disc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reducing the Unlabeled Sample Complexity of</td>\n",
       "      <td>In semi-supervised multi-view learning, unlab...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Non-transitive Hashing with Latent Similarity ...</td>\n",
       "      <td>Approximating the semantic similarity between...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>State-Driven Dynamic Sensor Selection and Pred...</td>\n",
       "      <td>An important problem in large-scale sensor mi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>From Group to Individual Labels using Deep Fea...</td>\n",
       "      <td>In many classication problems labels are rela...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Inferring Networks of Substitutable</td>\n",
       "      <td>In a modern recommender system, it is importa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Graph Query Reformulation with Diversity</td>\n",
       "      <td>We study a problem of graph-query reformulati...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A Learning-based Framework to Handle Multi-round</td>\n",
       "      <td>Considering nowadays companies providing simi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Using TF-IDF to Determine Word Relevance in Do...</td>\n",
       "      <td>In this paper, we examine the results of ap...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Who Supported Obama in 2012?</td>\n",
       "      <td>We present a new solution to the ecological i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Real-Time Top-R Topic Detection on Twitter wit...</td>\n",
       "      <td>Twitter is a whats-happening-right-now tool t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Website Optimization Problem and Its Solutions</td>\n",
       "      <td>Online controlled experiments are widely used...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RSC: Mining and Modeling</td>\n",
       "      <td>Can we identify patterns of temporal activiti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Discovering and Exploiting Deterministic Label</td>\n",
       "      <td>This work presents a probabilistic method for...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Accelerated Alternating Direction Method of Mu...</td>\n",
       "      <td>Recent years have seen a revival of interest ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Dimensionality Reduction via Graph Structure L...</td>\n",
       "      <td>We present a new dimensionality reduction set...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Network Lasso: Clustering and Optimization in ...</td>\n",
       "      <td>Convex optimization is an essential tool for ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Timeline Generation for Knowledge-Base Entities</td>\n",
       "      <td>We present a method called TIMEMACHINE to ge...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A PCA-Based Change Detection Framework for</td>\n",
       "      <td>Detecting changes in multidimensional data st...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Using Local Spectral Methods to Robustify</td>\n",
       "      <td>Graph-based learning methods have a variety o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>On Estimating the Swapping Rate for Categorica...</td>\n",
       "      <td>it is important to account for all When analy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FaitCrowd: Fine Grained Truth Discovery for</td>\n",
       "      <td>In crowdsourced data aggregation task, there ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SCRAM: A Sharing Considered Route Assignment</td>\n",
       "      <td>Recommending routes for a group of competing ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Anatomical Annotations for Drosophila Gene Exp...</td>\n",
       "      <td>In Drosophila gene expression pattern researc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>On the Discovery of Evolving Truth</td>\n",
       "      <td>In the era of big data, information regarding...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Unied and Contrasting Cuts in Multiple Graphs:</td>\n",
       "      <td>The analysis of data represented as graphs is...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Spectral Ensemble Clustering</td>\n",
       "      <td>Ensemble clustering, also known as consensus ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Certifying and Removing Disparate Impact</td>\n",
       "      <td>What does it mean for an algorithm to be bias...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>A Decision Tree Framework for Spatiotemporal</td>\n",
       "      <td>We study the problem of learning to predict a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Data-Driven Activity Prediction: Algorithms, E...</td>\n",
       "      <td>We consider a novel problem called Activity P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Hierarchical Graph-Coupled HMMs for Heterogeneous</td>\n",
       "      <td>The purpose of this study is to leverage mode...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>MASCOT: Memory-efcient and Accurate Sampling for</td>\n",
       "      <td>How can we estimate local triangle counts acc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Learning Tree Structure in Multi-Task Learning</td>\n",
       "      <td>In multi-task learning (MTL), multiple relate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Extreme States Distribution Decomposition Meth...</td>\n",
       "      <td>Nowadays, the development of most leading web...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0-Bit Consistent Weighted Sampling</td>\n",
       "      <td>We1 develop 0-bit consistent weighted samplin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Non-exhaustive, Overlapping Clustering via</td>\n",
       "      <td>Clustering is one of the most fundamental tas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>A Clustering-Based Framework to Control</td>\n",
       "      <td>Entity resolution (ER) is a common data clean...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Online Inuence Maximization</td>\n",
       "      <td>Social networks are commonly used for marketi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Flexible and Robust Multi-Network Clustering</td>\n",
       "      <td>Integrating multiple graphs (or networks) has...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Learning with Similarity Functions on Graphs u...</td>\n",
       "      <td>We develop and apply the BalcanBlumSrebro (BB...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Maximum Likelihood Postprocessing for Differen...</td>\n",
       "      <td>When analyzing data that has been perturbed f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>ClusType: Effective Entity Recognition and Typ...</td>\n",
       "      <td>Entity recognition is an important but challe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Estimating Local Intrinsic Dimensionality</td>\n",
       "      <td>This paper is concerned with the estimation o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Adaptive Message Update for Fast Afnity Propag...</td>\n",
       "      <td>Anity Propagation is a clustering algorithm u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>The Child is Father of the Man: Foresee the Su...</td>\n",
       "      <td>Understanding the dynamic mechanisms that dri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Portraying Collective Spatial Attention in Twi...</td>\n",
       "      <td>Microblogging platforms such as Twitter have ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Temporal Phenotyping from Longitudinal Electro...</td>\n",
       "      <td>The rapid growth in the development of health...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Simultaneous Discovery of Common and Discrimin...</td>\n",
       "      <td>1.  INTRODUCTION  Understanding large-scale ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Dirichlet-Hawkes Processes with Applications to</td>\n",
       "      <td>Clusters in document streams, such as online ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Scalable Large Near-Clique Detection in Large-...</td>\n",
       "      <td>Extracting dense subgraphs from large graphs ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Towards Decision Support and Goal Achievement:</td>\n",
       "      <td>Every day, people take actions, trying to ach...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Virus Propagation in Multiple Prole Networks</td>\n",
       "      <td>Suppose we have a virus or one competing idea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Beyond Triangles: A Distributed Framework for ...</td>\n",
       "      <td>We study the problem of approximating the 3-p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Quick Sensitivity Analysis for Incremental</td>\n",
       "      <td>We introduce a novel sensitivity analysis fra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Inuence at Scale: Distributed Computation of C...</td>\n",
       "      <td>We consider the task of evaluating the spread...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Instance Weighting for Patient-Specic Risk Str...</td>\n",
       "      <td>Accurate risk models for adverse outcomes can...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Monitoring Least Squares Models of Distributed...</td>\n",
       "      <td>Least squares regression is widely used to un...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Robust Treecode Approximation for Kernel Machines</td>\n",
       "      <td>Since exact evaluation of a kernel matrix req...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0                  Locally Densest Subgraph Discovery   \n",
       "1                  Collective Opinion Spam Detection:   \n",
       "2                      Deep Computational Phenotyping   \n",
       "3         Reducing the Unlabeled Sample Complexity of   \n",
       "4   Non-transitive Hashing with Latent Similarity ...   \n",
       "5   State-Driven Dynamic Sensor Selection and Pred...   \n",
       "6   From Group to Individual Labels using Deep Fea...   \n",
       "7                 Inferring Networks of Substitutable   \n",
       "8            Graph Query Reformulation with Diversity   \n",
       "9    A Learning-based Framework to Handle Multi-round   \n",
       "10  Using TF-IDF to Determine Word Relevance in Do...   \n",
       "11                       Who Supported Obama in 2012?   \n",
       "12  Real-Time Top-R Topic Detection on Twitter wit...   \n",
       "13     Website Optimization Problem and Its Solutions   \n",
       "14                           RSC: Mining and Modeling   \n",
       "15     Discovering and Exploiting Deterministic Label   \n",
       "16  Accelerated Alternating Direction Method of Mu...   \n",
       "17  Dimensionality Reduction via Graph Structure L...   \n",
       "18  Network Lasso: Clustering and Optimization in ...   \n",
       "19    Timeline Generation for Knowledge-Base Entities   \n",
       "20         A PCA-Based Change Detection Framework for   \n",
       "21          Using Local Spectral Methods to Robustify   \n",
       "22  On Estimating the Swapping Rate for Categorica...   \n",
       "23        FaitCrowd: Fine Grained Truth Discovery for   \n",
       "24       SCRAM: A Sharing Considered Route Assignment   \n",
       "25  Anatomical Annotations for Drosophila Gene Exp...   \n",
       "26                 On the Discovery of Evolving Truth   \n",
       "27     Unied and Contrasting Cuts in Multiple Graphs:   \n",
       "28                       Spectral Ensemble Clustering   \n",
       "29           Certifying and Removing Disparate Impact   \n",
       "..                                                ...   \n",
       "53       A Decision Tree Framework for Spatiotemporal   \n",
       "54  Data-Driven Activity Prediction: Algorithms, E...   \n",
       "55  Hierarchical Graph-Coupled HMMs for Heterogeneous   \n",
       "56   MASCOT: Memory-efcient and Accurate Sampling for   \n",
       "57     Learning Tree Structure in Multi-Task Learning   \n",
       "58  Extreme States Distribution Decomposition Meth...   \n",
       "59                 0-Bit Consistent Weighted Sampling   \n",
       "60         Non-exhaustive, Overlapping Clustering via   \n",
       "61            A Clustering-Based Framework to Control   \n",
       "62                        Online Inuence Maximization   \n",
       "63       Flexible and Robust Multi-Network Clustering   \n",
       "64  Learning with Similarity Functions on Graphs u...   \n",
       "65  Maximum Likelihood Postprocessing for Differen...   \n",
       "66  ClusType: Effective Entity Recognition and Typ...   \n",
       "67          Estimating Local Intrinsic Dimensionality   \n",
       "68  Adaptive Message Update for Fast Afnity Propag...   \n",
       "69  The Child is Father of the Man: Foresee the Su...   \n",
       "70  Portraying Collective Spatial Attention in Twi...   \n",
       "71  Temporal Phenotyping from Longitudinal Electro...   \n",
       "72  Simultaneous Discovery of Common and Discrimin...   \n",
       "73    Dirichlet-Hawkes Processes with Applications to   \n",
       "74  Scalable Large Near-Clique Detection in Large-...   \n",
       "75     Towards Decision Support and Goal Achievement:   \n",
       "76       Virus Propagation in Multiple Prole Networks   \n",
       "77  Beyond Triangles: A Distributed Framework for ...   \n",
       "78         Quick Sensitivity Analysis for Incremental   \n",
       "79  Inuence at Scale: Distributed Computation of C...   \n",
       "80  Instance Weighting for Patient-Specic Risk Str...   \n",
       "81  Monitoring Least Squares Models of Distributed...   \n",
       "82  Robust Treecode Approximation for Kernel Machines   \n",
       "\n",
       "                                             abstract clusters  cluster  \n",
       "0    Mining dense subgraphs from a large graph is ...      NaN        2  \n",
       "1    Online reviews capture the testimonials of re...      NaN        4  \n",
       "2    We apply deep learning to the problem of disc...      NaN        3  \n",
       "3    In semi-supervised multi-view learning, unlab...      NaN        3  \n",
       "4    Approximating the semantic similarity between...      NaN        0  \n",
       "5    An important problem in large-scale sensor mi...      NaN        3  \n",
       "6    In many classication problems labels are rela...      NaN        3  \n",
       "7    In a modern recommender system, it is importa...      NaN        3  \n",
       "8    We study a problem of graph-query reformulati...      NaN        3  \n",
       "9    Considering nowadays companies providing simi...      NaN        0  \n",
       "10     In this paper, we examine the results of ap...      NaN        1  \n",
       "11   We present a new solution to the ecological i...      NaN        3  \n",
       "12   Twitter is a whats-happening-right-now tool t...      NaN        4  \n",
       "13   Online controlled experiments are widely used...      NaN        4  \n",
       "14   Can we identify patterns of temporal activiti...      NaN        1  \n",
       "15   This work presents a probabilistic method for...      NaN        3  \n",
       "16   Recent years have seen a revival of interest ...      NaN        4  \n",
       "17   We present a new dimensionality reduction set...      NaN        2  \n",
       "18   Convex optimization is an essential tool for ...      NaN        0  \n",
       "19    We present a method called TIMEMACHINE to ge...      NaN        4  \n",
       "20   Detecting changes in multidimensional data st...      NaN        1  \n",
       "21   Graph-based learning methods have a variety o...      NaN        2  \n",
       "22   it is important to account for all When analy...      NaN        1  \n",
       "23   In crowdsourced data aggregation task, there ...      NaN        1  \n",
       "24   Recommending routes for a group of competing ...      NaN        3  \n",
       "25   In Drosophila gene expression pattern researc...      NaN        3  \n",
       "26   In the era of big data, information regarding...      NaN        1  \n",
       "27   The analysis of data represented as graphs is...      NaN        2  \n",
       "28   Ensemble clustering, also known as consensus ...      NaN        1  \n",
       "29   What does it mean for an algorithm to be bias...      NaN        3  \n",
       "..                                                ...      ...      ...  \n",
       "53   We study the problem of learning to predict a...      NaN        3  \n",
       "54   We consider a novel problem called Activity P...      NaN        3  \n",
       "55   The purpose of this study is to leverage mode...      NaN        1  \n",
       "56   How can we estimate local triangle counts acc...      NaN        2  \n",
       "57   In multi-task learning (MTL), multiple relate...      NaN        3  \n",
       "58   Nowadays, the development of most leading web...      NaN        3  \n",
       "59   We1 develop 0-bit consistent weighted samplin...      NaN        1  \n",
       "60   Clustering is one of the most fundamental tas...      NaN        1  \n",
       "61   Entity resolution (ER) is a common data clean...      NaN        1  \n",
       "62   Social networks are commonly used for marketi...      NaN        4  \n",
       "63   Integrating multiple graphs (or networks) has...      NaN        0  \n",
       "64   We develop and apply the BalcanBlumSrebro (BB...      NaN        2  \n",
       "65   When analyzing data that has been perturbed f...      NaN        1  \n",
       "66   Entity recognition is an important but challe...      NaN        3  \n",
       "67   This paper is concerned with the estimation o...      NaN        1  \n",
       "68   Anity Propagation is a clustering algorithm u...      NaN        1  \n",
       "69   Understanding the dynamic mechanisms that dri...      NaN        1  \n",
       "70   Microblogging platforms such as Twitter have ...      NaN        4  \n",
       "71   The rapid growth in the development of health...      NaN        2  \n",
       "72    1.  INTRODUCTION  Understanding large-scale ...      NaN        1  \n",
       "73   Clusters in document streams, such as online ...      NaN        3  \n",
       "74   Extracting dense subgraphs from large graphs ...      NaN        2  \n",
       "75   Every day, people take actions, trying to ach...      NaN        3  \n",
       "76   Suppose we have a virus or one competing idea...      NaN        0  \n",
       "77   We study the problem of approximating the 3-p...      NaN        2  \n",
       "78   We introduce a novel sensitivity analysis fra...      NaN        3  \n",
       "79   We consider the task of evaluating the spread...      NaN        4  \n",
       "80   Accurate risk models for adverse outcomes can...      NaN        1  \n",
       "81   Least squares regression is widely used to un...      NaN        1  \n",
       "82   Since exact evaluation of a kernel matrix req...      NaN        4  \n",
       "\n",
       "[83 rows x 4 columns]"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "print len(km.labels_.tolist())\n",
    "frame['cluster'] = np.asarray(km.labels_.tolist())\n",
    "frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster\n",
       "0     8\n",
       "1    23\n",
       "2    10\n",
       "3    24\n",
       "4    18\n",
       "Name: abstract, dtype: int64"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.groupby('cluster').abstract.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: network, model, propose, graph, provide, problem, ing, method, work, many, algorithm, framework, data, however, subject,\n",
      "\n",
      "Cluster 0 titles:Locally Densest Subgraph Discovery\n",
      "\n",
      "Cluster 1 words: data, model, used, method, set, propose, based, task, provide, paper, has, problem, algorithm, approach, categories,\n",
      "\n",
      "Cluster 1 titles:Collective Opinion Spam Detection:\n",
      "\n",
      "Cluster 2 words: graph, data, new, used, problem, network, method, set, application, studies, algorithm, develop, large, analysis, time,\n",
      "\n",
      "Cluster 2 titles:Deep Computational Phenotyping\n",
      "\n",
      "Cluster 3 words: learning, prediction, approach, data, problem, make, used, resulting, method, model, based, framework, descriptors, propose, challenges,\n",
      "\n",
      "Cluster 3 titles:Reducing the Unlabeled Sample Complexity of\n",
      "\n",
      "Cluster 4 words: algorithm, method, users, performed, used, problem, information, datasets, approach, evaluate, existing, large, present, detection, propose,\n",
      "\n",
      "Cluster 4 titles:Non-transitive Hashing with Latent Similarity Components\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :15]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.ix[i]['title']:\n",
    "        print ('%s' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Collective Opinion Spam Detection:'"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in frame[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Locally Densest Subgraph Discovery</td>\n",
       "      <td>Mining dense subgraphs from a large graph is ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Collective Opinion Spam Detection:</td>\n",
       "      <td>Online reviews capture the testimonials of re...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep Computational Phenotyping</td>\n",
       "      <td>We apply deep learning to the problem of disc...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reducing the Unlabeled Sample Complexity of</td>\n",
       "      <td>In semi-supervised multi-view learning, unlab...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Non-transitive Hashing with Latent Similarity ...</td>\n",
       "      <td>Approximating the semantic similarity between...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>State-Driven Dynamic Sensor Selection and Pred...</td>\n",
       "      <td>An important problem in large-scale sensor mi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>From Group to Individual Labels using Deep Fea...</td>\n",
       "      <td>In many classication problems labels are rela...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Inferring Networks of Substitutable</td>\n",
       "      <td>In a modern recommender system, it is importa...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Graph Query Reformulation with Diversity</td>\n",
       "      <td>We study a problem of graph-query reformulati...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A Learning-based Framework to Handle Multi-round</td>\n",
       "      <td>Considering nowadays companies providing simi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Using TF-IDF to Determine Word Relevance in Do...</td>\n",
       "      <td>In this paper, we examine the results of ap...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Who Supported Obama in 2012?</td>\n",
       "      <td>We present a new solution to the ecological i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Real-Time Top-R Topic Detection on Twitter wit...</td>\n",
       "      <td>Twitter is a whats-happening-right-now tool t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Website Optimization Problem and Its Solutions</td>\n",
       "      <td>Online controlled experiments are widely used...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RSC: Mining and Modeling</td>\n",
       "      <td>Can we identify patterns of temporal activiti...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Discovering and Exploiting Deterministic Label</td>\n",
       "      <td>This work presents a probabilistic method for...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Accelerated Alternating Direction Method of Mu...</td>\n",
       "      <td>Recent years have seen a revival of interest ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Dimensionality Reduction via Graph Structure L...</td>\n",
       "      <td>We present a new dimensionality reduction set...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Network Lasso: Clustering and Optimization in ...</td>\n",
       "      <td>Convex optimization is an essential tool for ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Timeline Generation for Knowledge-Base Entities</td>\n",
       "      <td>We present a method called TIMEMACHINE to ge...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A PCA-Based Change Detection Framework for</td>\n",
       "      <td>Detecting changes in multidimensional data st...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Using Local Spectral Methods to Robustify</td>\n",
       "      <td>Graph-based learning methods have a variety o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>On Estimating the Swapping Rate for Categorica...</td>\n",
       "      <td>it is important to account for all When analy...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FaitCrowd: Fine Grained Truth Discovery for</td>\n",
       "      <td>In crowdsourced data aggregation task, there ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SCRAM: A Sharing Considered Route Assignment</td>\n",
       "      <td>Recommending routes for a group of competing ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Anatomical Annotations for Drosophila Gene Exp...</td>\n",
       "      <td>In Drosophila gene expression pattern researc...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>On the Discovery of Evolving Truth</td>\n",
       "      <td>In the era of big data, information regarding...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Unied and Contrasting Cuts in Multiple Graphs:</td>\n",
       "      <td>The analysis of data represented as graphs is...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Spectral Ensemble Clustering</td>\n",
       "      <td>Ensemble clustering, also known as consensus ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Certifying and Removing Disparate Impact</td>\n",
       "      <td>What does it mean for an algorithm to be bias...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>A Decision Tree Framework for Spatiotemporal</td>\n",
       "      <td>We study the problem of learning to predict a...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Data-Driven Activity Prediction: Algorithms, E...</td>\n",
       "      <td>We consider a novel problem called Activity P...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Hierarchical Graph-Coupled HMMs for Heterogeneous</td>\n",
       "      <td>The purpose of this study is to leverage mode...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>MASCOT: Memory-efcient and Accurate Sampling for</td>\n",
       "      <td>How can we estimate local triangle counts acc...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Learning Tree Structure in Multi-Task Learning</td>\n",
       "      <td>In multi-task learning (MTL), multiple relate...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Extreme States Distribution Decomposition Meth...</td>\n",
       "      <td>Nowadays, the development of most leading web...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0-Bit Consistent Weighted Sampling</td>\n",
       "      <td>We1 develop 0-bit consistent weighted samplin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Non-exhaustive, Overlapping Clustering via</td>\n",
       "      <td>Clustering is one of the most fundamental tas...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>A Clustering-Based Framework to Control</td>\n",
       "      <td>Entity resolution (ER) is a common data clean...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Online Inuence Maximization</td>\n",
       "      <td>Social networks are commonly used for marketi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Flexible and Robust Multi-Network Clustering</td>\n",
       "      <td>Integrating multiple graphs (or networks) has...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Learning with Similarity Functions on Graphs u...</td>\n",
       "      <td>We develop and apply the BalcanBlumSrebro (BB...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Maximum Likelihood Postprocessing for Differen...</td>\n",
       "      <td>When analyzing data that has been perturbed f...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>ClusType: Effective Entity Recognition and Typ...</td>\n",
       "      <td>Entity recognition is an important but challe...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Estimating Local Intrinsic Dimensionality</td>\n",
       "      <td>This paper is concerned with the estimation o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Adaptive Message Update for Fast Afnity Propag...</td>\n",
       "      <td>Anity Propagation is a clustering algorithm u...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>The Child is Father of the Man: Foresee the Su...</td>\n",
       "      <td>Understanding the dynamic mechanisms that dri...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Portraying Collective Spatial Attention in Twi...</td>\n",
       "      <td>Microblogging platforms such as Twitter have ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Temporal Phenotyping from Longitudinal Electro...</td>\n",
       "      <td>The rapid growth in the development of health...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Simultaneous Discovery of Common and Discrimin...</td>\n",
       "      <td>1.  INTRODUCTION  Understanding large-scale ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Dirichlet-Hawkes Processes with Applications to</td>\n",
       "      <td>Clusters in document streams, such as online ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Scalable Large Near-Clique Detection in Large-...</td>\n",
       "      <td>Extracting dense subgraphs from large graphs ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Towards Decision Support and Goal Achievement:</td>\n",
       "      <td>Every day, people take actions, trying to ach...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Virus Propagation in Multiple Prole Networks</td>\n",
       "      <td>Suppose we have a virus or one competing idea...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Beyond Triangles: A Distributed Framework for ...</td>\n",
       "      <td>We study the problem of approximating the 3-p...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Quick Sensitivity Analysis for Incremental</td>\n",
       "      <td>We introduce a novel sensitivity analysis fra...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Inuence at Scale: Distributed Computation of C...</td>\n",
       "      <td>We consider the task of evaluating the spread...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Instance Weighting for Patient-Specic Risk Str...</td>\n",
       "      <td>Accurate risk models for adverse outcomes can...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Monitoring Least Squares Models of Distributed...</td>\n",
       "      <td>Least squares regression is widely used to un...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Robust Treecode Approximation for Kernel Machines</td>\n",
       "      <td>Since exact evaluation of a kernel matrix req...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0                  Locally Densest Subgraph Discovery   \n",
       "1                  Collective Opinion Spam Detection:   \n",
       "2                      Deep Computational Phenotyping   \n",
       "3         Reducing the Unlabeled Sample Complexity of   \n",
       "4   Non-transitive Hashing with Latent Similarity ...   \n",
       "5   State-Driven Dynamic Sensor Selection and Pred...   \n",
       "6   From Group to Individual Labels using Deep Fea...   \n",
       "7                 Inferring Networks of Substitutable   \n",
       "8            Graph Query Reformulation with Diversity   \n",
       "9    A Learning-based Framework to Handle Multi-round   \n",
       "10  Using TF-IDF to Determine Word Relevance in Do...   \n",
       "11                       Who Supported Obama in 2012?   \n",
       "12  Real-Time Top-R Topic Detection on Twitter wit...   \n",
       "13     Website Optimization Problem and Its Solutions   \n",
       "14                           RSC: Mining and Modeling   \n",
       "15     Discovering and Exploiting Deterministic Label   \n",
       "16  Accelerated Alternating Direction Method of Mu...   \n",
       "17  Dimensionality Reduction via Graph Structure L...   \n",
       "18  Network Lasso: Clustering and Optimization in ...   \n",
       "19    Timeline Generation for Knowledge-Base Entities   \n",
       "20         A PCA-Based Change Detection Framework for   \n",
       "21          Using Local Spectral Methods to Robustify   \n",
       "22  On Estimating the Swapping Rate for Categorica...   \n",
       "23        FaitCrowd: Fine Grained Truth Discovery for   \n",
       "24       SCRAM: A Sharing Considered Route Assignment   \n",
       "25  Anatomical Annotations for Drosophila Gene Exp...   \n",
       "26                 On the Discovery of Evolving Truth   \n",
       "27     Unied and Contrasting Cuts in Multiple Graphs:   \n",
       "28                       Spectral Ensemble Clustering   \n",
       "29           Certifying and Removing Disparate Impact   \n",
       "..                                                ...   \n",
       "53       A Decision Tree Framework for Spatiotemporal   \n",
       "54  Data-Driven Activity Prediction: Algorithms, E...   \n",
       "55  Hierarchical Graph-Coupled HMMs for Heterogeneous   \n",
       "56   MASCOT: Memory-efcient and Accurate Sampling for   \n",
       "57     Learning Tree Structure in Multi-Task Learning   \n",
       "58  Extreme States Distribution Decomposition Meth...   \n",
       "59                 0-Bit Consistent Weighted Sampling   \n",
       "60         Non-exhaustive, Overlapping Clustering via   \n",
       "61            A Clustering-Based Framework to Control   \n",
       "62                        Online Inuence Maximization   \n",
       "63       Flexible and Robust Multi-Network Clustering   \n",
       "64  Learning with Similarity Functions on Graphs u...   \n",
       "65  Maximum Likelihood Postprocessing for Differen...   \n",
       "66  ClusType: Effective Entity Recognition and Typ...   \n",
       "67          Estimating Local Intrinsic Dimensionality   \n",
       "68  Adaptive Message Update for Fast Afnity Propag...   \n",
       "69  The Child is Father of the Man: Foresee the Su...   \n",
       "70  Portraying Collective Spatial Attention in Twi...   \n",
       "71  Temporal Phenotyping from Longitudinal Electro...   \n",
       "72  Simultaneous Discovery of Common and Discrimin...   \n",
       "73    Dirichlet-Hawkes Processes with Applications to   \n",
       "74  Scalable Large Near-Clique Detection in Large-...   \n",
       "75     Towards Decision Support and Goal Achievement:   \n",
       "76       Virus Propagation in Multiple Prole Networks   \n",
       "77  Beyond Triangles: A Distributed Framework for ...   \n",
       "78         Quick Sensitivity Analysis for Incremental   \n",
       "79  Inuence at Scale: Distributed Computation of C...   \n",
       "80  Instance Weighting for Patient-Specic Risk Str...   \n",
       "81  Monitoring Least Squares Models of Distributed...   \n",
       "82  Robust Treecode Approximation for Kernel Machines   \n",
       "\n",
       "                                             abstract clusters  \n",
       "0    Mining dense subgraphs from a large graph is ...      NaN  \n",
       "1    Online reviews capture the testimonials of re...      NaN  \n",
       "2    We apply deep learning to the problem of disc...      NaN  \n",
       "3    In semi-supervised multi-view learning, unlab...      NaN  \n",
       "4    Approximating the semantic similarity between...      NaN  \n",
       "5    An important problem in large-scale sensor mi...      NaN  \n",
       "6    In many classication problems labels are rela...      NaN  \n",
       "7    In a modern recommender system, it is importa...      NaN  \n",
       "8    We study a problem of graph-query reformulati...      NaN  \n",
       "9    Considering nowadays companies providing simi...      NaN  \n",
       "10     In this paper, we examine the results of ap...      NaN  \n",
       "11   We present a new solution to the ecological i...      NaN  \n",
       "12   Twitter is a whats-happening-right-now tool t...      NaN  \n",
       "13   Online controlled experiments are widely used...      NaN  \n",
       "14   Can we identify patterns of temporal activiti...      NaN  \n",
       "15   This work presents a probabilistic method for...      NaN  \n",
       "16   Recent years have seen a revival of interest ...      NaN  \n",
       "17   We present a new dimensionality reduction set...      NaN  \n",
       "18   Convex optimization is an essential tool for ...      NaN  \n",
       "19    We present a method called TIMEMACHINE to ge...      NaN  \n",
       "20   Detecting changes in multidimensional data st...      NaN  \n",
       "21   Graph-based learning methods have a variety o...      NaN  \n",
       "22   it is important to account for all When analy...      NaN  \n",
       "23   In crowdsourced data aggregation task, there ...      NaN  \n",
       "24   Recommending routes for a group of competing ...      NaN  \n",
       "25   In Drosophila gene expression pattern researc...      NaN  \n",
       "26   In the era of big data, information regarding...      NaN  \n",
       "27   The analysis of data represented as graphs is...      NaN  \n",
       "28   Ensemble clustering, also known as consensus ...      NaN  \n",
       "29   What does it mean for an algorithm to be bias...      NaN  \n",
       "..                                                ...      ...  \n",
       "53   We study the problem of learning to predict a...      NaN  \n",
       "54   We consider a novel problem called Activity P...      NaN  \n",
       "55   The purpose of this study is to leverage mode...      NaN  \n",
       "56   How can we estimate local triangle counts acc...      NaN  \n",
       "57   In multi-task learning (MTL), multiple relate...      NaN  \n",
       "58   Nowadays, the development of most leading web...      NaN  \n",
       "59   We1 develop 0-bit consistent weighted samplin...      NaN  \n",
       "60   Clustering is one of the most fundamental tas...      NaN  \n",
       "61   Entity resolution (ER) is a common data clean...      NaN  \n",
       "62   Social networks are commonly used for marketi...      NaN  \n",
       "63   Integrating multiple graphs (or networks) has...      NaN  \n",
       "64   We develop and apply the BalcanBlumSrebro (BB...      NaN  \n",
       "65   When analyzing data that has been perturbed f...      NaN  \n",
       "66   Entity recognition is an important but challe...      NaN  \n",
       "67   This paper is concerned with the estimation o...      NaN  \n",
       "68   Anity Propagation is a clustering algorithm u...      NaN  \n",
       "69   Understanding the dynamic mechanisms that dri...      NaN  \n",
       "70   Microblogging platforms such as Twitter have ...      NaN  \n",
       "71   The rapid growth in the development of health...      NaN  \n",
       "72    1.  INTRODUCTION  Understanding large-scale ...      NaN  \n",
       "73   Clusters in document streams, such as online ...      NaN  \n",
       "74   Extracting dense subgraphs from large graphs ...      NaN  \n",
       "75   Every day, people take actions, trying to ach...      NaN  \n",
       "76   Suppose we have a virus or one competing idea...      NaN  \n",
       "77   We study the problem of approximating the 3-p...      NaN  \n",
       "78   We introduce a novel sensitivity analysis fra...      NaN  \n",
       "79   We consider the task of evaluating the spread...      NaN  \n",
       "80   Accurate risk models for adverse outcomes can...      NaN  \n",
       "81   Least squares regression is widely used to un...      NaN  \n",
       "82   Since exact evaluation of a kernel matrix req...      NaN  \n",
       "\n",
       "[83 rows x 3 columns]"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd2015 = { 'title': titles, 'abstract': abstracts, 'cluster': clusters}\n",
    "\n",
    "frame = pd.DataFrame(kdd2015, columns = ['title', 'abstract', 'clusters'])\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEZCAYAAACXRVJOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZCSLeCMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cb28f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "K = range(1,80)\n",
    "meandisortions = []\n",
    "\n",
    "for k in K:\n",
    "\tkmeans = KMeans(n_clusters = k)\n",
    "\tkmeans.fit(tfidf_matrix.todense())\n",
    "\tmeandisortions.append(sum(np.min(cdist(tfidf_matrix.todense(),kmeans.cluster_centers_,'euclidean'),axis=1))/tfidf_matrix.todense().shape[0])\n",
    "\n",
    "plt.plot(K,meandisortions,'bx-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Average distortion')\n",
    "plt.title('Selecting k with the Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.11065384,  0.        ,  0.07155162, ...,  0.13057627,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.14205952,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.19460852,  0.19460852],\n",
       "        [ 0.        ,  0.        ,  0.18234073, ...,  0.11091933,\n",
       "          0.        ,  0.        ],\n",
       "        ..., \n",
       "        [ 0.        ,  0.        ,  0.08678731, ...,  0.36955391,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.06561024,  0.        ,  0.        , ...,  0.10323052,\n",
       "          0.        ,  0.08988   ],\n",
       "        [ 0.10952396,  0.30621089,  0.        , ...,  0.17232395,\n",
       "          0.        ,  0.15003777]])"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests, unicodedata\n",
    "text = urllib2.urlopen('http://www.constitution.org/cons/constitu.txt')\n",
    "constitution = requests.get('http://www.constitution.org/cons/constitu.txt')\n",
    "\n",
    "const = unicodedata.normalize('NFKD', constitution.text).encode('ascii','ignore')\n",
    "type(const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = ' '.join(re.findall('[\\w]+',const))\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
    "\n",
    "# read the mask / color image\n",
    "# taken from http://jirkavinse.deviantart.com/art/quot-Real-Life-quot-Alice-282261010\n",
    "from PIL import Image\n",
    "import requests\n",
    "from StringIO import StringIO\n",
    "\n",
    "response = requests.get(\"http://www.personal.psu.edu/sdh5174/Mario_png.png\")\n",
    "\n",
    "mario_coloring = np.array(Image.open(StringIO(response.content)))\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", max_words=2000, mask=mario_coloring,\n",
    "               stopwords=STOPWORDS.add(\"said\"),\n",
    "               max_font_size=40, random_state=42)\n",
    "# generate word cloud\n",
    "wc.generate(text)\n",
    "\n",
    "# create coloring from image\n",
    "image_colors = ImageColorGenerator(mario_coloring)\n",
    "\n",
    "# show\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "# recolor wordcloud and show\n",
    "# we could also give color_func=image_colors directly in the constructor\n",
    "plt.imshow(wc.recolor(color_func=image_colors))\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.imshow(mario_coloring, cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.datasets import cancer\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sm.datasets.get_rdataset(\"Duncan\", \"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "air = sm.datasets.get_rdataset(\"airquality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(air.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "air.data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
