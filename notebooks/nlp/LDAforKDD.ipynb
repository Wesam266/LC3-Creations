{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import nltk\n",
    "import time\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import urllib2\n",
    "import unicodedata\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tqdm\n",
    "\n",
    "wordnet_tags = ['n', 'v', 'a', 's', 'r']\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from http://brandonrose.org/clustering\n",
    "\n",
    "\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmer = WordNetLemmatizer()\n",
    "wordnet_tags = ['n', 'v', 'a', 's', 'r']\n",
    "\n",
    "def tokenize_and_stem_n_lem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    ''' \n",
    "    Old code gave seperate lists for lem and stem; I lemmed, then just stemmed the result\n",
    "    #stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    #lems = [lemmer.lemmatize(l) for l in filtered_tokens]\n",
    "    '''\n",
    "    stems = [stemmer.stem(lemmer.lemmatize(l)) for l in filtered_tokens]\n",
    "    \n",
    "    #return stems,lems\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#strip any proper names from a text...unfortunately right now this is yanking the first word from a sentence too.\n",
    "import string\n",
    "def strip_proppers(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word.islower()]\n",
    "    return \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#strip any proper nouns (NNP) or plural proper nouns (NNPS) from a text\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def strip_proppers_POS(text):\n",
    "    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n",
    "    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
    "    return non_propernouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['pdf2txt.py', '/Users/linwood/projects/LC3-Creations/examples/KDDsample/p1.pdf']' returned non-zero exit status 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-937d3518cd62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfileName\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileName\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfileName\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pdf2txt.py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTESTDIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NFKD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['pdf2txt.py', '/Users/linwood/projects/LC3-Creations/examples/KDDsample/p1.pdf']' returned non-zero exit status 1"
     ]
    }
   ],
   "source": [
    "path        = os.path.abspath(os.getcwd())\n",
    "TESTDIR     = os.path.normpath(os.path.join(os.path.expanduser(\"~\"),\"projects\",\"LC3-Creations\", \"examples\",\"KDDsample\"))\n",
    "\n",
    "corpus = {}\n",
    "\n",
    "for dirName, subdirList, fileList in walk(TESTDIR):    \n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            a = unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore')\n",
    "            document = unicodedata.normalize('NFKD', a).encode('ascii','ignore')\n",
    "\n",
    "            if len(document)<300:\n",
    "                pass\n",
    "            else:\n",
    "\n",
    "                # The entire document\n",
    "                body = re.sub('[\\s]',\" \",document)\n",
    "\n",
    "                # Getting title\n",
    "                title = re.findall(\"^[^\\\\n\\\\n]+\",document)[0]\n",
    "\n",
    "                # Getting the abstract\n",
    "                try:\n",
    "                    abstract = re.findall (r'(Abstract|ABSTRACT)([^]]*)\\n',document[:2000])\n",
    "                except IndexError:\n",
    "                    abstract = re.findall (r'(Abstract|ABSTRACT)([^]]*)\\n',document[:2000])[0]\n",
    "                else:\n",
    "                    abstract = abstract = re.findall (r'(Abstract|ABSTRACT)([^]]*)\\n',document[:2000])[0][1]\n",
    "\n",
    "                if isinstance(abstract, tuple):\n",
    "                    abstract = re.sub('[\\s]',\" \",abstract[1])\n",
    "                elif isinstance(abstract,list):\n",
    "                    abstract = re.sub('[\\s]',\" \",abstract[1])\n",
    "                elif isinstance(abstract,str):\n",
    "                    abstract = re.sub('[\\s]',\" \", abstract)\n",
    "\n",
    "\n",
    "                # Extracts section with names and email addresses only\n",
    "                section  = re.findall (r'\\n\\n([^]]*)\\n\\n(Abstract|ABSTRACT)',document[:2000])\n",
    "\n",
    "                type(section[0][0])\n",
    "\n",
    "                if isinstance(section, list):\n",
    "                    section = re.sub('[\\s]',\" \",section[0][0])\n",
    "                else:\n",
    "                    section = re.sub('[\\s]',\" \",section)\n",
    "\n",
    "\n",
    "                # Code to extract entities from top section of pdf and store a relationship tree\n",
    "                tagged = nltk.pos_tag(nltk.word_tokenize(section))\n",
    "                entities = nltk.chunk.ne_chunk(tagged)\n",
    "\n",
    "                # Another entity extractor\n",
    "                st = StanfordNERTagger('/Users/linwood/stanford-corenlp-full-2015-04-20/classifiers/english.conll.4class.distsim.crf.ser.gz',\n",
    "                       '/Users/linwood/stanford-corenlp-full-2015-04-20/stanford-corenlp-3.5.2.jar',\n",
    "                       encoding='utf-8')\n",
    "                tokenized_text = word_tokenize(section)\n",
    "                stanentities = st.tag(tokenized_text)\n",
    "\n",
    "                # Calls function to lemmatize and stem the document; stores the result\n",
    "                tokenize_and_stem_n_lem(abstract);\n",
    "\n",
    "                '''\n",
    "                This gives seperate lists for lem and stem; replacement code stores combin\n",
    "                # Creates the json document format to store the files\n",
    "                corpus[str(fileName)]={}\n",
    "                corpus[str(fileName)]={'Title':title,'Abstract':abstract,'Entities':entities, \n",
    "                                       \"Stanford ER\":stanentities, \"Stems\": tokenize_and_stem_n_lem(abstract)[0], \n",
    "                                      \"Lems\": tokenize_and_stem_n_lem(abstract)[1]}'''\n",
    "\n",
    "                # Creates the json document format to store the files\n",
    "                corpus[str(fileName)]={}\n",
    "                corpus[str(fileName)]={'Title':title,'body':body,'Abstract':abstract,'Entities':entities, \n",
    "                                       \"Stanford ER\":stanentities, \"Stems\": tokenize_and_stem_n_lem(abstract)}\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating lists for abstracts, titles, and entities\n",
    "\n",
    "abstracts = [i['Abstract'] for i in corpus.values()[:]]\n",
    "titles = [i['Title'] for i in corpus.values()[:]]\n",
    "ents = [i['Entities'] for i in corpus.values()[:]]\n",
    "bodies = [i['body'] for i in corpus.values()[:]]\n",
    "\n",
    "\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in bodies:\n",
    "    allwords_stemmed = tokenize_and_stem_n_lem(i) #for some reason, had to add index in list here\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    "stopwords = set([l for l in stop_words.ENGLISH_STOP_WORDS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 11s, sys: 1.71 s, total: 3min 13s\n",
      "Wall time: 3min 12s\n",
      "CPU times: user 2.07 s, sys: 96.9 ms, total: 2.16 s\n",
      "Wall time: 2.1 s\n",
      "CPU times: user 6.43 ms, sys: 35 µs, total: 6.47 ms\n",
      "Wall time: 6.5 ms\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities \n",
    "\n",
    "#remove proper names\n",
    "%time preprocess = [strip_proppers_POS(doc) for doc in bodies]\n",
    "\n",
    "#tokenize\n",
    "%time tokenized_text = [tokenize_and_stem_n_lem(text) for text in preprocess[0]]\n",
    "\n",
    "#remove stop words\n",
    "%time texts = [[word for word in text if word not in stopwords] for text in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a Gensim dictionary from the texts\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "#remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "#convert the dictionary to a bag of words corpus for reference\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 36s, sys: 537 ms, total: 5min 37s\n",
      "Wall time: 5min 37s\n"
     ]
    }
   ],
   "source": [
    "%time lda = models.LdaModel(corpus, num_topics=5, id2word=dictionary, update_every=5, chunksize=10000, passes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.196*subgraph + 0.118*graph + 0.100*densiti + 0.065*g + 0.047*dens + 0.036*cid:48 + 0.032*-compact + 0.031*result + 0.014*greedi + 0.012*propos',\n",
       " u'0.156*g + 0.065*node + 0.044*comput + 0.034*region + 0.030*nd + 0.023*base + 0.022*repres + 0.022*return + 0.020*follow + 0.020*core',\n",
       " u'0.094*v + 0.035*compon + 0.031*contain + 0.028*line + 0.026*optim + 0.025*residu + 0.023*test + 0.022*identi + 0.021*becaus + 0.019*dataset',\n",
       " u'0.089*algorithm + 0.070*densest + 0.038*ani + 0.032*edg + 0.030*model + 0.029*connect + 0.027*report + 0.026*larg + 0.024*remov + 0.021*invalid',\n",
       " u'0.064*maxim + 0.058*use + 0.044*local + 0.031*prune + 0.027*time + 0.027*sub- + 0.027*denit + 0.024*relat + 0.018*k + 0.017*u']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "top_topics() takes at least 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-4b04a74f83a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: top_topics() takes at least 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "lda.top_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subgraph', 'graph', 'densiti', 'g', 'dens', 'cid:48', '-compact', 'result', 'greedi', 'propos', 'sinc', 'area', 'approach', 'problem', 'number', 'prove', 'set', 'delet', 'veric', 'differ']\n",
      "['g', 'node', 'comput', 'region', 'nd', 'base', 'repres', 'return', 'follow', 'core', 'top-k', 'procedur', 'bound', 'lower', 'shown', 'origin', 'communiti', 'obtain', 'onli', 'case']\n",
      "['v', 'compon', 'contain', 'line', 'optim', 'residu', 'test', 'identi', 'becaus', 'dataset', 'identifi', 'high', 'denot', 'sever', 'lemma', 'denser', 'n', '|e', 'tend', 'true']\n",
      "['algorithm', 'densest', 'ani', 'edg', 'model', 'connect', 'report', 'larg', 'remov', 'invalid', 'network', 'properti', 'ha', 'size', 'larger', 'cid:100', 'cid:101', 'max', 'techniqu', 'con-']\n",
      "['maxim', 'use', 'local', 'prune', 'time', 'sub-', 'denit', 'relat', 'k', 'u', 'dene', 'nding', 'compact', 'studi', 'doe', 'smaller', 'maximum', 'exist', '1/|v', 'iter']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "topics_matrix = lda.show_topics(formatted=False, num_words=20)\n",
    "topics_matrix = np.array(topics_matrix)\n",
    "\n",
    "topic_words = topics_matrix[:,:,1]\n",
    "for i in topic_words:\n",
    "    print([str(word) for word in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
