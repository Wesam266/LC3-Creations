{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook works on extracting unique named entities and organizations from KDD papers and passing them lists.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import subprocess\n",
    "import unicodedata\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk import Tree\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import polyglot\n",
    "import string\n",
    "from emailextractor import file_to_str, get_emails # paste code to .py file from following link and save within your environment path to call it: https://gist.github.com/dideler/5219706\n",
    "\n",
    "'''\n",
    "Severl issues extended the time to complete this work\n",
    "that were not related to the coding itself.  I had two\n",
    "Anaconda distros installed on my computer.  This led\n",
    "to problems with importing modules because the paths\n",
    "for installation and retrieval were mixed.  I had to\n",
    "uninstall both anacondas, reinstall, and then recreate\n",
    "my virtual environment.  Imports and installs worked cleanly\n",
    "\n",
    "To create ipython shells in a virtual environment, use:\n",
    "http://stackoverflow.com/questions/30492623/using-both-python-2-x-and-python-3-x-in-ipython-notebook\n",
    "\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path        = os.path.abspath(os.getcwd())\n",
    "TESTDIR     = os.path.normpath(os.path.join(os.path.expanduser(\"~\"),\"projects\",\"LC3-Creations\", \"examples\",\"KDDsample\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "I experienced unicode problems early on.  Everytime I had an error, I scoured the internet for solutions. Here's the credit.\n",
    "\n",
    "\n",
    "\n",
    "- For Typeerror codes using subprocess to convert pdf2txt output to straight unicode --> http://stackoverflow.com/questions/33283603/python-popen-communicate-str-encodeencoding-utf-8-errors-ignore-cr\n",
    "- For problems with ASCII characters --> http://stackoverflow.com/questions/175240/how-do-i-convert-a-files-format-from-unicode-to-ascii-using-python\n",
    "- For unicode characters left in unicode converted to a string  --> http://stackoverflow.com/questions/8689795/how-can-i-remove-non-ascii-characters-but-leave-periods-and-spaces-using-python\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "a = unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,\"p1.pdf\")))]),errors='ignore')\n",
    "document = filter(lambda x: x in string.printable,unicodedata.normalize('NFKD', a).encode('ascii','ignore').decode('unicode_escape').encode('ascii','ignore'))\n",
    "lower = document.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online Controlled Experiments: \n",
      "\n",
      "Lessons from Running A/B/n Tests for 12 Years \n",
      "\n",
      " \n",
      "\n",
      "Ron Kohavi \n",
      "\n",
      "Distinguished Engineer & General Manager,  \n",
      "\n",
      "Analysis & Experimentation \n",
      "\n",
      "Microsoft \n",
      "\n",
      "Bellevue, WA, USA \n",
      "\n",
      "RonKohavi@outlook.com  \n",
      "\n",
      "in \n",
      "\n",
      "He \n",
      "\n",
      "2005 \n",
      "\n",
      " \n",
      "BIO \n",
      "Ronny  Kohavi is  a  Microsoft \n",
      "Distinguished  Engineer  and \n",
      "for \n",
      "the  General  Manager \n",
      "and \n",
      "Microsofts  Analysis \n",
      "at \n",
      "Experimentation \n",
      "team \n",
      "Microsoft. \n",
      "joined \n",
      "Microsoft \n",
      "and \n",
      "founded  the  Experimentation \n",
      "Platform  team  in  2006.  He \n",
      "was previously the director of \n",
      "data  mining  and  personalization  at  Amazon.com,  and  the  Vice \n",
      "President  of  Business  Intelligence  at  Blue  Martini  Software, \n",
      "which  went  public  in  2000,  and  later  acquired  by  Red  Prairie. \n",
      "Prior to joining Blue Martini, Kohavi managed MineSet project, \n",
      "Silicon  Graphics  award-winning  product  for  data  mining  and \n",
      "visualization. He joined Silicon Graphics after getting a Ph.D. in \n",
      "Machine  Learning  from  Stanford  University,  where  he  led  the \n",
      "MLC++  project,  the  Machine  Learning  library  in  C++  used  in \n",
      "MineSet and at Blue Martini Software. Kohavi received his BA \n",
      "from  the  Technion,  Israel.  He  was  the  General  Chair  for  KDD \n",
      "2004, co-chair of KDD 99s industrial track with Jim Gray, and \n",
      "co-chair of the KDD Cup 2000 with Carla Brodley. He was an \n",
      "invited speaker at the National Academy of Engineering in 2000, \n",
      "a keynote speaker at PAKDD 2001, an invited speaker at KDD \n",
      "2001s  industrial  track,  a  keynote  speaker  at  EC  2010  and  at \n",
      "Recsys 2012. His papers have over 26,000 citations and three of \n",
      "his  papers  are  in  the  top  1,000  most-cited  papers  in  Computer \n",
      "Science. \n",
      "\n",
      " \n",
      "\n",
      "Talk slides are available at: http://bit.ly/KDD2015Kohavi  \n",
      "\n",
      " \n",
      "Abstract \n",
      "The  Internet  provides  developers  of  connected  software, \n",
      "including web sites, applications, and devices, an unprecedented \n",
      "opportunity to accelerate innovation by evaluating ideas quickly \n",
      "and  accurately  using  trustworthy  controlled  experiments  (e.g., \n",
      "A/B  tests  and  their  generalizations).  From  front-end  user-\n",
      "interface  changes  to  backend  recommendation  systems  and \n",
      "relevance  algorithms,  from  search  engines  (e.g.,  Google, \n",
      "Microsofts  Bing,  Yahoo)  to  retailers  (e.g.,  Amazon,  eBay, \n",
      "Netflix,  Etsy)  to  social  networking  services  (e.g.,  Facebook, \n",
      "LinkedIn,  Twitter)  to  Travel  services  (e.g.,  Expedia,  Airbnb, \n",
      "Booking.com)  to  many  startups,  online  controlled  experiments \n",
      "are now utilized to make data-driven decisions at a wide range of \n",
      "companies.  While  the  theory  of  a  controlled  experiment  is \n",
      "simple, and dates back to Sir Ronald A. Fishers experiments at \n",
      "the Rothamsted Agricultural Experimental Station in England in \n",
      "the  1920s,  the  deployment  and  mining  of  online  controlled \n",
      "experiments at scale (e.g., hundreds of experiments run every day \n",
      "at Bing) and deployment of online controlled experiments across \n",
      "dozens of web sites and applications has taught us many lessons.  \n",
      "We  provide  an  introduction,  share  real  examples,  key  lessons, \n",
      "and cultural challenges. \n",
      "   \n",
      "Categories and Subject Descriptors \n",
      "G.3 Probability and Statistics/Experimental Design: controlled \n",
      "experiments; randomized experiments; A/B testing. \n",
      "General Terms \n",
      "Measurement; Design; Experimentation \n",
      " \n",
      "Keywords \n",
      "Controlled experiments; A/B testing; online experiments \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Permission to make digital or hard copies of part or all of this work for personal or \n",
      "classroom  use  is  granted  without  fee  provided  that  copies  are  not  made  or \n",
      "distributed for profit or commercial advantage, and that copies bear this notice and \n",
      "the full citation on the first page. Copyrights for third-party components of this work \n",
      "must be honored. For all other uses, contact the owner/author(s). Copyright is held \n",
      "by the author/owner(s). \n",
      "KDD15, August 1013, 2015, Sydney, NSW, Australia. \n",
      "ACM 978-1-4503-3664-2/15/08.. \n",
      "http://dx.doi.org/10.1145/2783258.2785464 \n",
      "\n",
      "1\f",
      "\n"
     ]
    }
   ],
   "source": [
    "print document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"returns named entity chunks in a given text\"\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(re.sub('[\\s]',\" \", document)))\n",
    "entities = nltk.chunk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online,PERSON\n",
      "Ron Kohavi Distinguished Engineer,PERSON\n",
      "General Manager,ORGANIZATION\n",
      "Analysis,PERSON\n",
      "Experimentation Microsoft Bellevue,ORGANIZATION\n",
      "WA,ORGANIZATION\n",
      "USA,ORGANIZATION\n",
      "BIO Ronny Kohavi,ORGANIZATION\n",
      "Microsoft Distinguished Engineer,ORGANIZATION\n",
      "General Manager,ORGANIZATION\n",
      "Microsofts Analysis,PERSON\n",
      "Experimentation,ORGANIZATION\n",
      "Microsoft,PERSON\n",
      "Microsoft,PERSON\n",
      "Experimentation Platform,ORGANIZATION\n",
      "Amazon.com,ORGANIZATION\n",
      "Business Intelligence,ORGANIZATION\n",
      "Red Prairie,PERSON\n",
      "Blue Martini,PERSON\n",
      "Kohavi,PERSON\n",
      "MineSet,ORGANIZATION\n",
      "Silicon Graphics,PERSON\n",
      "Silicon Graphics,PERSON\n",
      "Stanford University,ORGANIZATION\n",
      "Machine,ORGANIZATION\n",
      "MineSet,ORGANIZATION\n",
      "Kohavi,PERSON\n",
      "Technion,ORGANIZATION\n",
      "General,ORGANIZATION\n",
      "KDD,ORGANIZATION\n",
      "KDD,ORGANIZATION\n",
      "Jim Gray,PERSON\n",
      "KDD,ORGANIZATION\n",
      "Carla Brodley,PERSON\n",
      "National Academy,ORGANIZATION\n",
      "PAKDD,ORGANIZATION\n",
      "KDD,ORGANIZATION\n",
      "EC,ORGANIZATION\n",
      "Recsys,ORGANIZATION\n",
      "Computer Science,ORGANIZATION\n",
      "Microsofts Bing,PERSON\n",
      "eBay,ORGANIZATION\n",
      "Netflix,PERSON\n",
      "LinkedIn,ORGANIZATION\n",
      "Twitter,PERSON\n",
      "Sir Ronald,PERSON\n",
      "Rothamsted,ORGANIZATION\n",
      "Bing,ORGANIZATION\n",
      "Subject Descriptors,PERSON\n",
      "Terms Measurement,PERSON\n",
      "Experimentation Keywords Controlled,ORGANIZATION\n",
      "Copyright,PERSON\n",
      "NSW,ORGANIZATION\n",
      "ACM,ORGANIZATION\n"
     ]
    }
   ],
   "source": [
    "# putting it in similar formats for visuals\n",
    "for l in entities:\n",
    "    if isinstance(l,nltk.tree.Tree):\n",
    "        if l.label() == 'PERSON':\n",
    "            print \" \".join(map(itemgetter(0), l))+\",\"+l.label()\n",
    "    if isinstance(l,nltk.tree.Tree):\n",
    "        if l.label() == 'ORGANIZATION':\n",
    "            print \" \".join(map(itemgetter(0), l))+\",\"+l.label()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I established two lists to hold the values that I extract from the text.  This itemgetter function will check for unique values.  First, I iterate over the extracted entities and see if the objects is a nltk.tree.Tree with a \"Person\" label.  If it is, and the length is equal to 1 (first or last name only), I append that value to the list. If it's larger, I iterate of the entity tree and pull out the first value only using itemgetter.  Then, I join the values from the list and append it to the destination list.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a list out of NLTK's standard NE chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Online', 'Ron Kohavi Distinguished Engineer', 'Analysis', 'Microsofts Analysis', 'Microsoft', 'Red Prairie', 'Blue Martini', 'Kohavi', 'Silicon Graphics', 'Jim Gray', 'Carla Brodley', 'Microsofts Bing', 'Netflix', 'Twitter', 'Sir Ronald', 'Subject Descriptors', 'Terms Measurement', 'Copyright']\n",
      "\n",
      "\n",
      "['General Manager', 'Experimentation Microsoft Bellevue', 'WA', 'USA', 'BIO Ronny Kohavi', 'Microsoft Distinguished Engineer', 'Experimentation', 'Experimentation Platform', 'Amazon.com', 'Business Intelligence', 'MineSet', 'Stanford University', 'Machine', 'Technion', 'General', 'KDD', 'National Academy', 'PAKDD', 'EC', 'Recsys', 'Computer Science', 'eBay', 'LinkedIn', 'Rothamsted', 'Bing', 'Experimentation Keywords Controlled', 'NSW', 'ACM']\n",
      "\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "persons = []\n",
    "organizations = []\n",
    "locations =[]\n",
    "\n",
    "for l in entities:\n",
    "    if isinstance(l,nltk.tree.Tree):\n",
    "        if l.label() == 'PERSON':\n",
    "            if len(l)== 1:\n",
    "                if l[0][0] in persons:\n",
    "                    pass\n",
    "                else:\n",
    "                    persons.append(l[0][0])\n",
    "            else:\n",
    "                if \" \".join(map(itemgetter(0), l)) in persons:\n",
    "                    pass\n",
    "                else:\n",
    "                    persons.append(\" \".join(map(itemgetter(0), l)))\n",
    "                    \n",
    "for o in entities:\n",
    "    if isinstance(o,nltk.tree.Tree):\n",
    "        if o.label() == 'ORGANIZATION':\n",
    "            if len(o)== 1:\n",
    "                if o[0][0] in organizations:\n",
    "                    pass\n",
    "                else:\n",
    "                    organizations.append(o[0][0])\n",
    "            else:\n",
    "                if \" \".join(map(itemgetter(0), o)) in organizations:\n",
    "                    pass\n",
    "                else:\n",
    "                    organizations.append(\" \".join(map(itemgetter(0), o)))\n",
    "                    \n",
    "for o in entities:\n",
    "    if isinstance(o,nltk.tree.Tree):\n",
    "        if o.label() == 'LOCATION':\n",
    "            if len(o)== 1:\n",
    "                if o[0][0] in locations:\n",
    "                    pass\n",
    "                else:\n",
    "                    locations.append(o[0][0])\n",
    "            else:\n",
    "                if \" \".join(map(itemgetter(0), o)) in locations:\n",
    "                    pass\n",
    "                else:\n",
    "                    locations.append(\" \".join(map(itemgetter(0), o)))\n",
    "                    \n",
    "                \n",
    "print persons\n",
    "print\n",
    "print\n",
    "print organizations\n",
    "print\n",
    "print\n",
    "print locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Manager\n",
      "Experimentation Microsoft Bellevue\n",
      "BIO Ronny Kohavi\n",
      "Microsoft Distinguished Engineer\n",
      "General Manager\n",
      "Experimentation Platform\n",
      "Business Intelligence\n",
      "Stanford University\n",
      "National Academy\n",
      "Computer Science\n",
      "Experimentation Keywords Controlled\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "for o in entities:\n",
    "    if isinstance(o,nltk.tree.Tree):\n",
    "        if o.label() == 'ORGANIZATION' or o.label() == 'GPE':\n",
    "            if len(o)>1:\n",
    "                print \" \".join(map(itemgetter(0), o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to iterate over the extracted list of entities to get a better break between person's and their university name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = [nltk.word_tokenize(l) for l in persons]\n",
    "fin = [nltk.chunk.ne_chunk(nltk.pos_tag(l)) for l in tokens]\n",
    "fin;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating lists of named entities from Stanford's NER model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function looks though an extracted stanford ner list, and finds continuous entitiy labels.  This should create first name, last name records of entities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st = StanfordNERTagger('/Users/linwood/stanford-corenlp-full-2015-04-20/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "       '/Users/linwood/stanford-corenlp-full-2015-04-20/stanford-corenlp-3.5.2.jar',\n",
    "       encoding='utf-8')\n",
    "tokenized_text = word_tokenize(re.sub('[\\s]',\" \", document))\n",
    "stanentities = st.tag(tokenized_text)\n",
    "\n",
    "\n",
    "def get_continuous_chunks(tagged_sent):\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for token, tag in tagged_sent:\n",
    "        if tag != \"O\":\n",
    "            current_chunk.append((token, tag))\n",
    "        else:\n",
    "            if current_chunk: # if the current chunk is not empty\n",
    "                continuous_chunk.append(current_chunk)\n",
    "                current_chunk = []\n",
    "    # Flush the final current_chunk into the continuous_chunk, if any.\n",
    "    if current_chunk:\n",
    "        continuous_chunk.append(current_chunk)\n",
    "    return continuous_chunk\n",
    "\n",
    "ne_tagged_sent = [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION')]\n",
    "\n",
    "named_entities = get_continuous_chunks(stanentities)\n",
    "named_entities_str = [\" \".join([token for token, tag in ne]) for ne in named_entities]\n",
    "named_entities_str_tag = [(\" \".join([token for token, tag in ne]), ne[0][1]) for ne in named_entities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Years Ron Kohavi Distinguished Engineer & General Manager , Analysis & Experimentation Microsoft Bellevue',\n",
       "  u'ORGANIZATION'),\n",
       " (u'USA', u'ORGANIZATION'),\n",
       " (u'Ronny Kohavi', u'PERSON'),\n",
       " (u'Microsoft', u'ORGANIZATION'),\n",
       " (u'General Manager', u'ORGANIZATION'),\n",
       " (u'Experimentation', u'ORGANIZATION'),\n",
       " (u'Microsoft', u'ORGANIZATION'),\n",
       " (u'Microsoft', u'ORGANIZATION'),\n",
       " (u'2006', u'DATE'),\n",
       " (u'Amazon.com', u'ORGANIZATION'),\n",
       " (u'Blue Martini Software', u'ORGANIZATION'),\n",
       " (u'2000', u'DATE'),\n",
       " (u'Red Prairie', u'ORGANIZATION'),\n",
       " (u'Blue Martini', u'ORGANIZATION'),\n",
       " (u'Kohavi', u'PERSON'),\n",
       " (u'Silicon Graphics', u'ORGANIZATION'),\n",
       " (u'Machine Learning', u'ORGANIZATION'),\n",
       " (u'Stanford University', u'ORGANIZATION'),\n",
       " (u'Machine Learning', u'ORGANIZATION'),\n",
       " (u'Blue Martini Software', u'ORGANIZATION'),\n",
       " (u'BA', u'ORGANIZATION'),\n",
       " (u'Technion', u'LOCATION'),\n",
       " (u'Israel', u'LOCATION'),\n",
       " (u'KDD 2004', u'ORGANIZATION'),\n",
       " (u'KDD', u'ORGANIZATION'),\n",
       " (u'Jim Gray', u'PERSON'),\n",
       " (u'KDD', u'ORGANIZATION'),\n",
       " (u'2000', u'DATE'),\n",
       " (u'Carla Brodley', u'PERSON'),\n",
       " (u'National Academy of Engineering', u'ORGANIZATION'),\n",
       " (u'2000', u'DATE'),\n",
       " (u'2001', u'DATE'),\n",
       " (u'KDD', u'ORGANIZATION'),\n",
       " (u'EC', u'ORGANIZATION'),\n",
       " (u'2012', u'DATE'),\n",
       " (u'Computer Science', u'ORGANIZATION'),\n",
       " (u'Google', u'ORGANIZATION'),\n",
       " (u'Microsofts Bing', u'PERSON'),\n",
       " (u'Yahoo', u'ORGANIZATION'),\n",
       " (u'Amazon', u'LOCATION'),\n",
       " (u'Netflix', u'ORGANIZATION'),\n",
       " (u'Etsy', u'ORGANIZATION'),\n",
       " (u'Facebook', u'LOCATION'),\n",
       " (u'Expedia', u'ORGANIZATION'),\n",
       " (u'Airbnb', u'LOCATION'),\n",
       " (u'Ronald A. Fishers', u'PERSON'),\n",
       " (u'Rothamsted Agricultural Experimental Station', u'ORGANIZATION'),\n",
       " (u'England', u'LOCATION'),\n",
       " (u'1920s', u'DATE'),\n",
       " (u'August 1013', u'DATE'),\n",
       " (u'2015', u'DATE'),\n",
       " (u'Sydney', u'LOCATION'),\n",
       " (u'NSW', u'ORGANIZATION'),\n",
       " (u'Australia', u'LOCATION')]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_entities = get_continuous_chunks(stanentities)\n",
    "named_entities_str = [\" \".join([token for token, tag in ne]) for ne in named_entities]\n",
    "named_entities_str_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new =[word_tokenize(l) for l in persons]\n",
    "stan = [st.tag(l) for l in new]\n",
    "stan;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Ronny Kohavi',\n",
       " u'Kohavi',\n",
       " u'Jim Gray',\n",
       " u'Carla Brodley',\n",
       " u'Microsofts Bing',\n",
       " u'Ronald A. Fishers']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare=[]\n",
    "for l,m in named_entities_str_tag:\n",
    "    l=re.sub('(\\])',\" \",l).strip()\n",
    "    if m == 'PERSON':\n",
    "        if l in compare:\n",
    "            pass\n",
    "        else:\n",
    "            compare.append(l)\n",
    "    else:\n",
    "        pass\n",
    "compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list1 = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list2 = [i for i in xrange(7,17,1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{7, 8, 9}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list1) & set(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parts_of_speech(corpus):\n",
    "    \"returns named entity chunks in a given text\"\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(corpus))\n",
    "    entities = nltk.chunk.ne_chunk(tagged)\n",
    "    # Another entity extractor\n",
    "    st = StanfordNERTagger('/Users/linwood/stanford-corenlp-full-2015-04-20/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "           '/Users/linwood/stanford-corenlp-full-2015-04-20/stanford-corenlp-3.5.2.jar',\n",
    "           encoding='utf-8')\n",
    "    tokenized_text = word_tokenize(corpus)\n",
    "    stanentities = st.tag(tokenized_text)\n",
    "    return entities\n",
    "def find_entities(chunks):\n",
    "    \"given list of tagged parts of speech, returns unique named entities\"\n",
    "\n",
    "    def traverse(tree):\n",
    "        \"recursively traverses an nltk.tree.Tree to find named entities\"\n",
    "        entity_names = []\n",
    "    \n",
    "        if hasattr(tree, 'node') and tree.node:\n",
    "            if tree.node == 'NE':\n",
    "                entity_names.append(' '.join([child[0] for child in tree]))\n",
    "            else:\n",
    "                for child in tree:\n",
    "                    entity_names.extend(traverse(child))\n",
    "    \n",
    "        return entity_names\n",
    "    \n",
    "    named_entities = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        entities = sorted(list(set([word for tree in chunk\n",
    "                            for word in traverse(tree)])))\n",
    "        for e in entities:\n",
    "            if e not in named_entities:\n",
    "                named_entities.append(e)\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(u'Years', u'ORGANIZATION'),\n",
       "  (u'Ron', u'ORGANIZATION'),\n",
       "  (u'Kohavi', u'ORGANIZATION'),\n",
       "  (u'Distinguished', u'ORGANIZATION'),\n",
       "  (u'Engineer', u'ORGANIZATION'),\n",
       "  (u'&', u'ORGANIZATION'),\n",
       "  (u'General', u'ORGANIZATION'),\n",
       "  (u'Manager', u'ORGANIZATION'),\n",
       "  (u',', u'ORGANIZATION'),\n",
       "  (u'Analysis', u'ORGANIZATION'),\n",
       "  (u'&', u'ORGANIZATION'),\n",
       "  (u'Experimentation', u'ORGANIZATION'),\n",
       "  (u'Microsoft', u'ORGANIZATION'),\n",
       "  (u'Bellevue', u'ORGANIZATION')],\n",
       " [(u'USA', u'ORGANIZATION')],\n",
       " [(u'Ronny', u'PERSON'), (u'Kohavi', u'PERSON')],\n",
       " [(u'Microsoft', u'ORGANIZATION')],\n",
       " [(u'General', u'ORGANIZATION'), (u'Manager', u'ORGANIZATION')],\n",
       " [(u'Experimentation', u'ORGANIZATION')],\n",
       " [(u'Microsoft', u'ORGANIZATION')],\n",
       " [(u'Microsoft', u'ORGANIZATION')],\n",
       " [(u'2006', u'DATE')],\n",
       " [(u'Amazon.com', u'ORGANIZATION')],\n",
       " [(u'Blue', u'ORGANIZATION'),\n",
       "  (u'Martini', u'ORGANIZATION'),\n",
       "  (u'Software', u'ORGANIZATION')],\n",
       " [(u'2000', u'DATE')],\n",
       " [(u'Red', u'ORGANIZATION'), (u'Prairie', u'ORGANIZATION')],\n",
       " [(u'Blue', u'ORGANIZATION'), (u'Martini', u'ORGANIZATION')],\n",
       " [(u'Kohavi', u'PERSON')],\n",
       " [(u'Silicon', u'ORGANIZATION'), (u'Graphics', u'ORGANIZATION')],\n",
       " [(u'Machine', u'ORGANIZATION'), (u'Learning', u'ORGANIZATION')],\n",
       " [(u'Stanford', u'ORGANIZATION'), (u'University', u'ORGANIZATION')],\n",
       " [(u'Machine', u'ORGANIZATION'), (u'Learning', u'ORGANIZATION')],\n",
       " [(u'Blue', u'ORGANIZATION'),\n",
       "  (u'Martini', u'ORGANIZATION'),\n",
       "  (u'Software', u'ORGANIZATION')],\n",
       " [(u'BA', u'ORGANIZATION')],\n",
       " [(u'Technion', u'LOCATION')],\n",
       " [(u'Israel', u'LOCATION')],\n",
       " [(u'KDD', u'ORGANIZATION'), (u'2004', u'DATE')],\n",
       " [(u'KDD', u'ORGANIZATION')],\n",
       " [(u'Jim', u'PERSON'), (u'Gray', u'PERSON')],\n",
       " [(u'KDD', u'ORGANIZATION')],\n",
       " [(u'2000', u'DATE')],\n",
       " [(u'Carla', u'PERSON'), (u'Brodley', u'PERSON')],\n",
       " [(u'National', u'ORGANIZATION'),\n",
       "  (u'Academy', u'ORGANIZATION'),\n",
       "  (u'of', u'ORGANIZATION'),\n",
       "  (u'Engineering', u'ORGANIZATION')],\n",
       " [(u'2000', u'DATE')],\n",
       " [(u'2001', u'DATE')],\n",
       " [(u'KDD', u'ORGANIZATION')],\n",
       " [(u'EC', u'ORGANIZATION')],\n",
       " [(u'2012', u'DATE')],\n",
       " [(u'Computer', u'ORGANIZATION'), (u'Science', u'ORGANIZATION')],\n",
       " [(u'Google', u'ORGANIZATION')],\n",
       " [(u'Microsofts', u'PERSON'), (u'Bing', u'PERSON')],\n",
       " [(u'Yahoo', u'ORGANIZATION')],\n",
       " [(u'Amazon', u'LOCATION')],\n",
       " [(u'Netflix', u'ORGANIZATION')],\n",
       " [(u'Etsy', u'ORGANIZATION')],\n",
       " [(u'Facebook', u'LOCATION')],\n",
       " [(u'Expedia', u'ORGANIZATION')],\n",
       " [(u'Airbnb', u'LOCATION')],\n",
       " [(u'Ronald', u'PERSON'), (u'A.', u'PERSON'), (u'Fishers', u'PERSON')],\n",
       " [(u'Rothamsted', u'ORGANIZATION'),\n",
       "  (u'Agricultural', u'ORGANIZATION'),\n",
       "  (u'Experimental', u'ORGANIZATION'),\n",
       "  (u'Station', u'ORGANIZATION')],\n",
       " [(u'England', u'LOCATION')],\n",
       " [(u'1920s', u'DATE')],\n",
       " [(u'August', u'DATE'), (u'1013', u'DATE')],\n",
       " [(u'2015', u'DATE')],\n",
       " [(u'Sydney', u'LOCATION')],\n",
       " [(u'NSW', u'ORGANIZATION')],\n",
       " [(u'Australia', u'LOCATION')]]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting entities and creating lists using Polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from polyglot.text import Text\n",
    "e=Text(re.sub('[\\s]',\" \",document[:10000])).entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code iterates over the polyglot extracted entities and creates a list of person, locations, and organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import unicodedata\n",
    "\n",
    "def extraction(corpus):\n",
    "    \n",
    "    # extract entities from a single string; remove whitespace characters\n",
    "    try:\n",
    "        e = Text(re.sub('[\\s]',\" \",corpus)).entities\n",
    "    except:\n",
    "        pass #e = Text(re.sub(\"(r'(x0)',\" \",\"(re.sub('[\\s]',\" \",corpus)))).entities\n",
    "    \n",
    "    current_person =[]\n",
    "    persons =[]\n",
    "    current_org=[]\n",
    "    organizations=[]\n",
    "    current_loc=[]\n",
    "    locations=[]\n",
    "\n",
    "    for l in e:\n",
    "        if l.tag == 'I-PER':\n",
    "            for m in l:\n",
    "                current_person.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_person: # if the current chunk is not empty\n",
    "                        persons.append(\" \".join(current_person))\n",
    "                        current_person = []\n",
    "        elif l.tag == 'I-ORG':\n",
    "            for m in l:\n",
    "                current_org.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_org: # if the current chunk is not empty\n",
    "                        organizations.append(\" \".join(current_org))\n",
    "                        current_org = []\n",
    "        elif l.tag == 'I-LOC':\n",
    "            for m in l:\n",
    "                current_loc.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_loc: # if the current chunk is not empty\n",
    "                        locations.append(\" \".join(current_loc))\n",
    "                        current_loc = []\n",
    "    results = {}\n",
    "    results['persons']=persons\n",
    "    results['organizations']=organizations\n",
    "    results['locations']=locations\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ron Kohavi',\n",
       " 'Ronny Kohavi',\n",
       " 'Blue Martini',\n",
       " 'Kohavi',\n",
       " 'Kohavi',\n",
       " 'Jim Gray',\n",
       " 'Carla Brodley',\n",
       " 'Microsofts Bing',\n",
       " 'Sir Ronald A',\n",
       " 'KDD15']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction(document)['persons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truth Sets to test extraction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 authors\n",
      "\n",
      "There are 3 author organizations\n",
      "\n",
      "There are 7 author locations\n",
      "\n",
      "There are 152 authors in the references\n"
     ]
    }
   ],
   "source": [
    "#p19.pdf\n",
    "\n",
    "p19pdf_authors=['Tim Althoff*','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang']\n",
    "p19pdf_author_organizations=['Computer Science Department','Stanford University','Google']\n",
    "p19pdf_author_locations=['Stanford, CA','Stanford','CA','Google','1600 Amphitheatre Parkway, Mountain View, CA 94043','1600 Amphitheatre Parkway','Mountain View']\n",
    "\n",
    "p19pdf_references_authors =['A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']\n",
    "p19pdf_allauthors=['Tim Althoff*','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang','A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']\n",
    "\n",
    "print \"There are %r authors\" % len(p19pdf_authors)\n",
    "print  # white space\n",
    "print \"There are %r author organizations\" %len(p19pdf_author_organizations)\n",
    "print \n",
    "print \"There are %r author locations\" % len(p19pdf_author_locations)\n",
    "print  \n",
    "print \"There are %r authors in the references\" %len(p19pdf_references_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 authors\n",
      "\n",
      "There are 6 author organizations\n",
      "\n",
      "There are 8 author locations\n",
      "\n",
      "There are 106 authors in the references\n"
     ]
    }
   ],
   "source": [
    "#p29.pdf\n",
    "\n",
    "p29pdf_authors=['Laurent Amsaleg','Stéphane Girard','Oussama Chelly','Teddy Furon','Michael E. Houle','Ken-ichi Kawarabayashi',\n",
    "               'Michael Nett']\n",
    "p29pdf_author_organizations=['Equipe LINKMEDIA','Campus Universitaire de Beaulieu','CNRS/IRISA Rennes','National Institute of Informatics',\n",
    "                             'Equipe MISTIS INRIA','Google']\n",
    "p29pdf_author_locations=['Campus Universitaire de Beaulieu','35042 Rennes Cedex, France','France','-1-2 Hitotsubashi, Chiyoda-ku Tokyo 101-8430, Japan',\n",
    "                        'Japan','6-10-1 Roppongi, Minato-ku Tokyo 106-6126','Inovallée, 655, Montbonnot 38334 Saint-Ismier Cedex','Tokyo']\n",
    "\n",
    "p29pdf_references_authors =['A. A. Balkema','L. de Haan','N. Bingham', 'C. Goldie','J. Teugels','N. Boujemaa', 'J. Fauqueur', 'M. Ferecatu', 'F. Fleuret',\n",
    "                            'V. Gouet', 'B. LeSaux','H. Sahbi','C. Bouveyron', 'G. Celeux', 'S. Girard','J. Bruske', 'G. Sommer',\n",
    "                           'F. Camastra','A. Vinciarelli','S. Coles','J. Costa' ,'A. Hero','T. de Vries', 'S. Chawla','M. E. Houle',\n",
    "                           'R. A. Fisher','L. H. C. Tippett','M. I. Fraga Alves', 'L. de Haan','T. Lin','M. I. Fraga Alves', 'M. I. Gomes','L. de Haan',\n",
    "                           'B. V. Gnedenko',' A. Gupta', 'R. Krauthgamer','J. R. Lee','A. Gupta', 'R. Krauthgamer','J. R. Lee','M. Hein','J.-Y. Audibert',\n",
    "                           'B. M. Hill','M. E. Houle','M. E. Houle','M. E. Houle','M. E. Houle', 'H. Kashima', 'M. Nett','M. E. Houle', 'X. Ma', 'M. Nett',\n",
    "                            'V. Oria','M. E. Houle', 'X. Ma', 'V. Oria','J. Sun','M. E. Houle','M. Nett','H. Jegou', 'R. Tavenard', 'M. Douze','L. Amsaleg',\n",
    "                           'I. Jollie','D. R. Karger','M. Ruhl','J. Karhunen','J. Joutsensalo','Y. LeCun', 'L. Bottou', 'Y. Bengio', 'P. Haner',\n",
    "                           'J. Pickands, III','C. R. Rao','S. T. Roweis','L. K. Saul','A. Rozza', 'G. Lombardi', 'C. Ceruti', 'E. Casiraghi', 'P. Campadelli',\n",
    "                           'B. Scholkopf', 'A. J. Smola','K.-R. Muller','U. Shaft','R. Ramakrishnan',' F. Takens','J. Tenenbaum', 'V. D. Silva','J. Langford',\n",
    "                           'J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. Venna','S. Kaski',\n",
    "                           'P. Verveer','R. Duin','J. von Brunken', 'M. E. Houle', 'A. Zimek','J. von Brunken', 'M. E. Houle','A. Zimek']\n",
    "\n",
    "p29pdf_allauthors=['Laurent Amsaleg','Stéphane Girard','Oussama Chelly','Teddy Furon','Michael E. Houle','Ken-ichi Kawarabayashi',\n",
    "               'Michael Nett','A. A. Balkema','L. de Haan','N. Bingham', 'C. Goldie','J. Teugels','N. Boujemaa', 'J. Fauqueur', 'M. Ferecatu', 'F. Fleuret',\n",
    "                            'V. Gouet', 'B. LeSaux','H. Sahbi','C. Bouveyron', 'G. Celeux', 'S. Girard','J. Bruske', 'G. Sommer',\n",
    "                           'F. Camastra','A. Vinciarelli','S. Coles','J. Costa' ,'A. Hero','T. de Vries', 'S. Chawla','M. E. Houle',\n",
    "                           'R. A. Fisher','L. H. C. Tippett','M. I. Fraga Alves', 'L. de Haan','T. Lin','M. I. Fraga Alves', 'M. I. Gomes','L. de Haan',\n",
    "                           'B. V. Gnedenko',' A. Gupta', 'R. Krauthgamer','J. R. Lee','A. Gupta', 'R. Krauthgamer','J. R. Lee','M. Hein','J.-Y. Audibert',\n",
    "                           'B. M. Hill','M. E. Houle','M. E. Houle','M. E. Houle','M. E. Houle', 'H. Kashima', 'M. Nett','M. E. Houle', 'X. Ma', 'M. Nett',\n",
    "                            'V. Oria','M. E. Houle', 'X. Ma', 'V. Oria','J. Sun','M. E. Houle','M. Nett','H. Jegou', 'R. Tavenard', 'M. Douze','L. Amsaleg',\n",
    "                           'I. Jollie','D. R. Karger','M. Ruhl','J. Karhunen','J. Joutsensalo','Y. LeCun', 'L. Bottou', 'Y. Bengio', 'P. Haner',\n",
    "                           'J. Pickands, III','C. R. Rao','S. T. Roweis','L. K. Saul','A. Rozza', 'G. Lombardi', 'C. Ceruti', 'E. Casiraghi', 'P. Campadelli',\n",
    "                           'B. Scholkopf', 'A. J. Smola','K.-R. Muller','U. Shaft','R. Ramakrishnan',' F. Takens','J. Tenenbaum', 'V. D. Silva','J. Langford',\n",
    "                           'J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. Venna','S. Kaski',\n",
    "                           'P. Verveer','R. Duin','J. von Brunken', 'M. E. Houle', 'A. Zimek','J. von Brunken', 'M. E. Houle','A. Zimek']\n",
    "\n",
    "\n",
    "print \"There are %r authors\" % len(p29pdf_authors)\n",
    "print  # white space\n",
    "print \"There are %r author organizations\" %len(p29pdf_author_organizations)\n",
    "print \n",
    "print \"There are %r author locations\" % len(p29pdf_author_locations)\n",
    "print  \n",
    "print \"There are %r authors in the references\" %len(p29pdf_references_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(compare) & set(p29pdf_allauthors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p29pdf_allauthors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[\\s]',\" \",document)[9300:10500]\n",
    "#regexp.search(re.sub('[\\s]',\" \",document)).group(1)[4900:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'locations': ['Microsoft Bellevue',\n",
       "  'WA',\n",
       "  'USA',\n",
       "  'Red Prairie',\n",
       "  'Israel',\n",
       "  'England',\n",
       "  'Sydney',\n",
       "  'NSW',\n",
       "  'Australia'],\n",
       " 'organizations': ['Microsoft',\n",
       "  'Microsoft',\n",
       "  'Microsoft',\n",
       "  'Business Intelligence',\n",
       "  'Blue Martini Software',\n",
       "  'Blue',\n",
       "  'Silicon Graphics',\n",
       "  'Graphics',\n",
       "  'Stanford University',\n",
       "  'Blue Martini Software',\n",
       "  'Technion',\n",
       "  'National Academy of Engineering',\n",
       "  'Computer Science',\n",
       "  'Google',\n",
       "  'Yahoo',\n",
       "  'eBay',\n",
       "  'Netflix',\n",
       "  'Facebook',\n",
       "  'ACM'],\n",
       " 'persons': ['Ron Kohavi',\n",
       "  'Ronny Kohavi',\n",
       "  'Blue Martini',\n",
       "  'Kohavi',\n",
       "  'Kohavi',\n",
       "  'Jim Gray',\n",
       "  'Carla Brodley',\n",
       "  'Microsofts Bing',\n",
       "  'Sir Ronald A',\n",
       "  'KDD15']}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ron Kohavi',\n",
       " 'Ronny Kohavi',\n",
       " 'Blue Martini',\n",
       " 'Kohavi',\n",
       " 'Kohavi',\n",
       " 'Jim Gray',\n",
       " 'Carla Brodley',\n",
       " 'Microsofts Bing',\n",
       " 'Sir Ronald A',\n",
       " 'KDD15']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class entities(object):\n",
    "  def __init__(self):\n",
    "    self.persons = extraction(document)['persons']\n",
    "    self.organizations = extraction(document)['organizations']\n",
    "\n",
    "my_shape = entities()\n",
    "my_shape.persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to extract emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ohavi@outlook.com',)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(get_emails(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get only the Title and Author Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'online controlled experiments:   lessons from running a/b/n tests for 12 years      ron kohavi   distinguished engineer & general manager,    analysis & experimentation   microsoft   bellevue, wa, usa   ronkohavi@outlook.com    in   he   2005     bio  ronny  kohavi is  a  microsoft  distinguished  engineer  and  for  the  general  manager  and  microsofts  analysis  at  experimentation  team  microsoft.  joined  microsoft  and  founded  the  experimentation  platform  team  in  2006.  he  was previously the director of  data  mining  and  personalization  at  amazon.com,  and  the  vice  president  of  business  intelligence  at  blue  martini  software,  which  went  public  in  2000,  and  later  acquired  by  red  prairie.  prior to joining blue martini, kohavi managed mineset project,  silicon  graphics  award-winning  product  for  data  mining  and  visualization. he joined silicon graphics after getting a ph.d. in  machine  learning  from  stanford  university,  where  he  led  the  mlc++  project,  the  machine  learning  library  in  c++  used  in  mineset and at blue martini software. kohavi received his ba  from  the  technion,  israel.  he  was  the  general  chair  for  kdd  2004, co-chair of kdd 99s industrial track with jim gray, and  co-chair of the kdd cup 2000 with carla brodley. he was an  invited speaker at the national academy of engineering in 2000,  a keynote speaker at pakdd 2001, an invited speaker at kdd  2001s  industrial  track,  a  keynote  speaker  at  ec  2010  and  at  recsys 2012. his papers have over 26,000 citations and three of  his  papers  are  in  the  top  1,000  most-cited  papers  in  computer  science.      talk slides are available at: http://bit.ly/kdd2015kohavi      '"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=re.compile('(.*)(?=ABSTRACT)')\n",
    "try:\n",
    "    topsec = p.search(re.sub('[\\s]',\" \",document)).group(1)\n",
    "except AttributeError:\n",
    "    p=re.compile('(.*)(?=abstract)')\n",
    "    topsec = p.search(re.sub('[\\s]',\" \",lower)).group(1)\n",
    "topsec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get Title Only (or most of it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Online Controlled Experiments: '"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=re.compile('(.+)(\\\\n\\\\n)')\n",
    "q=re.compile('(?<=\\\\n\\\\n)(.+?)(?=\\\\n\\\\n)')\n",
    "#p.search(document).group(1)+\" \"+q.search(document).group(1)\n",
    "p.search(document).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get Abstract only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  the  internet  provides  developers  of  connected  software,  including web sites, applications, and devices, an unprecedented  opportunity to accelerate innovation by evaluating ideas quickly  and  accurately  using  trustworthy  controlled  experiments  (e.g.,  a/b  tests  and  their  generalizations).  from  front-end  user- interface  changes  to  backend  recommendation  systems  and  relevance  algorithms,  from  search  engines  (e.g.,  google,  microsofts  bing,  yahoo)  to  retailers  (e.g.,  amazon,  ebay,  netflix,  etsy)  to  social  networking  services  (e.g.,  facebook,  linkedin,  twitter)  to  travel  services  (e.g.,  expedia,  airbnb,  booking.com)  to  many  startups,  online  controlled  experiments  are now utilized to make data-driven decisions at a wide range of  companies.  while  the  theory  of  a  controlled  experiment  is  simple, and dates back to sir ronald a. fishers experiments at  the rothamsted agricultural experimental station in england in  the  1920s,  the  deployment  and  mining  of  online  controlled  experiments at scale (e.g., hundreds of experiments run every day  at bing) and deployment of online controlled experiments across  dozens of web sites and applications has taught us many lessons.   we  provide  an  introduction,  share  real  examples,  key  lessons,  and cultural challenges.      '"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= re.compile('(?<=ABSTRACT)(.+)(?=Categories and Subject Descriptors)')\n",
    "try:\n",
    "    p.search(re.sub('[\\s]',\" \",document)).group(1)\n",
    "except AttributeError:\n",
    "    p=re.compile('(?<=abstract)(.+)(?=categories and subject descriptors)')\n",
    "    abstract=p.search(re.sub('[\\s]',\" \",lower)).group(1)\n",
    "else:\n",
    "    pass\n",
    "abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get keywords only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file does not have a keywords section or the section headers are different\n"
     ]
    }
   ],
   "source": [
    "p= re.compile('(?<=Keywords)(.+)(?=INTRODUCTION)')\n",
    "try:\n",
    "    keywords=p.search(re.sub('[\\s]',\" \",document)).group(1)\n",
    "    try:\n",
    "        p=re.compile('(?<=Keywords)(.+)')\n",
    "        keywords=p.search(re.sub('[\\s]',\" \",document)).group(1)\n",
    "    \n",
    "    except AttributeError:\n",
    "        print \"Unexpected error:\", sys.exc_info()[0]\n",
    "except:\n",
    "    print \"This file does not have a keywords section or the section headers are different\"\n",
    "    pass\n",
    "keywords;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['keywords ']"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('(?=keywords)(.+)',lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  the  internet  provides  developers  of  connected  software,  including web sites, applications, and devices, an unprecedented  opportunity to accelerate innovation by evaluating ideas quickly  and  accurately  using  trustworthy  controlled  experiments  (e.g.,  a/b  tests  and  their  generalizations).  from  front-end  user- interface  changes  to  backend  recommendation  systems  and  relevance  algorithms,  from  search  engines  (e.g.,  google,  microsofts  bing,  yahoo)  to  retailers  (e.g.,  amazon,  ebay,  netflix,  etsy)  to  social  networking  services  (e.g.,  facebook,  linkedin,  twitter)  to  travel  services  (e.g.,  expedia,  airbnb,  booking.com)  to  many  startups,  online  controlled  experiments  are now utilized to make data-driven decisions at a wide range of  companies.  while  the  theory  of  a  controlled  experiment  is  simple, and dates back to sir ronald a. fishers experiments at  the rothamsted agricultural experimental station in england in  the  1920s,  the  deployment  and  mining  of  online  controlled  experiments at scale (e.g., hundreds of experiments run every day  at bing) and deployment of online controlled experiments across  dozens of web sites and applications has taught us many lessons.   we  provide  an  introduction,  share  real  examples,  key  lessons,  and cultural challenges.      '"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= re.compile('(?<=ABSTRACT)(.+)(?=Categories and Subject Descriptors)')\n",
    "try:\n",
    "    abstract = p.search(re.sub('[\\s]',\" \",document)).group(1)\n",
    "except AttributeError:\n",
    "    p=re.compile('(?<=abstract)(.+)(?=categories and subject descriptors)')\n",
    "    abstract = p.search(re.sub('[\\s]',\" \",lower)).group(1)\n",
    "else:\n",
    "    pass\n",
    "abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get Categories and Subject Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  G.3 Probability and Statistics/Experimental Design: controlled  experiments; randomized experiments; A/B testing.  General Terms  Measurement; Design; Experimentation    '"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= re.compile('(?<=Categories and Subject Descriptors)(.+)(?=Keywords)')\n",
    "try:\n",
    "    catnsub = re.findall('\\[(.*?)\\]',p.search(re.sub('[\\s]',\" \",document)).group(1))\n",
    "    try:\n",
    "        catnsub = p.search(re.sub('[\\s]',\" \",document)).group(1)\n",
    "    except AttributeError:\n",
    "        print \"Unexpected error:\", sys.exc_info()[0]\n",
    "except:\n",
    "    print \"This file does not have a categories and subject descriptors section or the section headers are different\"\n",
    "    pass\n",
    "catnsub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get Body only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file does not have a body or the section headers are different\n"
     ]
    }
   ],
   "source": [
    "p= re.compile('(?<=INTRODUCTION)(.+)(?=CONCLUSION)')\n",
    "try:\n",
    "    body = p.search(re.sub('[\\s]',\" \",document)).group(1)\n",
    "    try:\n",
    "        p= re.compile('(?<=introduction)(.+)(?=conclusion)')\n",
    "        body = p.search(re.sub('[\\s]',\" \",lower)).group(1)\n",
    "    except AttributeError:\n",
    "        print \"Unexpected error:\", sys.exc_info()[0]\n",
    "except:\n",
    "    print \"This file does not have a body or the section headers are different\"\n",
    "    pass\n",
    "body;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get CONCLUSION only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This may not have a conclusion or has different section headers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'S  We presented a system called TIMEMACHINE for automatic time- line generation for entities in a knowledge base. The timeline gen- eration problem is formulated in a submodular optimization frame- work that jointly optimizes for relevance, content diversity and tem- poral diversity. Web-based co-occurrence signals are used to de- termine the relevance of other entities and dates to the timeline subject. We proved that an efcient greedy approximation algo- rithm achieves near-optimal performance. The proposed approach is evaluated through a comprehensive series of user studies demon- strating that both temporal diversity and content diversity are cru- cial, and that web-based co-occurrence signals signicantly im- prove over a baseline model that relies on global importance.  Acknowledgments. We thank Evgeniy Gabrilovich for many help- ful discussions, Arun Chaganty, Stefanie Jegelka, Karthik Raman, Sujith Ravi, and Ravi Kumar for their insights on submodular opti- mization, Jeff Tamer and Patri Friedman for their support with the user studies, Danila Sinopalnikov and Alexander Lyashuk for their help with the co-occurrence pipeline, and Jure Leskovec, David Hallac, Caroline Suen, and the anonymous reviewers for their valu- able feedback.  8. '"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= re.compile('(?<=CONCLUSION)(.+)(?=REFERENCES)')\n",
    "try:\n",
    "    conclusion = p.search(re.sub('[\\s]',\" \",document)).group(1)\n",
    "    try:\n",
    "        p= re.compile('(?<=conclusion)(.+)(?=references)')\n",
    "        conclusion = p.search(re.sub('[\\s]',\" \",lower)).group(1)\n",
    "    except AttributeError:\n",
    "        print \"Unexpected error:\", sys.exc_info()[0]\n",
    "except AttributeError:\n",
    "    print \"This may not have a conclusion or has different section headers\"\n",
    "    pass\n",
    "conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get References only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file may not have references or has different section headers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' [1] A. Ahmed, C. H. Teo, S. Vishwanathan, and A. Smola. Fair and  balanced: Learning to present news stories. In WSDM, 2012.  [2] J. Allan, R. Gupta, and V. Khandelwal. Temporal summaries of new  topics. In SIGIR, 2001.  [15] D. Graus, M.-H. Peetz, D. Odijk, O. de Rooij, and M. de Rijke.  yourHistorySemantic linking for a personalized timeline of historic events. Workshop: LinkedUp Challenge at OKCon, 2013.  [16] T. Huet, J. Biega, and F. M. Suchanek. Mining history with le monde.  In AKBC, 2013.  [17] H. Ji, T. Cassidy, Q. Li, and S. Tamang. Tackling representation,  annotation and classication challenges for temporal knowledge base population. KAIS, 2013.  [18] A. Kannan, S. Baker, K. Ramnath, J. Fiss, D. Lin, L. Vanderwende,  R. Ansary, A. Kapoor, Q. Ke, M. Uyttendaele, et al. Mining text snippets for images on the web. In SIGKDD, 2014.  [19] S. M. Katz. Estimation of probabilities from sparse data for the  language model component of a speech recognizer. In IEEE Trans Sig. Process., 1987.  [20] A. Krause and D. Golovin. Submodular function maximization. In Tractability: Practical Approaches to Hard Problems (to appear). Cambridge University Press, 2014.  [21] J. Leskovec, A. Krause, C. Guestrin, C. Faloutsos, J. VanBriesen, and  N. Glance. Cost-effective outbreak detection in networks. In SIGKDD, 2007.  [22] J. Li and C. Cardie. Timeline generation: tracking individuals on  twitter. In WWW, 2014.  [23] C.-Y. Lin. Rouge: A package for automatic evaluation of summaries.  In Proc. ACL Text Summarization Workshop, 2004.  [24] H. Lin and J. A. Bilmes. Learning mixtures of submodular shells  with application to document summarization. In UAI, 2012.  [25] X. Ling and D. S. Weld. Temporal information extraction. In AAAI  Conference on Articial Intelligence, 2010.  [26] A. Mazeika, T. Tylenda, and G. Weikum. Entity timelines: Visual  analytics and named entity evolution. In CIKM, 2011.  [27] M. Minoux. Accelerated greedy algorithms for maximizing  submodular set functions. In Optimization Techniques, pages 234243. Springer, 1978.  [28] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of  approximations for maximizing submodular set functions  I. Mathematical Programming, 14(1):265294, 1978.  [29] R. Qian. Timeline: Understanding Important Events in Peoples  Lives. http://blogs.bing.com/search/2014/02/21/ timeline-understanding-important-events-in- peoples-lives/, February 2014. Last retrieved on Feb 18, 2015.  [30] D. Shahaf, C. Guestrin, and E. Horvitz. Metro maps of science. In  [3] T. Althoff, X. L. Dong, K. Murphy, S. Alai, V. Dang, and W. Zhang.  SIGKDD, 2012.  TimeMachine: Timeline Generation for Knowledge-Base Entities. arXiv:1502.04662, 2015.  [4] R. A. Baeza-Yates and B. Ribeiro-Neto. Modern Information  Retrieval. ACM Press, New York, 1999.  [31] D. Shahaf, J. Yang, C. Suen, J. Jacobs, H. Wang, and J. Leskovec. Information cartography: creating zoomable, large-scale maps of information. In SIGKDD, 2013.  [32] W. Shen, J. Wang, and J. Han. Entity linking with a knowledge base:  [5] D. Bamman and N. Smith. Unsupervised discovery of biographical  Issues, techniques, and solutions. TKDE, 2015.  structure from text. TACL, 2(10):363376, 2014.  [6] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor.  Freebase: a collaboratively created graph database for structuring human knowledge. In SIGMOD, 2008.  [33] R. Sipos, A. Swaminathan, P. Shivaswamy, and T. Joachims.  Temporal corpus summarization using submodular word coverage. In CIKM, 2012.  [34] K. Sprck Jones. Automatic summarising: The state of the art.  [7] G. Calinescu, C. Chekuri, M. Pl, and J. Vondrk. Maximizing a  Information Processing & Management, 43(6):14491481, 2007.  monotone submodular function subject to a matroid constraint. SIAM Journal on Computing, 40(6):17401766, 2011.  [35] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a core of  semantic knowledge. In WWW, 2007.  [8] J. Carbonell and J. Goldstein. The use of MMR, diversity-based  reranking for reordering documents and producing summaries. In SIGIR, 1998.  [9] B. Carterette, P. N. Bennett, D. M. Chickering, and S. T. Dumais.  Here or there: Preference Judgments for Relevance. In Advances in Information Retrieval. Springer, 2008.  [10] A. Dasgupta, R. Kumar, and S. Ravi. Summarization through  submodularity and dispersion. In ACL, 2013.  [11] Q. X. Do, W. Lu, and D. Roth. Joint inference for event timeline  construction. In EMNLP-CoNLL, 2012.  [12] X. Dong, E. Gabrilovich, G. Heitz, W. Horn, N. Lao, K. Murphy,  T. Strohmann, S. Sun, and W. Zhang. Knowledge vault: A web-scale approach to probabilistic knowledge fusion. In SIGKDD, 2014. [13] M. Dubinko, R. Kumar, J. Magnani, J. Novak, P. Raghavan, and  A. Tomkins. Visualizing tags over time. TWEB, 1(2):7, 2007.  [14] U. Feige. A threshold of ln n for approximating set cover. Journal of  the ACM (JACM), 45(4):634652, 1998.  [36] F. M. Suchanek and N. Preda. Semantic Culturomics (Vision paper).  In Very Large Databases (VLDB), 2014.  [37] R. Swan and J. Allan. Automatic generation of overview timelines.  In SIGIR, 2000.  [38] T. Tran, A. Ceroni, M. Georgescu, K. D. Naini, and M. Fisichella.  Wikipevent: Leveraging wikipedia edit history for event detection. In WISE. Springer, 2014.  [39] T. A. Tuan, S. Elbassuoni, N. Preda, and G. Weikum. CATE:  Context-Aware Timeline for Entity Illustration. In WWW, 2011.  [40] Y. Wang, M. Zhu, L. Qu, M. Spaniol, and G. Weikum. Timely Yago:  harvesting, querying, and visualizing temporal knowledge from Wikipedia. In EDBT, 2010.  [41] G. Weikum, N. Ntarmos, M. Spaniol, P. Triantallou, A. A. Benczr,  S. Kirkpatrick, P. Rigaux, and M. Williamson. Longitudinal Analytics on Web Archive Data: Its About Time! In CIDR, 2011. [42] X. W. Zhao, Y. Guo, R. Yan, Y. He, and X. Li. Timeline generation  with social attention. In SIGIR, 2013.  28 '"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= re.compile('(?<=REFERENCES)(.+)')\n",
    "try:\n",
    "    references = p.search(re.sub('[\\s]',\" \",document)).group(1)\n",
    "    try:\n",
    "        p= re.compile('(?<=references)(.+)')\n",
    "        references = p.search(re.sub('[\\s]',\" \",lower)).group(1)\n",
    "    except AttributeError:\n",
    "        print \"Unexpected error:\", sys.exc_info()[0]\n",
    "except AttributeError:\n",
    "    print \"This file may not have references or has different section headers\"\n",
    "    pass\n",
    "references  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to count the number of references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file does not have references or the section headers are different\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "create a match of all references by counting the number of integers enclosed by brackets (i.e. '[2]').  \n",
    "\n",
    "This is how references are labeled in the research papers\n",
    "'''\n",
    "# test to see if references exist\n",
    "if references in globals():\n",
    "    len(re.findall('\\[(.*?)\\]',regexp.search(re.sub('[\\s]',\" \",document)).group(1)))\n",
    "    \n",
    "else:\n",
    "    print \"This file does not have references or the section headers are different\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NLTK standard chunk comparison to hand labeled entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"returns named entity chunks in a given text\"\n",
    "testtagged = nltk.pos_tag(nltk.word_tokenize(abstract))\n",
    "testentities = nltk.chunk.ne_chunk(testtagged)\n",
    "\n",
    "machinelist_persons=[]\n",
    "machinelist_orgs=[]\n",
    "\n",
    "for l in testentities:\n",
    "    if isinstance(l,nltk.tree.Tree):\n",
    "        if l.label() == 'PERSON':\n",
    "            if \" \".join(map(itemgetter(0), l)) not in machinelist_persons:\n",
    "                machinelist_persons.append(\" \".join(map(itemgetter(0), l)))\n",
    "\n",
    "    if isinstance(l,nltk.tree.Tree):\n",
    "        if l.label() == 'ORGANIZATION':\n",
    "            if \" \".join(map(itemgetter(0), l)) not in machinelist_orgs:\n",
    "                machinelist_orgs.append(\" \".join(map(itemgetter(0), l)))\n",
    "print len(machinelist_persons),len(p19pdf_authors)                \n",
    "set(machinelist_persons) & set(p19pdf_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative Cases</th>\n",
       "      <td>0</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Cases</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Predicted Negative  Predicted Positive\n",
       "Negative Cases                   0                  -6\n",
       "Positive Cases                   0                   0"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Using explanations from this page to estimate precision and recall\n",
    "http://www.kdnuggets.com/faq/precision-recall.html\n",
    "'''\n",
    "inboth = float(len(set(machinelist_persons) & set(p19pdf_authors)))\n",
    "ext_len=float(len(machinelist_persons))\n",
    "true_len=float(len(p19pdf_authors))\n",
    "\n",
    "d = {'Predicted Negative': [0,0], 'Predicted Positive': [ext_len-true_len,inboth]}\n",
    "metrics = pd.DataFrame(d, index=['Negative Cases','Positive Cases'])\n",
    "try:\n",
    "    print \"The \\\"Accuracy\\\" is %r.\\nThe \\\"Recall\\\" is %r.\\nThe \\\"Precision\\\" is %r.\" % ((round((inboth/ext_len)*100)),(round(inboth/true_len)*100),(inboth/(inboth+ext_len-true_len))*100)\n",
    "except ZeroDivisionError:\n",
    "    print \"Not enough data\"\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'J. Biega',\n",
       " 'J. Fiss',\n",
       " 'J. Jacobs',\n",
       " 'J. Magnani',\n",
       " 'J. Novak',\n",
       " 'J. VanBriesen',\n",
       " 'J. Wang',\n",
       " 'J. Yang'}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"returns named entity chunks in a given text\"\n",
    "testtagged = nltk.pos_tag(nltk.word_tokenize(references))\n",
    "testentities = nltk.chunk.ne_chunk(testtagged)\n",
    "\n",
    "machinelist_persons=[]\n",
    "machinelist_orgs=[]\n",
    "\n",
    "for l in testentities:\n",
    "    if isinstance(l,nltk.tree.Tree):\n",
    "        if l.label() == 'PERSON':\n",
    "            if \" \".join(map(itemgetter(0), l)) not in machinelist_persons:\n",
    "                machinelist_persons.append(\" \".join(map(itemgetter(0), l)))\n",
    "\n",
    "    if isinstance(l,nltk.tree.Tree):\n",
    "        if l.label() == 'ORGANIZATION':\n",
    "            if \" \".join(map(itemgetter(0), l)) not in machinelist_orgs:\n",
    "                machinelist_orgs.append(\" \".join(map(itemgetter(0), l)))\n",
    "print len(machinelist_persons),len(p19pdf_references_authors)                \n",
    "set(machinelist_persons) & set(p19pdf_references_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ahmed',\n",
       " 'Alai',\n",
       " 'Allan',\n",
       " 'Allan',\n",
       " 'Althoff',\n",
       " 'Ansary',\n",
       " 'Baeza-Yates',\n",
       " 'Baker',\n",
       " 'Bamman',\n",
       " 'Benczr',\n",
       " 'Bennett',\n",
       " 'Biega',\n",
       " 'BilmesX.',\n",
       " 'Bollacker',\n",
       " 'C.-Y.',\n",
       " 'Calinescu',\n",
       " 'Carbonell',\n",
       " 'Cardie',\n",
       " 'Cardie',\n",
       " 'Carterette',\n",
       " 'Cassidy',\n",
       " 'Ceroni',\n",
       " 'Chekuri',\n",
       " 'Chickering',\n",
       " 'Dang',\n",
       " 'Dasgupta',\n",
       " 'Dong',\n",
       " 'Dong',\n",
       " 'Dubinko',\n",
       " 'Dumais',\n",
       " 'Elbassuoni',\n",
       " 'Evans',\n",
       " 'Faloutsos',\n",
       " 'Feige',\n",
       " 'Fisher',\n",
       " 'Fisichella',\n",
       " 'Fiss',\n",
       " 'Gabrilovich',\n",
       " 'Georgescu',\n",
       " 'Glance',\n",
       " 'Goldstein',\n",
       " 'Golovin',\n",
       " 'Graus',\n",
       " 'Guestrin',\n",
       " 'Guestrin',\n",
       " 'Guo',\n",
       " 'Gupta',\n",
       " 'Han',\n",
       " 'Heitz',\n",
       " 'Horn',\n",
       " 'Horvitz',\n",
       " 'Huet',\n",
       " 'Jacobs',\n",
       " 'Joachims',\n",
       " 'Jones',\n",
       " 'Kannan',\n",
       " 'Kapoor',\n",
       " 'Kasneci',\n",
       " 'Katz',\n",
       " 'Khandelwal',\n",
       " 'Kirkpatrick',\n",
       " 'Krause',\n",
       " 'Krause',\n",
       " 'Kumar',\n",
       " 'Kumar',\n",
       " 'Lao',\n",
       " 'Leskovec',\n",
       " 'Leskovec',\n",
       " 'Lin',\n",
       " 'Lin',\n",
       " 'Lin',\n",
       " 'Ling',\n",
       " 'M.-H.',\n",
       " 'Magnani',\n",
       " 'Mazeika',\n",
       " 'Minoux',\n",
       " 'Murphy',\n",
       " 'Murphy',\n",
       " 'Naini',\n",
       " 'Nemhauser',\n",
       " 'Novak',\n",
       " 'Ntarmos',\n",
       " 'Odijk',\n",
       " 'Paritosh',\n",
       " 'Peetz',\n",
       " 'Preda',\n",
       " 'Preda',\n",
       " 'Qian',\n",
       " 'Raghavan',\n",
       " 'Ramnath',\n",
       " 'Ravi',\n",
       " 'Ribeiro-Neto',\n",
       " 'Rigaux',\n",
       " 'Rijke',\n",
       " 'Rooij',\n",
       " 'Roth',\n",
       " 'Shahaf',\n",
       " 'Shahaf',\n",
       " 'Shen',\n",
       " 'Shivaswamy',\n",
       " 'Sipos',\n",
       " 'Smith',\n",
       " 'Smola',\n",
       " 'Spaniol',\n",
       " 'Spaniol',\n",
       " 'Sprck',\n",
       " 'Strohmann',\n",
       " 'Sturge',\n",
       " 'Suchanek',\n",
       " 'Suchanek',\n",
       " 'Suchanek',\n",
       " 'Suen',\n",
       " 'Sun',\n",
       " 'Swaminathan',\n",
       " 'Swan',\n",
       " 'Tamang',\n",
       " 'Taylor',\n",
       " 'Teo',\n",
       " 'Tomkins',\n",
       " 'Tran',\n",
       " 'Triantallou',\n",
       " 'Tuan',\n",
       " 'Tylenda',\n",
       " 'Uyttendaele',\n",
       " 'VanBriesen',\n",
       " 'Vanderwende',\n",
       " 'Vishwanathan',\n",
       " 'Vondrk',\n",
       " 'Wang',\n",
       " 'Wang',\n",
       " 'Wang',\n",
       " 'Weikum',\n",
       " 'Weikum',\n",
       " 'Weikum',\n",
       " 'Weikum',\n",
       " 'Weikum',\n",
       " 'Weld',\n",
       " 'Williamson',\n",
       " 'Wolsey',\n",
       " 'Yan',\n",
       " 'Yang',\n",
       " 'Zhang',\n",
       " 'Zhang',\n",
       " 'Zhao',\n",
       " 'Zhu']"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the list of hand labeled names to just keep the last name for comparison\n",
    "\n",
    "newlist=[]\n",
    "#print machinelist_persons\n",
    "#print p19pdf_references_authors\n",
    "for l in p19pdf_references_authors:\n",
    "    for m in l.split():\n",
    "        if len(m)<=2:\n",
    "            pass\n",
    "        else:\n",
    "            newlist.extend([m])        \n",
    "sorted(newlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"Accuracy\" is 20.0.\n",
      "The \"Recall\" is 0.0.\n",
      "The \"Precision\" is -7.6923076923076925.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative Cases</th>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Cases</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Predicted Negative  Predicted Positive\n",
       "Negative Cases                   0                 112\n",
       "Positive Cases                   0                   8"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Using explanations from this page to estimate precision and recall\n",
    "http://www.kdnuggets.com/faq/precision-recall.html\n",
    "'''\n",
    "inboth = float(len(set(machinelist_persons) & set(p19pdf_references_authors)))\n",
    "ext_len=float(len(machinelist_persons))\n",
    "true_len=float(len(p19pdf_references_authors))\n",
    "\n",
    "d = {'Predicted Negative': [0,0], 'Predicted Positive': [abs(ext_len-true_len),inboth]}\n",
    "metrics = pd.DataFrame(d, index=['Negative Cases','Positive Cases'])\n",
    "print \"The \\\"Accuracy\\\" is %r.\\nThe \\\"Recall\\\" is %r.\\nThe \\\"Precision\\\" is %r.\" % ((round((inboth/ext_len)*100)),(round(inboth/true_len)*100),(inboth/(inboth+ext_len-true_len))*100)\n",
    "#,,\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
