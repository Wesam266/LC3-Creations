{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook works on extracting unique named entities and organizations from KDD papers and passing them lists.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import subprocess\n",
    "import unicodedata\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk import Tree\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import polyglot\n",
    "import string\n",
    "from emailextractor import file_to_str, get_emails # paste code to .py file from following link and save within your environment path to call it: https://gist.github.com/dideler/5219706\n",
    "\n",
    "'''\n",
    "Severl issues extended the time to complete this work\n",
    "that were not related to the coding itself.  I had two\n",
    "Anaconda distros installed on my computer.  This led\n",
    "to problems with importing modules because the paths\n",
    "for installation and retrieval were mixed.  I had to\n",
    "uninstall both anacondas, reinstall, and then recreate\n",
    "my virtual environment.  Imports and installs worked cleanly\n",
    "\n",
    "To create ipython shells in a virtual environment, use:\n",
    "http://stackoverflow.com/questions/30492623/using-both-python-2-x-and-python-3-x-in-ipython-notebook\n",
    "\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path        = os.path.abspath(os.getcwd())\n",
    "TESTDIR     = os.path.normpath(os.path.join(os.path.expanduser(\"~\"),\"projects\",\"LC3-Creations\", \"examples\",\"KDDsample\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "I experienced unicode problems early on.  Everytime I had an error, I scoured the internet for solutions. Here's the credit.\n",
    "\n",
    "\n",
    "\n",
    "- For Typeerror codes using subprocess to convert pdf2txt output to straight unicode --> http://stackoverflow.com/questions/33283603/python-popen-communicate-str-encodeencoding-utf-8-errors-ignore-cr\n",
    "- For problems with ASCII characters --> http://stackoverflow.com/questions/175240/how-do-i-convert-a-files-format-from-unicode-to-ascii-using-python\n",
    "- For unicode characters left in unicode converted to a string  --> http://stackoverflow.com/questions/8689795/how-can-i-remove-non-ascii-characters-but-leave-periods-and-spaces-using-python\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "a = unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,\"p1005.pdf\")))]),errors='ignore')\n",
    "document = filter(lambda x: x in string.printable,unicodedata.normalize('NFKD', a).encode('ascii','ignore').decode('unicode_escape').encode('ascii','ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mining Frequent Itemsets through Progressive Sampling\\n\\nwith Rademacher Averages\\n\\nMatteo Riondato\\n\\nDept. of Computer Science\\n\\nBrown University\\n\\nProvidence, RI 02912\\n\\nmatteo@cs.brown.edu\\n\\nEli Upfal\\n\\nDept. of Computer Science\\n\\nBrown University\\n\\nProvidence, RI 02912\\neli@cs.brown.edu\\n\\nABSTRACT\\nWe present an algorithm to extract an high-quality approx-\\nimation of the (top-k) Frequent itemsets (FIs) from ran-\\ndom samples of a transactional dataset. With high prob-\\nability the approximation is a superset of the FIs, and no\\nitemset with frequency much lower than the threshold is in-\\ncluded in it. The algorithm employs progressive sampling,\\nwith a stopping condition based on bounds to the empirical\\nRademacher average, a key concept from statistical learning\\ntheory. The computation of the bounds uses characteris-\\ntic quantities that can be obtained eciently with a sin-\\ngle scan of the sample. Therefore, evaluating the stopping\\ncondition is fast, and does not require an expensive mining\\nof each sample. Our experimental evaluation conrms the\\npracticality of our approach on real datasets, outperforming\\napproaches based on one-shot static sampling.\\n\\nCategories and Subject Descriptors\\nH.2.8 [Database Management]: Database Applications\\nData mining\\n\\nGeneral Terms\\nAlgorithms, Theory, Performance, Experimentation\\n\\nKeywords\\nFrequent Itemsets; Pattern Mining; Rademacher Averages;\\nSampling; Statistical Learning Theory\\n\\n1.\\n\\nINTRODUCTION\\n\\nThe task of Frequent Itemsets (FIs) mining is to extract all\\nsets of items that appear in at least a fraction  of a transac-\\ntional dataset D, or the k most frequent set of items [2]. It is\\na fundamental primitive of knowledge discovery and is use-\\nful, among the others, for market basket analysis, inference,\\nclassication, and network management [13]. Exact algo-\\nrithms to mine FIs have since long been available but their\\n\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided tha'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"returns named entity chunks in a given text\"\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(re.sub('[\\s]',\" \", document)))\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "# Another entity extractor\n",
    "st = StanfordNERTagger('/Users/linwood/stanford-corenlp-full-2015-04-20/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "       '/Users/linwood/stanford-corenlp-full-2015-04-20/stanford-corenlp-3.5.2.jar',\n",
    "       encoding='utf-8')\n",
    "tokenized_text = word_tokenize(re.sub('[\\s]',\" \", document))\n",
    "stanentities = st.tag(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I established two lists to hold the values that I extract from the text.  This itemgetter function will check for unique values.  First, I iterate over the extracted entities and see if the objects is a nltk.tree.Tree with a \"Person\" label.  If it is, and the length is equal to 1 (first or last name only), I append that value to the list. If it's larger, I iterate of the entity tree and pull out the first value only using itemgetter.  Then, I join the values from the list and append it to the destination list.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a list out of NLTK's standard NE chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rademacher Averages Matteo Riondato Dept', 'Eli Upfal Dept', 'Subject Descriptors', 'Database Applications Data', 'Algorithms', 'Experimentation Keywords Frequent Itemsets', 'Pattern Mining', 'Rademacher Averages', 'Statistical Learning', 'Permissions', 'Outline', 'Riondato', 'Upfal', 'Chen', 'Chuang', 'Pietracaprina', 'Scheer', 'Wrobel', 'Elomaa', 'Assume', 'B', 'Hence', 'Thm', 'S S.', 'C1', 'D', 'C1 IS', 'X X', 'Si', 'Due', 'Grahne', 'Zhu', 'Unless', 'Frequency Estimation', 'Fast', 'Bartlett', 'Model', 'Mach', 'J. Pei', 'Data Mining', 'Knowl', 'Disc', 'John', 'Static', 'Johnson', 'J. Thaler', 'Space', 'Data Mining Knowl', 'Rademacher', 'Version', 'Learn', 'Machine Learning']\n",
      "\n",
      "\n",
      "['Computer Science Brown University Providence', 'FIs', 'Frequent Itemsets', 'ACM', 'NSW', 'CIs', 'Parthasarathy', 'fD', 'D', 'TOPK', 'fA', 'SAMPLING', 'AI', 'Thm', 'VS', 'Massarts Lemma', 'RS', 'Euclidean', 'vVS', 'Hoedings', 'esRS', 'Closed Itemsets', 'vS', 'Rademacher Average', 'Closed', 'lnX', 'aX', 'aIS', 's2fS', 'ACa', 'aC1', 'fS', 'NLopt', 'ISi', 'Liberty', 'Riondato', 'PARMA', 'MapReduce', 'EXPERIMENTAL', 'VC', 'Appendix', 'AMD', 'RAM', 'FIMI03', 'FI', 'errS', 'FPavg', 'NSF', 'NIH', 'REFERENCES', 'SIGMOD', 'ECML01', 'ICDT09', 'PAKDD05', 'AAAI02', 'SIGKDD Explor', 'SIGMOD00', 'URL', 'IEEE Trans', 'CoRR', 'ICDM02', 'ICDT99', 'CIKM12', 'ACM Trans', 'Nature of Statistical', 'NY', 'USA', 'University Press', 'VLDB96']\n",
      "\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "persons = []\n",
    "organizations = []\n",
    "locations =[]\n",
    "\n",
    "for l in entities:\n",
    "    if isinstance(l,nltk.tree.Tree):\n",
    "        if l.label() == 'PERSON':\n",
    "            if len(l)== 1:\n",
    "                if l[0][0] in persons:\n",
    "                    pass\n",
    "                else:\n",
    "                    persons.append(l[0][0])\n",
    "            else:\n",
    "                if \" \".join(map(itemgetter(0), l)) in persons:\n",
    "                    pass\n",
    "                else:\n",
    "                    persons.append(\" \".join(map(itemgetter(0), l)))\n",
    "                    \n",
    "for o in entities:\n",
    "    if isinstance(o,nltk.tree.Tree):\n",
    "        if o.label() == 'ORGANIZATION':\n",
    "            if len(o)== 1:\n",
    "                if o[0][0] in organizations:\n",
    "                    pass\n",
    "                else:\n",
    "                    organizations.append(o[0][0])\n",
    "            else:\n",
    "                if \" \".join(map(itemgetter(0), o)) in organizations:\n",
    "                    pass\n",
    "                else:\n",
    "                    organizations.append(\" \".join(map(itemgetter(0), o)))\n",
    "                    \n",
    "for o in entities:\n",
    "    if isinstance(o,nltk.tree.Tree):\n",
    "        if o.label() == 'LOCATION':\n",
    "            if len(o)== 1:\n",
    "                if o[0][0] in locations:\n",
    "                    pass\n",
    "                else:\n",
    "                    locations.append(o[0][0])\n",
    "            else:\n",
    "                if \" \".join(map(itemgetter(0), o)) in locations:\n",
    "                    pass\n",
    "                else:\n",
    "                    locations.append(\" \".join(map(itemgetter(0), o)))\n",
    "                    \n",
    "                \n",
    "print persons\n",
    "print\n",
    "print\n",
    "print organizations\n",
    "print\n",
    "print\n",
    "print locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer Science Brown University Providence\n",
      "Computer Science Brown University Providence\n",
      "Frequent Itemsets\n",
      "Frequent Itemsets\n",
      "Massarts Lemma\n",
      "Closed Itemsets\n",
      "Closed Itemsets\n",
      "Rademacher Average\n",
      "SIGKDD Explor\n",
      "IEEE Trans\n",
      "ACM Trans\n",
      "Nature of Statistical\n",
      "New York\n",
      "University Press\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "for o in entities:\n",
    "    if isinstance(o,nltk.tree.Tree):\n",
    "        if o.label() == 'ORGANIZATION' or o.label() == 'GPE':\n",
    "            if len(o)>1:\n",
    "                print \" \".join(map(itemgetter(0), o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to iterate over the extracted list of entities to get a better break between person's and their university name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = [nltk.word_tokenize(l) for l in persons]\n",
    "fin = [nltk.chunk.ne_chunk(nltk.pos_tag(l)) for l in tokens]\n",
    "fin;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new =[word_tokenize(l) for l in persons]\n",
    "stan = [st.tag(l) for l in new]\n",
    "stan;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating lists of named entities from Stanford's NER model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function looks though an extracted stanford ner list, and finds continuous entitiy labels.  This should create first name, last name records of entities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_continuous_chunks(tagged_sent):\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for token, tag in tagged_sent:\n",
    "        if tag != \"O\":\n",
    "            current_chunk.append((token, tag))\n",
    "        else:\n",
    "            if current_chunk: # if the current chunk is not empty\n",
    "                continuous_chunk.append(current_chunk)\n",
    "                current_chunk = []\n",
    "    # Flush the final current_chunk into the continuous_chunk, if any.\n",
    "    if current_chunk:\n",
    "        continuous_chunk.append(current_chunk)\n",
    "    return continuous_chunk\n",
    "\n",
    "ne_tagged_sent = [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION')]\n",
    "\n",
    "named_entities = get_continuous_chunks(stanentities)\n",
    "named_entities_str = [\" \".join([token for token, tag in ne]) for ne in named_entities]\n",
    "named_entities_str_tag = [(\" \".join([token for token, tag in ne]), ne[0][1]) for ne in named_entities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Rademacher Averages Matteo Riondato Dept', u'ORGANIZATION'),\n",
       " (u'Computer Science Brown University Providence', u'ORGANIZATION'),\n",
       " (u'Eli Upfal Dept', u'ORGANIZATION'),\n",
       " (u'Computer Science Brown University Providence', u'ORGANIZATION'),\n",
       " (u'FIs', u'ORGANIZATION'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'Database Applications Data mining General Terms Algorithms',\n",
       "  u'ORGANIZATION'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'August 10-13', u'DATE'),\n",
       " (u'2015', u'DATE'),\n",
       " (u'Sydney', u'LOCATION'),\n",
       " (u'NSW', u'ORGANIZATION'),\n",
       " (u'Australia', u'LOCATION'),\n",
       " (u'2015', u'DATE'),\n",
       " (u'$ 15.00', u'MONEY'),\n",
       " (u'FIs', u'ORGANIZATION'),\n",
       " (u'FIs', u'ORGANIZATION'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'FIs', u'ORGANIZATION'),\n",
       " (u'FIs', u'ORGANIZATION'),\n",
       " (u'Parthasarathy', u'PERSON'),\n",
       " (u'Scheer', u'PERSON'),\n",
       " (u'Wrobel', u'PERSON'),\n",
       " (u'Scheer', u'PERSON'),\n",
       " (u'Wrobel', u'PERSON'),\n",
       " (u'FIs', u'ORGANIZATION'),\n",
       " (u'Pietracaprina', u'ORGANIZATION'),\n",
       " (u'Cherno', u'ORGANIZATION'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'Elomaa', u'ORGANIZATION'),\n",
       " (u'FIs', u'ORGANIZATION'),\n",
       " (u'fA', u'ORGANIZATION'),\n",
       " (u'fA', u'ORGANIZATION'),\n",
       " (u'fA', u'ORGANIZATION'),\n",
       " (u'FIs', u'ORGANIZATION'),\n",
       " (u'1006', u'DATE'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'Massarts Lemma', u'ORGANIZATION'),\n",
       " (u'CIs', u'ORGANIZATION'),\n",
       " (u'SA', u'ORGANIZATION'),\n",
       " (u'B SA', u'ORGANIZATION'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'FIs', u'ORGANIZATION'),\n",
       " (u'i1', u'ORGANIZATION'),\n",
       " (u'S0', u'ORGANIZATION'),\n",
       " (u'ISi', u'ORGANIZATION'),\n",
       " (u'ISi', u'ORGANIZATION'),\n",
       " (u'S0', u'ORGANIZATION'),\n",
       " (u'2i', u'ORGANIZATION'),\n",
       " (u'1010', u'DATE'),\n",
       " (u'PARMA', u'LOCATION'),\n",
       " (u'FIs', u'ORGANIZATION'),\n",
       " (u'PARMA', u'LOCATION'),\n",
       " (u'FIs', u'ORGANIZATION'),\n",
       " (u'FIs', u'ORGANIZATION'),\n",
       " (u'Grahne', u'PERSON'),\n",
       " (u'AMD', u'ORGANIZATION'),\n",
       " (u'RAM', u'ORGANIZATION'),\n",
       " (u'1000', u'DATE'),\n",
       " (u'1657', u'DATE'),\n",
       " (u'2088', u'DATE'),\n",
       " (u'] 46 43 81 443 59 58', u'DATE'),\n",
       " (u'100 %', u'PERCENT'),\n",
       " (u'15 %', u'PERCENT'),\n",
       " (u'92 %', u'PERCENT'),\n",
       " (u'49 %', u'PERCENT'),\n",
       " (u'1.4 %', u'PERCENT'),\n",
       " (u'Sect', u'LOCATION'),\n",
       " (u'1013', u'DATE'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'FIs', u'ORGANIZATION'),\n",
       " (u'NSF', u'ORGANIZATION'),\n",
       " (u'NIH', u'ORGANIZATION'),\n",
       " (u'] R. Agrawal', u'PERSON'),\n",
       " (u'R. Srikant', u'PERSON'),\n",
       " (u'] R. Agrawal', u'PERSON'),\n",
       " (u'T. Imieliski', u'PERSON'),\n",
       " (u'A. Swami', u'PERSON'),\n",
       " (u'June 1993', u'DATE'),\n",
       " (u'] B. Gu', u'PERSON'),\n",
       " (u'B. Liu', u'PERSON'),\n",
       " (u'F. Hu', u'PERSON'),\n",
       " (u'H. Liu', u'PERSON'),\n",
       " (u'] P. L. Bartlett', u'PERSON'),\n",
       " (u'S. Boucheron', u'PERSON'),\n",
       " (u'G. Lugosi', u'PERSON'),\n",
       " (u'2002', u'DATE'),\n",
       " (u'] S. Boucheron', u'PERSON'),\n",
       " (u'O. Bousquet', u'PERSON'),\n",
       " (u'G. Lugosi', u'PERSON'),\n",
       " (u'2005', u'DATE'),\n",
       " (u'] V. T. Chakaravarthy', u'PERSON'),\n",
       " (u'V. Pandit', u'PERSON'),\n",
       " (u'Y. Sabharwal', u'PERSON'),\n",
       " (u'] B. Chen', u'PERSON'),\n",
       " (u'P. Haas', u'PERSON'),\n",
       " (u'P. Scheuermann', u'PERSON'),\n",
       " (u'K.-T. Chuang', u'PERSON'),\n",
       " (u'M.-S. Chen', u'PERSON'),\n",
       " (u'W.-C. Yang', u'ORGANIZATION'),\n",
       " (u'T. Elomaa', u'ORGANIZATION'),\n",
       " (u'M. Kriinen', u'PERSON'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'B. Goethals', u'PERSON'),\n",
       " (u'M. J. Zaki', u'PERSON'),\n",
       " (u'June 2004', u'DATE'),\n",
       " (u'] G. Grahne', u'PERSON'),\n",
       " (u'J. Zhu', u'PERSON'),\n",
       " (u'] J. Han', u'PERSON'),\n",
       " (u'J. Pei', u'PERSON'),\n",
       " (u'Y. Yin', u'PERSON'),\n",
       " (u'] J. Han', u'PERSON'),\n",
       " (u'H. Cheng', u'PERSON'),\n",
       " (u'D. Xin', u'PERSON'),\n",
       " (u'X. Yan', u'PERSON'),\n",
       " (u'2007', u'DATE'),\n",
       " (u'G. H. John', u'PERSON'),\n",
       " (u'P. Langley', u'PERSON'),\n",
       " (u'S. G. Johnson', u'PERSON'),\n",
       " (u'V. Koltchinskii', u'PERSON'),\n",
       " (u'July 2001', u'DATE'),\n",
       " (u'M. Mitzenmacher', u'PERSON'),\n",
       " (u'J. Thaler', u'PERSON'),\n",
       " (u'J. Ullman', u'PERSON'),\n",
       " (u'July 2014', u'DATE'),\n",
       " (u'S. Parthasarathy', u'PERSON'),\n",
       " (u'N. Pasquier', u'PERSON'),\n",
       " (u'Y. Bastide', u'PERSON'),\n",
       " (u'R. Taouil', u'PERSON'),\n",
       " (u'L. Lakhal', u'PERSON'),\n",
       " (u'A. Pietracaprina', u'PERSON'),\n",
       " (u'M. Riondato', u'PERSON'),\n",
       " (u'E. Upfal', u'PERSON'),\n",
       " (u'F. Vandin', u'ORGANIZATION'),\n",
       " (u'Data Mining Knowl', u'ORGANIZATION'),\n",
       " (u'2010', u'DATE'),\n",
       " (u'D. Jensen', u'PERSON'),\n",
       " (u'T. Oates', u'PERSON'),\n",
       " (u'] M. Riondato', u'PERSON'),\n",
       " (u'R. Fonseca', u'PERSON'),\n",
       " (u'MapReduce', u'ORGANIZATION'),\n",
       " (u'] M. Riondato', u'PERSON'),\n",
       " (u'E. Upfal', u'ORGANIZATION'),\n",
       " (u'Rademacher', u'PERSON'),\n",
       " (u'cs.brown.edu %', u'PERCENT'),\n",
       " (u'] M. Riondato', u'PERSON'),\n",
       " (u'E. Upfal', u'ORGANIZATION'),\n",
       " (u'2014', u'DATE'),\n",
       " (u'] M. Riondato', u'PERSON'),\n",
       " (u'F. Vandin', u'PERSON'),\n",
       " (u'V. N. Vapnik', u'PERSON'),\n",
       " (u'Nature of Statistical Learning', u'ORGANIZATION'),\n",
       " (u'New York', u'LOCATION'),\n",
       " (u'USA', u'ORGANIZATION'),\n",
       " (u'1999', u'DATE'),\n",
       " (u'G. I. Webb', u'PERSON'),\n",
       " (u'T. Scheer', u'PERSON'),\n",
       " (u'S. Wrobel', u'PERSON'),\n",
       " (u'Dec. 2002', u'DATE'),\n",
       " (u'S. Ben-David', u'PERSON'),\n",
       " (u'University Press', u'ORGANIZATION'),\n",
       " (u'2014', u'DATE'),\n",
       " (u'H. Toivonen', u'PERSON')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_entities = get_continuous_chunks(stanentities)\n",
    "named_entities_str = [\" \".join([token for token, tag in ne]) for ne in named_entities]\n",
    "named_entities_str_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Rademacher',\n",
       " u'Parthasarathy',\n",
       " u'Scheer',\n",
       " u'Wrobel',\n",
       " u'Grahne',\n",
       " u'  R. Agrawal',\n",
       " u'R. Srikant',\n",
       " u'T. Imieliski',\n",
       " u'A. Swami',\n",
       " u'  B. Gu',\n",
       " u'B. Liu',\n",
       " u'F. Hu',\n",
       " u'H. Liu',\n",
       " u'  P. L. Bartlett',\n",
       " u'S. Boucheron',\n",
       " u'G. Lugosi',\n",
       " u'  S. Boucheron',\n",
       " u'O. Bousquet',\n",
       " u'  V. T. Chakaravarthy',\n",
       " u'V. Pandit',\n",
       " u'Y. Sabharwal',\n",
       " u'  B. Chen',\n",
       " u'P. Haas',\n",
       " u'P. Scheuermann',\n",
       " u'K.-T. Chuang',\n",
       " u'M.-S. Chen',\n",
       " u'M. Kriinen',\n",
       " u'B. Goethals',\n",
       " u'M. J. Zaki',\n",
       " u'  G. Grahne',\n",
       " u'J. Zhu',\n",
       " u'  J. Han',\n",
       " u'J. Pei',\n",
       " u'Y. Yin',\n",
       " u'H. Cheng',\n",
       " u'D. Xin',\n",
       " u'X. Yan',\n",
       " u'G. H. John',\n",
       " u'P. Langley',\n",
       " u'S. G. Johnson',\n",
       " u'V. Koltchinskii',\n",
       " u'M. Mitzenmacher',\n",
       " u'J. Thaler',\n",
       " u'J. Ullman',\n",
       " u'S. Parthasarathy',\n",
       " u'N. Pasquier',\n",
       " u'Y. Bastide',\n",
       " u'R. Taouil',\n",
       " u'L. Lakhal',\n",
       " u'A. Pietracaprina',\n",
       " u'M. Riondato',\n",
       " u'E. Upfal',\n",
       " u'D. Jensen',\n",
       " u'T. Oates',\n",
       " u'  M. Riondato',\n",
       " u'R. Fonseca',\n",
       " u'F. Vandin',\n",
       " u'V. N. Vapnik',\n",
       " u'G. I. Webb',\n",
       " u'T. Scheer',\n",
       " u'S. Wrobel',\n",
       " u'S. Ben-David',\n",
       " u'H. Toivonen']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare=[]\n",
    "for l,m in named_entities_str_tag:\n",
    "    l=re.sub('(\\])',\" \",l)\n",
    "    if m == 'PERSON':\n",
    "        if l in compare:\n",
    "            pass\n",
    "        else:\n",
    "            compare.append(l)\n",
    "    else:\n",
    "        pass\n",
    "compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list1 = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list2 = [i for i in xrange(7,17,1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{7, 8, 9}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list1) & set(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parts_of_speech(corpus):\n",
    "    \"returns named entity chunks in a given text\"\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(corpus))\n",
    "    entities = nltk.chunk.ne_chunk(tagged)\n",
    "    # Another entity extractor\n",
    "    st = StanfordNERTagger('/Users/linwood/stanford-corenlp-full-2015-04-20/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "           '/Users/linwood/stanford-corenlp-full-2015-04-20/stanford-corenlp-3.5.2.jar',\n",
    "           encoding='utf-8')\n",
    "    tokenized_text = word_tokenize(corpus)\n",
    "    stanentities = st.tag(tokenized_text)\n",
    "    return entities\n",
    "def find_entities(chunks):\n",
    "    \"given list of tagged parts of speech, returns unique named entities\"\n",
    "\n",
    "    def traverse(tree):\n",
    "        \"recursively traverses an nltk.tree.Tree to find named entities\"\n",
    "        entity_names = []\n",
    "    \n",
    "        if hasattr(tree, 'node') and tree.node:\n",
    "            if tree.node == 'NE':\n",
    "                entity_names.append(' '.join([child[0] for child in tree]))\n",
    "            else:\n",
    "                for child in tree:\n",
    "                    entity_names.extend(traverse(child))\n",
    "    \n",
    "        return entity_names\n",
    "    \n",
    "    named_entities = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        entities = sorted(list(set([word for tree in chunk\n",
    "                            for word in traverse(tree)])))\n",
    "        for e in entities:\n",
    "            if e not in named_entities:\n",
    "                named_entities.append(e)\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting entities and creating lists using Polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from polyglot.text import Text\n",
    "e=Text(re.sub('[\\s]',\" \",document[:10000])).entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code iterates over the polyglot extracted entities and creates a list of person, locations, and organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import unicodedata\n",
    "\n",
    "def extraction(corpus):\n",
    "    \n",
    "    # extract entities from a single string; remove whitespace characters\n",
    "    try:\n",
    "        e = Text(re.sub('[\\s]',\" \",corpus)).entities\n",
    "    except:\n",
    "        pass #e = Text(re.sub(\"(r'(x0)',\" \",\"(re.sub('[\\s]',\" \",corpus)))).entities\n",
    "    \n",
    "    current_person =[]\n",
    "    persons =[]\n",
    "    current_org=[]\n",
    "    organizations=[]\n",
    "    current_loc=[]\n",
    "    locations=[]\n",
    "\n",
    "    for l in e:\n",
    "        if l.tag == 'I-PER':\n",
    "            for m in l:\n",
    "                current_person.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_person: # if the current chunk is not empty\n",
    "                        persons.append(\" \".join(current_person))\n",
    "                        current_person = []\n",
    "        elif l.tag == 'I-ORG':\n",
    "            for m in l:\n",
    "                current_org.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_org: # if the current chunk is not empty\n",
    "                        organizations.append(\" \".join(current_org))\n",
    "                        current_org = []\n",
    "        elif l.tag == 'I-LOC':\n",
    "            for m in l:\n",
    "                current_loc.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_loc: # if the current chunk is not empty\n",
    "                        locations.append(\" \".join(current_loc))\n",
    "                        current_loc = []\n",
    "    results = {}\n",
    "    results['persons']=persons\n",
    "    results['organizations']=organizations\n",
    "    results['locations']=locations\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Matteo Riondato',\n",
       " 'matteo',\n",
       " 'cs.brown.edu Eli Upfal',\n",
       " 'KDD15',\n",
       " 'Chen',\n",
       " 'Chuang',\n",
       " 'thm',\n",
       " 'Pietracaprina',\n",
       " 'Scheer',\n",
       " 'Scheer',\n",
       " 'Pietracaprina et',\n",
       " 'Cherno',\n",
       " 'David',\n",
       " 'Elomaa',\n",
       " 'aX',\n",
       " 'X',\n",
       " 'Si1',\n",
       " 'Liberty',\n",
       " 'Riondato',\n",
       " 'Riondato',\n",
       " 'Grahne',\n",
       " 'Zhu',\n",
       " 'matteo',\n",
       " 'radeprogrfi.tar.bz2',\n",
       " 'errorrelative',\n",
       " 'error1012',\n",
       " 'Agrawal',\n",
       " 'Agrawal',\n",
       " 'Mining',\n",
       " 'Liu',\n",
       " 'Hu',\n",
       " 'Liu',\n",
       " 'Eciently',\n",
       " 'Bartlett',\n",
       " 'Lugosi',\n",
       " 'Lugosi',\n",
       " 'Pandit',\n",
       " 'Chen',\n",
       " 'Haas',\n",
       " 'Chuang',\n",
       " 'Chen',\n",
       " 'Yang',\n",
       " 'Kriinen',\n",
       " 'J',\n",
       " 'Zaki',\n",
       " 'SIGKDD',\n",
       " 'Grahne',\n",
       " 'Zhu',\n",
       " 'Han',\n",
       " 'Pei',\n",
       " 'Han',\n",
       " 'Cheng',\n",
       " 'John',\n",
       " 'Johnson',\n",
       " 'J',\n",
       " 'Thaler',\n",
       " 'Ullman',\n",
       " 'CoRR',\n",
       " 'Jensen',\n",
       " 'Oates',\n",
       " 'Ecient',\n",
       " 'Fonseca',\n",
       " 'Rademacher',\n",
       " 'Webb',\n",
       " 'Scheer',\n",
       " 'Ben',\n",
       " 'David']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction(document)['persons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regexp = re.compile(\"REFERENCES(.*)$\")\n",
    "references = Text(regexp.search(re.sub('[\\s]',\" \",document)).group(1)).entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regexp1 = re.compile(\"REFERENCES(.*)$\")\n",
    "references = Text(regexp.search(re.sub('[\\s]',\" \",document)).group(1)).entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extraction(regexp.search(re.sub('[\\s]',\" \",document)).group(1))['persons'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truth Sets to test extraction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 authors\n",
      "\n",
      "There are 3 author organizations\n",
      "\n",
      "There are 7 author locations\n",
      "\n",
      "There are 152 authors in the references\n"
     ]
    }
   ],
   "source": [
    "#p19.pdf\n",
    "\n",
    "p19pdf_authors=['Tim Althoff','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang']\n",
    "p19pdf_author_organizations=['Computer Science Department','Stanford University','Google']\n",
    "p19pdf_author_locations=['Stanford, CA','Stanford','CA','Google','1600 Amphitheatre Parkway, Mountain View, CA 94043','1600 Amphitheatre Parkway','Mountain View']\n",
    "\n",
    "p19pdf_references_authors =['A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']\n",
    "p19pdf_allauthors=['Tim Althoff','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang','A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']\n",
    "\n",
    "print \"There are %r authors\" % len(p19pdf_authors)\n",
    "print  # white space\n",
    "print \"There are %r author organizations\" %len(p19pdf_author_organizations)\n",
    "print \n",
    "print \"There are %r author locations\" % len(p19pdf_author_locations)\n",
    "print  \n",
    "print \"There are %r authors in the references\" %len(p19pdf_references_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 authors\n",
      "\n",
      "There are 6 author organizations\n",
      "\n",
      "There are 8 author locations\n",
      "\n",
      "There are 106 authors in the references\n"
     ]
    }
   ],
   "source": [
    "#p29.pdf\n",
    "\n",
    "p29pdf_authors=['Laurent Amsaleg','Stéphane Girard','Oussama Chelly','Teddy Furon','Michael E. Houle','Ken-ichi Kawarabayashi',\n",
    "               'Michael Nett']\n",
    "p29pdf_author_organizations=['Equipe LINKMEDIA','Campus Universitaire de Beaulieu','CNRS/IRISA Rennes','National Institute of Informatics',\n",
    "                             'Equipe MISTIS INRIA','Google']\n",
    "p29pdf_author_locations=['Campus Universitaire de Beaulieu','35042 Rennes Cedex, France','France','-1-2 Hitotsubashi, Chiyoda-ku Tokyo 101-8430, Japan',\n",
    "                        'Japan','6-10-1 Roppongi, Minato-ku Tokyo 106-6126','Inovallée, 655, Montbonnot 38334 Saint-Ismier Cedex','Tokyo']\n",
    "\n",
    "p29pdf_references_authors =['A. A. Balkema','L. de Haan','N. Bingham', 'C. Goldie','J. Teugels','N. Boujemaa', 'J. Fauqueur', 'M. Ferecatu', 'F. Fleuret',\n",
    "                            'V. Gouet', 'B. LeSaux','H. Sahbi','C. Bouveyron', 'G. Celeux', 'S. Girard','J. Bruske', 'G. Sommer',\n",
    "                           'F. Camastra','A. Vinciarelli','S. Coles','J. Costa' ,'A. Hero','T. de Vries', 'S. Chawla','M. E. Houle',\n",
    "                           'R. A. Fisher','L. H. C. Tippett','M. I. Fraga Alves', 'L. de Haan','T. Lin','M. I. Fraga Alves', 'M. I. Gomes','L. de Haan',\n",
    "                           'B. V. Gnedenko',' A. Gupta', 'R. Krauthgamer','J. R. Lee','A. Gupta', 'R. Krauthgamer','J. R. Lee','M. Hein','J.-Y. Audibert',\n",
    "                           'B. M. Hill','M. E. Houle','M. E. Houle','M. E. Houle','M. E. Houle', 'H. Kashima', 'M. Nett','M. E. Houle', 'X. Ma', 'M. Nett',\n",
    "                            'V. Oria','M. E. Houle', 'X. Ma', 'V. Oria','J. Sun','M. E. Houle','M. Nett','H. Jegou', 'R. Tavenard', 'M. Douze','L. Amsaleg',\n",
    "                           'I. Jollie','D. R. Karger','M. Ruhl','J. Karhunen','J. Joutsensalo','Y. LeCun', 'L. Bottou', 'Y. Bengio', 'P. Haner',\n",
    "                           'J. Pickands, III','C. R. Rao','S. T. Roweis','L. K. Saul','A. Rozza', 'G. Lombardi', 'C. Ceruti', 'E. Casiraghi', 'P. Campadelli',\n",
    "                           'B. Scholkopf', 'A. J. Smola','K.-R. Muller','U. Shaft','R. Ramakrishnan',' F. Takens','J. Tenenbaum', 'V. D. Silva','J. Langford',\n",
    "                           'J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. Venna','S. Kaski',\n",
    "                           'P. Verveer','R. Duin','J. von Brunken', 'M. E. Houle', 'A. Zimek','J. von Brunken', 'M. E. Houle','A. Zimek']\n",
    "\n",
    "p29pdf_allauthors=['Laurent Amsaleg','Stéphane Girard','Oussama Chelly','Teddy Furon','Michael E. Houle','Ken-ichi Kawarabayashi',\n",
    "               'Michael Nett','A. A. Balkema','L. de Haan','N. Bingham', 'C. Goldie','J. Teugels','N. Boujemaa', 'J. Fauqueur', 'M. Ferecatu', 'F. Fleuret',\n",
    "                            'V. Gouet', 'B. LeSaux','H. Sahbi','C. Bouveyron', 'G. Celeux', 'S. Girard','J. Bruske', 'G. Sommer',\n",
    "                           'F. Camastra','A. Vinciarelli','S. Coles','J. Costa' ,'A. Hero','T. de Vries', 'S. Chawla','M. E. Houle',\n",
    "                           'R. A. Fisher','L. H. C. Tippett','M. I. Fraga Alves', 'L. de Haan','T. Lin','M. I. Fraga Alves', 'M. I. Gomes','L. de Haan',\n",
    "                           'B. V. Gnedenko',' A. Gupta', 'R. Krauthgamer','J. R. Lee','A. Gupta', 'R. Krauthgamer','J. R. Lee','M. Hein','J.-Y. Audibert',\n",
    "                           'B. M. Hill','M. E. Houle','M. E. Houle','M. E. Houle','M. E. Houle', 'H. Kashima', 'M. Nett','M. E. Houle', 'X. Ma', 'M. Nett',\n",
    "                            'V. Oria','M. E. Houle', 'X. Ma', 'V. Oria','J. Sun','M. E. Houle','M. Nett','H. Jegou', 'R. Tavenard', 'M. Douze','L. Amsaleg',\n",
    "                           'I. Jollie','D. R. Karger','M. Ruhl','J. Karhunen','J. Joutsensalo','Y. LeCun', 'L. Bottou', 'Y. Bengio', 'P. Haner',\n",
    "                           'J. Pickands, III','C. R. Rao','S. T. Roweis','L. K. Saul','A. Rozza', 'G. Lombardi', 'C. Ceruti', 'E. Casiraghi', 'P. Campadelli',\n",
    "                           'B. Scholkopf', 'A. J. Smola','K.-R. Muller','U. Shaft','R. Ramakrishnan',' F. Takens','J. Tenenbaum', 'V. D. Silva','J. Langford',\n",
    "                           'J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. Venna','S. Kaski',\n",
    "                           'P. Verveer','R. Duin','J. von Brunken', 'M. E. Houle', 'A. Zimek','J. von Brunken', 'M. E. Houle','A. Zimek']\n",
    "\n",
    "\n",
    "print \"There are %r authors\" % len(p29pdf_authors)\n",
    "print  # white space\n",
    "print \"There are %r author organizations\" %len(p29pdf_author_organizations)\n",
    "print \n",
    "print \"There are %r author locations\" % len(p29pdf_author_locations)\n",
    "print  \n",
    "print \"There are %r authors in the references\" %len(p29pdf_references_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(compare) & set(p29pdf_allauthors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p29pdf_allauthors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [9] on learning two-level decision trees, whose settings and problem are very dierent from the ones we study.  3. DEFINITIONS AND PRELIMINARIES Let I be a set of items with an arbitrary xed total order <. A transaction is a subset of I, and a transactional dataset is a collection of transactions. An itemset is a set of items that appear together in a transaction (i.e., a subset of a transaction). Given an itemset A and a transaction  s.t. A  , we say that A appears or is contained in  and that  contains A. The support set TD(A) of A in D is the subset of transactions in D that contain the itemset A, and the frequency of itemset A in dataset D is the fraction of transactions of D that contain A:  fD(A) = |TD(A)|/|D| .  Given a frequency threshold   (0, 1], the set FI(D,I, ) of Frequent Itemsets (FIs) in D w.r.t.  is the collection of all itemsets with frequency at least  in D:  FI(D,I, ) = {(A, fD(A)) : A  I  fD(A)  } .  Similarly, let f least k itemsets have frequency at least f set of the top-k FIs is  (k) D be the maximum frequency such that at D in D, then the  (k)  TOPK(D,I, k) = FI(D,I, f  (k) D ) .  Note that |TOPK(D,I, k)|  k. Goal. We aim at approximating the collection of'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[\\s]',\" \",document)[9300:10500]\n",
    "#regexp.search(re.sub('[\\s]',\" \",document)).group(1)[4900:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'locations': ['RI',\n",
       "  'RI',\n",
       "  'Sydney',\n",
       "  'NSW',\n",
       "  'Australia',\n",
       "  'Wrobel',\n",
       "  'Wrobel',\n",
       "  'CIs',\n",
       "  'Ca',\n",
       "  'Thm',\n",
       "  'Thm',\n",
       "  'Thm',\n",
       "  'Thm',\n",
       "  'Liberty et',\n",
       "  'Thm',\n",
       "  'Thm',\n",
       "  'PARMA',\n",
       "  'PARMA',\n",
       "  'sion',\n",
       "  'MapReduce',\n",
       "  'New York',\n",
       "  'NY',\n",
       "  'USA'],\n",
       " 'organizations': ['Computer Science Brown University Providence',\n",
       "  'Computer Science Brown University Providence',\n",
       "  'FIs',\n",
       "  'FIs',\n",
       "  'ACM',\n",
       "  'FIs',\n",
       "  'CIs',\n",
       "  'CIs',\n",
       "  'FIs',\n",
       "  'FIs',\n",
       "  'Def',\n",
       "  'Def',\n",
       "  'Def',\n",
       "  '2n2',\n",
       "  'CIs',\n",
       "  'SA',\n",
       "  'B SA',\n",
       "  'CIs',\n",
       "  'CIs',\n",
       "  'CIs',\n",
       "  'aIS',\n",
       "  'aIS',\n",
       "  'aIS',\n",
       "  'ACa',\n",
       "  'aIS',\n",
       "  'ACa',\n",
       "  'nal',\n",
       "  'Si1',\n",
       "  'AMD',\n",
       "  'GNU',\n",
       "  'Linux',\n",
       "  'FIs',\n",
       "  'Def',\n",
       "  'FI',\n",
       "  'BMS',\n",
       "  'BMS',\n",
       "  'NSF',\n",
       "  'Swami',\n",
       "  'Yan',\n",
       "  'Data Mining',\n",
       "  'Langley',\n",
       "  'IEEE',\n",
       "  'Liberty',\n",
       "  'Mining',\n",
       "  'Mining',\n",
       "  'PARMA',\n",
       "  'ACM',\n",
       "  'Springer',\n",
       "  'Verlag',\n",
       "  'University Press'],\n",
       " 'persons': ['Matteo Riondato',\n",
       "  'matteo',\n",
       "  'cs.brown.edu Eli Upfal',\n",
       "  'KDD15',\n",
       "  'Chen',\n",
       "  'Chuang',\n",
       "  'thm',\n",
       "  'Pietracaprina',\n",
       "  'Scheer',\n",
       "  'Scheer',\n",
       "  'Pietracaprina et',\n",
       "  'Cherno',\n",
       "  'David',\n",
       "  'Elomaa',\n",
       "  'aX',\n",
       "  'X',\n",
       "  'Si1',\n",
       "  'Liberty',\n",
       "  'Riondato',\n",
       "  'Riondato',\n",
       "  'Grahne',\n",
       "  'Zhu',\n",
       "  'matteo',\n",
       "  'radeprogrfi.tar.bz2',\n",
       "  'errorrelative',\n",
       "  'error1012',\n",
       "  'Agrawal',\n",
       "  'Agrawal',\n",
       "  'Mining',\n",
       "  'Liu',\n",
       "  'Hu',\n",
       "  'Liu',\n",
       "  'Eciently',\n",
       "  'Bartlett',\n",
       "  'Lugosi',\n",
       "  'Lugosi',\n",
       "  'Pandit',\n",
       "  'Chen',\n",
       "  'Haas',\n",
       "  'Chuang',\n",
       "  'Chen',\n",
       "  'Yang',\n",
       "  'Kriinen',\n",
       "  'J',\n",
       "  'Zaki',\n",
       "  'SIGKDD',\n",
       "  'Grahne',\n",
       "  'Zhu',\n",
       "  'Han',\n",
       "  'Pei',\n",
       "  'Han',\n",
       "  'Cheng',\n",
       "  'John',\n",
       "  'Johnson',\n",
       "  'J',\n",
       "  'Thaler',\n",
       "  'Ullman',\n",
       "  'CoRR',\n",
       "  'Jensen',\n",
       "  'Oates',\n",
       "  'Ecient',\n",
       "  'Fonseca',\n",
       "  'Rademacher',\n",
       "  'Webb',\n",
       "  'Scheer',\n",
       "  'Ben',\n",
       "  'David']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Matteo Riondato',\n",
       " 'matteo',\n",
       " 'cs.brown.edu Eli Upfal',\n",
       " 'KDD15',\n",
       " 'Chen',\n",
       " 'Chuang',\n",
       " 'thm',\n",
       " 'Pietracaprina',\n",
       " 'Scheer',\n",
       " 'Scheer',\n",
       " 'Pietracaprina et',\n",
       " 'Cherno',\n",
       " 'David',\n",
       " 'Elomaa',\n",
       " 'aX',\n",
       " 'X',\n",
       " 'Si1',\n",
       " 'Liberty',\n",
       " 'Riondato',\n",
       " 'Riondato',\n",
       " 'Grahne',\n",
       " 'Zhu',\n",
       " 'matteo',\n",
       " 'radeprogrfi.tar.bz2',\n",
       " 'errorrelative',\n",
       " 'error1012',\n",
       " 'Agrawal',\n",
       " 'Agrawal',\n",
       " 'Mining',\n",
       " 'Liu',\n",
       " 'Hu',\n",
       " 'Liu',\n",
       " 'Eciently',\n",
       " 'Bartlett',\n",
       " 'Lugosi',\n",
       " 'Lugosi',\n",
       " 'Pandit',\n",
       " 'Chen',\n",
       " 'Haas',\n",
       " 'Chuang',\n",
       " 'Chen',\n",
       " 'Yang',\n",
       " 'Kriinen',\n",
       " 'J',\n",
       " 'Zaki',\n",
       " 'SIGKDD',\n",
       " 'Grahne',\n",
       " 'Zhu',\n",
       " 'Han',\n",
       " 'Pei',\n",
       " 'Han',\n",
       " 'Cheng',\n",
       " 'John',\n",
       " 'Johnson',\n",
       " 'J',\n",
       " 'Thaler',\n",
       " 'Ullman',\n",
       " 'CoRR',\n",
       " 'Jensen',\n",
       " 'Oates',\n",
       " 'Ecient',\n",
       " 'Fonseca',\n",
       " 'Rademacher',\n",
       " 'Webb',\n",
       " 'Scheer',\n",
       " 'Ben',\n",
       " 'David']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class entities(object):\n",
    "  def __init__(self):\n",
    "    self.persons = extraction(document)['persons']\n",
    "    self.organizations = extraction(document)['organizations']\n",
    "\n",
    "my_shape = entities()\n",
    "my_shape.persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to extract emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('matteo@cs.brown.edu', 'eli@cs.brown.edu', 'ermissions@acm.org')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(get_emails(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get only the Title and Author Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mining Frequent Itemsets through Progressive Sampling  with Rademacher Averages  Matteo Riondato  Dept. of Computer Science  Brown University  Providence, RI 02912  matteo@cs.brown.edu  Eli Upfal  Dept. of Computer Science  Brown University  Providence, RI 02912 eli@cs.brown.edu  '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=re.compile('(.*)(?=ABSTRACT)')\n",
    "p.search(re.sub('[\\s]',\" \",document)).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get Title Only (or most of it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mining Frequent Itemsets through Progressive Sampling'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=re.compile('(.+)(\\\\n\\\\n)')\n",
    "q=re.compile('(?<=\\\\n\\\\n)(.+?)(?=\\\\n\\\\n)')\n",
    "#p.search(document).group(1)+\" \"+q.search(document).group(1)\n",
    "p.search(document).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get Abstract only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' We present an algorithm to extract an high-quality approx- imation of the (top-k) Frequent itemsets (FIs) from ran- dom samples of a transactional dataset. With high prob- ability the approximation is a superset of the FIs, and no itemset with frequency much lower than the threshold is in- cluded in it. The algorithm employs progressive sampling, with a stopping condition based on bounds to the empirical Rademacher average, a key concept from statistical learning theory. The computation of the bounds uses characteris- tic quantities that can be obtained eciently with a sin- gle scan of the sample. Therefore, evaluating the stopping condition is fast, and does not require an expensive mining of each sample. Our experimental evaluation conrms the practicality of our approach on real datasets, outperforming approaches based on one-shot static sampling.  '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= re.compile('(?<=ABSTRACT)(.+)(?=Categories and Subject Descriptors)')\n",
    "p.search(re.sub('[\\s]',\" \",document)).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get keywords only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Frequent Itemsets; Pattern Mining; Rademacher Averages; Sampling; Statistical Learning Theory  1.  '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= re.compile('(?<=Keywords)(.+)(?=INTRODUCTION)')\n",
    "p.search(re.sub('[\\s]',\" \",document)).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get Categories and Subject Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Database Management']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= re.compile('(?<=Categories and Subject Descriptors)(.+)(?=Keywords)')\n",
    "re.findall('\\[(.*?)\\]',p.search(re.sub('[\\s]',\" \",document)).group(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get Body only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  The task of Frequent Itemsets (FIs) mining is to extract all sets of items that appear in at least a fraction  of a transac- tional dataset D, or the k most frequent set of items [2]. It is a fundamental primitive of knowledge discovery and is use- ful, among the others, for market basket analysis, inference, classication, and network management [13]. Exact algo- rithms to mine FIs have since long been available but their  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full cita- tion on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specic permission and/or a fee. Request permissions from Permissions@acm.org. KDD15, August 10-13, 2015, Sydney, NSW, Australia. c(cid:13) 2015 ACM. ISBN 978-1-4503-3664-2/15/08 ...$15.00. DOI: http://dx.doi.org/10.1145/2783258.2783265 .  practicality is hindered by the need to scan the dataset mul- tiple times [1, 12]. When the dataset is too large to t into main memory, as it is the case for many modern datasets, the running time of exact FIs mining algorithms may be too high to be practical. A natural way to reduce the depen- dency on the dataset size is to only analyze a small random sample of the dataset that can reside in main memory. The collection of FIs obtained from the sample will be an approx- imation to the exact collection, due to the fact that only a subset of the dataset is analyzed. Approximate collections of FIs are nevertheless acceptable in most cases due to the exploratory nature of the FIs mining step in the knowledge discovery process. There is an intrinsic trade-o between the size of the sample (number of transactions in the sam- ple) and the accuracy of the estimation, but a loose analysis of this trade-o may result in sample sizes much larger than what is needed to obtain and approximation with the desired level of accuracy and condence. It is therefore necessary, although challenging, to develop algorithms that leverage on tight bounds to the trade-o between sample size and accuracy in order to fully exploit the power of sampling. Contributions. In this work we study the trade-o be- tween approximation quality and sample size using concepts and results from statistical learning theory [29]. We present a randomized algorithm to mine a high-quality approximation of the collection of FIs w.r.t. a minimum frequency thresh- old  (and of the top-k most frequent itemsets) from random samples of the dataset D. With probability at least 1, for some user-specied   (0, 1), the returned approximation is a superset of the exact collection of FIs and no itemset in the approximation may have frequency less than   , for some user-specied   (0, 1). Moreover, the estima- tion of the frequency of all itemsets in the output is within /2 of their exact value. The algorithm uses progressive sampling, i.e., it starts from a small sample and enlarges it until a suitable stopping condition is veried, meaning that an high-quality approximation can be obtained from the sample. The stopping condition is based on bounds to the empirical Rademacher average of the problem at hand, a key concept from statistical learning theory [5]. In partic- ular we prove that we can bound the empirical Rademacher average and therefore the maximum deviation between the frequency of an itemset in the dataset and the frequency of that itemset in the sample using a function of the sample size, of , and of a partitioning of the set of Closed Itemsets (CIs) [19] in the sample. We also give a bound, which is of independent interest, to the number of CIs in the sam- ple. To our knowledge this is the rst algorithm that uses  1005 bounds on the empirical Rademacher average in the domain of pattern mining, and one of the rst to adapt these highly theoretical concepts to develop an ecient algorithm for an important practical task. We conducted an extensive ex- perimental evaluation to test our algorithm and assess its performances in terms of the quality of the returned col- lection of itemsets and of the runtime, comparing it with standard baselines. Outline. We start by reviewing related works in Sect. 2. We then formalize the problem of FIs mining and formally dene the concept of approximation in Sect. 1. Our algorithm and its analysis are presented in Sect. 4. The goals, methodology, and results of the experimental evaluation can be found in Sect. 5. Finally, we draw some conclusions and suggest some future directions in Sect. 6.  2. RELATED WORK  The idea of using random samples to speed up the extrac- tion of FIs has been studied since shortly after the rst e- cient exact algorithms had been presented [28]. Many works focused on deriving bounds for the size of a single sample to obtain high-quality approximation. Riondato and Upfal [24] present what is currently the best available bound. We refer the interested reader to their extensive discussion of previous results on xed sample sizes and we focus here on the works that examined progressive sampling [7, 8, 14, 18, 20, 21, 26]. The use of progressive sampling, in contrast with a sam- ple of xed size, can contribute to an even greater speed up of the extraction of FIs, especially when combined with an appropriate schedule and starting sampling size [3, 14, 21]. Developing a stopping rule that allows to obtain approxi- mations of guaranteed quality is a challenging task. Chen et al. [7], Parthasarathy [18], and Chuang et al. [8] pro- pose progressive-sampling-based algorithms that use heuris- tics based on self-similarity or the frequency of single items to determine the stopping sampling size. Because of the use of heuristics, these approaches oer no guarantee on the quality of the obtained collection. In contrast, our algori- thm returns, with high probability, a collection of itemsets with strong approximation guarantees.  Pietracaprina et al. [20] and Scheer and Wrobel [26] fo- cuses on extracting the top-k most frequent itemsets us- ing progressive sampling. The stopping condition suggested by Scheer and Wrobel [26] employs progressive ltering of the set of candidate FIs based on Cherno bounds until only k itemsets are left, but oers no guarantee on whether the returned collection contains any of the actual top-k FIs, rather a much weaker guarantee is oered. Our algorithm instead guarantees that the returned collection of itemsets is a superset of the top-k FIs. The algorithm by Pietracaprina et al. [20] uses a stopping condition based on the frequency of all itemsets in the sample. The analysis is based on tra- ditional Cherno and union bounds and limited to itemsets up to a xed length. Our algorithm does not suer from this limitation and our analysis uses powerful deviation bounds based on Rademacher averages.  Another major point of dierence between our work and the ones previously presented is the fact that checking the stopping condition of our algorithm does not require to run an exact FI mining algorithm on the sample. As a con- sequence, our stopping condition is much more ecient to evaluate, resulting in lower running time.  We use bounds to the Rademacher averages [4, 16], an  important concept from statistical learning theory. We only introduce the necessary notation and results, and we refer the reader to the book by Shalev-Shwartz and Ben-David [27] for an in-depth presentation of these topics.  To the best of our knowledge, the only previous use of bounds or estimates of the Rademacher averages in a pro- gressive sampling setting is the work by Elomaa and Krii- nen [9] on learning two-level decision trees, whose settings and problem are very dierent from the ones we study.  3. DEFINITIONS AND PRELIMINARIES Let I be a set of items with an arbitrary xed total order <. A transaction is a subset of I, and a transactional dataset is a collection of transactions. An itemset is a set of items that appear together in a transaction (i.e., a subset of a transaction). Given an itemset A and a transaction  s.t. A  , we say that A appears or is contained in  and that  contains A. The support set TD(A) of A in D is the subset of transactions in D that contain the itemset A, and the frequency of itemset A in dataset D is the fraction of transactions of D that contain A:  fD(A) = |TD(A)|/|D| .  Given a frequency threshold   (0, 1], the set FI(D,I, ) of Frequent Itemsets (FIs) in D w.r.t.  is the collection of all itemsets with frequency at least  in D:  FI(D,I, ) = {(A, fD(A)) : A  I  fD(A)  } .  Similarly, let f least k itemsets have frequency at least f set of the top-k FIs is  (k) D be the maximum frequency such that at D in D, then the  (k)  TOPK(D,I, k) = FI(D,I, f  (k) D ) .  Note that |TOPK(D,I, k)|  k. Goal. We aim at approximating the collection of (top-k) FIs by mining (i.e., extracting the FIs from) random samples of D (i.e., random collections of transactions from D).  Definition 1. For ,   (0, 1), a (, )-approximation of FI(D,I, ) is a collection C = {(A, fA) : A  I, fA  (0, 1]} such that, with probability at least 1  :  C; and  1. for any (A, fD(A))  FI(D,I, ) there is a pair (A, fA)  2. for any (A, fA)  C, it holds fD(A)    ; and 3. for any (A, fA)  C, it holds |fD(A)  fA|  /2.  An (, )-approximation of FI(D,I, f of TOPK(D,I, k).  (k) D ) is an (, )-approximation  4. A PROGRESSIVE SAMPLING ALGORI-  THM WITH GUARANTEES  We want to compute an (, )-approximation to FI(D,I, ) (or to TOPK(D,I, k)) from random samples of D of progres- sively increasing size (i.e., through progressive sampling). In the rest of this section we focus on FI(D,I, ), while the case for top-k FIs is presented in Sect. 4.5.  The basic steps of the iterative progressive sampling pro-  cess are:  1. at iteration i, create a random sample Si of some pre- dened size |Si| by drawing transaction uniformly and independently at random (with replacement) from D;  1006 X  D  nX  i=1  2. check a stopping condition to determine if an (, )- approximation of FI(D,I, ) can be extracted from Si; 3. if the stopping condition is satised, return the collec- tion FI(Si,D, ) for an appropriate value of  and exit, otherwise increase i and return to step 1.  In order to obtain an algorithm from this high-level de- scription, it is necessary to formally specify the following components:  1. a sampling schedule (|Si|)i1 of sample sizes. 2. a stopping condition involving the sample Si, and an ecient procedure to check this condition. 3. a revised minimum frequency threshold . Any non-decreasing sequence (|Si|)i1 can act as a sample schedule, giving complete freedom to the algorithm designer In Sect. 4.6 we show how to and the user in this sense. compute the next sample size using information from the current sample. The choice of  and of the stopping condition are inter- twined. We choose  = /2, as motivated by the following lemma, and this choice denes rigorous requirements for the stopping condition.  Lemma 1. Let S be a sample of D, and consider the event (1)  ES : |fD(A)  fS(A)|    2 for all A  I .  If  (2) then the collection FI(S,I,  /2) is a (, )-approximation to FI(D,I, ).  Pr(ES)  1  ,  Proof. Assume that the event ES in (1) is veried, which happens by hypothesis with probability at least 1 . Then for no itemset B  FI(D,I, ) we may have fS(B) <  /2, hence B  FI(S,I,   /2), as required by property 1 from Def. 1. Let C be any itemset with fD(C) <   . We have that fS(C) <   /2, so C / FI(S,I,   /2), which is the condition specied by property 2 from Def. 1. Property 3 from Def. 1 follows from the fact that the event ES is veried.  This lemma gives the intuition behind the stopping condi- tion of our algorithm: we can stop when (2) holds for the sample S under consideration, as we can then use  = /2 to extract FI(D,I, ), which is an (, )-approximation to FI(D,I, ).  The rest of this section is devoted to formalize this condi- tion and derive a procedure to check whether (2) holds for a sample S. with probability at least 1  ,  Checking whether (2) holds is equivalent to checking whether,  sup AI  |fD(A)  fS(A)|   2 , hence we focus on bounding this quantity. 4.1 Rademacher averages A : 2I  {0, 1} as:  (cid:26) 1 if A  B  A(B) =  0 otherwise for any A  I, B  I .  For each itemset A  I, we dene the indicator function  When B is a transaction, A(B) = 1 if the itemset A appears in the transaction B. Hence, we have  fD(A) = 1 |D|  A()  and analogously for the frequency fS(A) of A in a sample S. Assume that the sample S has size |S| = n. For each i  S, 1  i  n, let i be a Rademacher random variable, i.e., a random variable taking value 1 or 1, each with prob- ability 1/2. The random variables i are independent. The (sample) conditional Rademacher average is the quantity  \"  #  RS = E  1 n  sup AI  iA(i)  ,  where E denotes the expectation taken only w.r.t. the ran- dom variables i, 1  i  n (i.e., conditionally on the sam- ple) [5, 16]. An important result from statistical learning theory bounds the supremum of the deviations with the con- ditional Rademacher average.  (Thm. 3.2 [5]). With probability at least  Theorem 1  1  ,  |fD(A)  fS(A)|  2RS +  sup AI  Note that the bound in the above theorem depends only on properties of the sample. Computing RS directly is not easy. It would rst require to mine all itemsets from S (i.e., extracting FI(S,I, 1/|S|), which is excessively expensive, and then to nd the expec- tation over the i variables. Given that no analytical meth- ods are currently available to compute this expectation in general, this second step would require an expensive Monte- Carlo simulation [5]. Nevertheless a dierent result from statistical learning theory allows us to bound RS using com- binatorial properties of the sample. For any itemset A  I, let vS(A) be the n-dimensional vector  vS(A) = (A(1), . . . , A(n)),  and let VS = {vS(A), A  I}. Since VS is a set rather than a bag, we have |VS|  2|I| (and potentially |VS| (cid:28) 2|I|). (Massarts Lemma, Thm. 3.3 [5]).  Theorem 2  r2 ln(2/)  .  n  p2 ln |VS|  ,  RS  max AI  kvS(A)k  n where k  k denotes the Euclidean norm.  Although the above is the form in which the Theorem is usually stated, a careful reading of its proof allows us to state the following stronger version.  Theorem 3. Let w : R+  R+ be the function 2)) .  w(s) = 1  2kvk2  exp(s  /(2n  ln X  (3)  Then  s  vVS  RS  min sR+  w(s) .  1007 Proof. As in the proof for [5, Thm. 3.3] we can use the independence of the is and the Hoedings inequality to show that, for any s > 0 and for any itemset A  I, we have  \"     1 n  s  nX  !#  E  exp  iA(i)   exp  We can use this inequality to write  (cid:19)  .  2n2  (cid:18) s2kvS(A)k2 #! !# !#  iA(i)  iA(i)  iA(i)  (cid:19)  .  esRS = exp  sE  i=1    \" X X   E  AI  AI  exp  E  exp  i=1  1 n  max AI  \" nX   nX   \" nX (cid:18) s2kvS(A)k2  s max AI  exp  1 n  1 n  i=1  i=1  s  2n2  We can now take the logarithm on both sides and divide by s (which is strictly positive) and we obtain w. Since the above inequalities are true for any s > 0, we can choose the one that minimizes the r.h.s. to obtain the thesis.  As we show later, computing the function w is too ex- pensive for our purposes as it requires the computation of the set VS, therefore, in the following, we develop an upper bound to w that is easy and fast to compute. 4.2 Connection with Closed Itemsets  collection of Closed Itemsets [19].  We now show a connection between the set VS and the We recall that a Closed Itemset (CI) is an itemset A  I such that none of its proper supersets has the same fre- quency of A (i.e., there is no B (cid:41) A s.t. fS(B) = fS(A)) [19]. Let CI(S) be the set of CIs in the sample.  Lemma 2. The set VS contains all and only the vectors  vS(A) for all A  CI(S):  VS = {vS(A), A  CI(S)}, and |VS| = |CI(S)| .  To prove Lemma 2, we need the following result. Lemma 3. Let S  S. There is at most one CI A in S  whose support set in S is TS(A) = S. Proof. Suppose that there could be more than one CI in S with support set S, for example, w.l.o.g., two itemsets C and D. Then the support set of C  D in S would be exactly S, so C and D can not be closed, as there is a superset of them with the same support set. We reached a contradiction, so the thesis is true.  We can now prove Lemma 2. Proof of Lemma 2. Let A be a CI in S, and let SA be the set of subsets of A with the same frequency in S as A:  SA = {B  A : fS(B) = fS(A)} .  The elements of SA are the itemsets that appear in all and only the transactions of S where A appears. This means that, for all B  SA, vS(B) = vS(A). To conclude the proof it is sucient to show that there can not be two CIs C and D in S s.t. vS(C) = vS(D). This is an immediate consequence of Lemma 3 and the proof is complete.  This result explains why computing the function w from (3) is expensive: we would need to extract the set CI(S) of all CIs in the sample (i.e., mine the sample at frequency 1/|S|). In the following we develop an upper bound to w that can be computed eciently with a single scan of the sample. 4.3 Bounding the Rademacher Average In this section we show how to eciently bound the con- ditional Rademacher average RS. To do so, we dene a function w which is an upper bound to w from (3) in every point of R+. The advantage of w is that it can be com- puted using just the frequencies in the sample of the items in I and some additional information that can be obtained with a single scan of the sample. To dene w we need a partitioning of CI(S) that we now introduce. Assume to sort the items in IS in increasing order by their frequency in S, ties broken arbitrarily (e.g., according to the order < on I). Let <i denote the resulting ordering. Given an item a, assume to sort the transactions of TS({a}) in increasing order by the number of items they contain that come after a in the ordering <i, ties broken arbitrarily (e.g., using unique transaction identiers). Let <a denote the resulting ordering. Let C1 = CI(S)  IS and C2+ be the subset of CI(S) con- taining only the CIs of size at least two. We partition C2+ as follows. Let A  C2+ and let a  A be the item in A that comes before any other item in A according to the order <i. Let  be transaction containing A that comes before any other transaction containing A in the order <a. Clearly a  . We assign A to the set Ca,. Consider now a transaction   TS({a}), and assume that it contains exactly ka, items that come after a in the order- ing <i. In the ordering <a, the transaction  comes  1. before all transactions with more than ka, items that  come after a in the ordering <i and  2. before zero or more of the transactions with exactly ka, items that come after a in the ordering <i (the exact number of such transactions depends on the tie- breaking criteria).  For each r  1, let ga,r be the number of transactions in TS({a}) containing exactly r items that come after a in the ordering <i. Let a = max{r : ga,r > 0} and let  aX  ha,r =  ga,j .  jr  The value a is the maximum r such that there exists at least one transaction in TS({a}) containing exactly r items that come after a in the order <i. Each value ha,r is the number of transactions in TS({a}) that contain at least r items that come after a in the order <i. Now, assume that  is the a,-th transaction in the or- dering <a that contains exactly ka, items that come after a in the ordering <i. In other words, if we consider only the transactions containing exactly ka, items that come after a in the ordering <i, then  is the a,-th of such transactions in the ordering <a. We have the following result on the size of Ca,.  Lemma 4. We have  |Ca,|  2min{ka, ,ha,ka, a, }  .  1008 Proof. The quantity 2ka, is the number of subsets B of  such that B = {a}  C where C is any non-empty subset of  containing only items that come after a in the order <i. Since Ca, contains only itemsets that appear in  and are in the form of B, then |Ca,|  2ka, . Consider now an itemset A  Ca,. Apart from , A can only appear in transactions 0  TS({a}) such that  <a 0, as A = {a} C, for C as above. This is true for any itemset A  Ca,. Let T denote the set of such transactions, then |T | = ha,ka,  a,. From Lemma 3 we have that there is at most one CI for each set D = {}  F of transactions, where F  T , so there at most 2ha,ka, a, CIs in Ca,.  From Lemma 4 and the fact that  CI(S) = C1  C2+ = C1   [  aIS  [  Ca,  TS({a})   (4)  we have the following result on the number of Closed Item- sets in S, which is of independent interest.  Corollary 1.  |CI(S)|  |IS| + X  X  aIS  TS({a})  2min{ka, ,ha,ka, a, }  .  The following lemma puts together the above results to  obtain an upper bound to RS.  Lemma 5. Let w : R+  R+ be the function      lnX  aX  ga,rX  aIS  r=1  j=1  1 +  2min{r,ha,ri}  !  !  .  s2fS (a)  2n  e  w(s) = 1  s  Then  RS  min sR+  w(s) .  Proof. Consider the function w from (3). From the denition of Euclidean norm, we have that, for any A   I, kvS(A)k = pnfS(A). Using this fact and combining  .  Lemma 2 and the equality from (4), we can rewrite w as  2n + X  w(s) = 1  X  X  s2fS (A)  s2fS (a)  ln  2n  e  e  s  aIS  TS(a)  ACa,  We now show that w(s)  w(s) for any s  R+. The thesis will then follow from Thm. 3.  First of all, since C1  IS, we have  e  s2 fS (a)  2n  X 2n  X  aIS  s2fS (A)  e  Then, for any a  IS,  X  X  s2fS (a)  2n  e  .  2min{ka, ,ha,ka, a, }  e  X X  aC1  aC1  ACa,  TS({a})  TS({a}) where we used Lemma 4 to bound the size of Ca, and the fact that for any A  Ca,, fS(A)  fS(a), given the anti- monotonicity property of the frequency. Finally, we can rewrite the right-hand side of this last  equation as  aX  ga,rX  r=1  j=1  2min{r,ha,rj}  e  s2fS (a)  2n  .  r2 ln(2/) |Si|   2 .  By combining these equations we have that w(s)  w(s) for any s  R+, and the thesis follows from Thm. 3.  We are now ready to formally state our stopping con- dition that guarantees that an (, )-approximation can be computed when the condition is satised.  Theorem 4  (Stopping condition). Let i be the min-  imum index for which it holds that  ) +  2 w(s  (5) Then FI(Si,I, /2) is an (, )-approximation to FI(D,I, ). Proof. The proof follows by combining Lemma 1, Thm. 1,  Thm. 3, and Lemma 5. 4.4 Computing the bound  We now discuss how it is possible to check the stopping condition with a single scan of the sample. In particular, it is possible to obtain the expression for w with a single scan, then its minimum of w can be found by computing the value s which minimizes w.  Computing w. To compute the expression for w we only need the quantities ga,k and ha,r for any a  IS and for all r, 1  r  a. These can be computed with a single scan of the sample. Indeed, the order <i can be obtained from the frequencies of the items in the sample, which we assumed to have been computed during the sample creation. Then, it is sucient to look at each transaction  once, sort its items according to the order <i and, for any item a  , increment ga,ka, by one and increase by one all counters ha,r for 1  r  ka,.  Minimizing w. The function w has rst and second deriva- tives w.r.t. s everywhere in R+ and it is convex, so it has a global minimum which can be found eciently using a non-linear optimization solver like NLopt [15].  Algorithm 1 presents the pseudocode of our progressive sampling algorithm to compute an (, )-approximation to FI(D,I, ). The function random_sample(D, m) returns m transactions sampled at random with replacement from D. 4.5 Top-k Frequent Itemsets  Only minor modications are needed to obtain an algori- thm for computing (, )-approximations to the set of top-k FIs. The main dierences from the algorithm presented in the previous section are: 1. a stricter stopping condition; and 2. the need to run an exact mining algorithm on the - nal sample twice, one to nd the top-k-th highest frequency (k) in the sample and one to extract the approximation at f S a lowered frequency threshold that depends on f  (k) S .  Theorem 5. Let i be the minimum index for which it  s2fS (a)  2n  , holds that  r2 ln(2/) |Si|   4 ,  2 w(s  ) +  (k) Si  and let f quent itemset in Si. Then FI(Si,I, f approximation to TOPK(D,I, k).  be the frequency in Si of the k-th most fre-  /2) is an (, )-  (k) Si  The proof leverages on Thm. 4, following the same steps  as the proof for [24, Lemma 5.3].  1009 Algorithm 1: Progressive sampling algorithm input : a dataset D built on alphabet I, parameters , ,   (0, 1), a sampling schedule (|Si|)i1. output: An (, )-approximation to FI(D,I, ) i  0, S0  , |S0|  0 repeat  i  i + 1 S random_sample (D, |Si|  |Si1|) Si  Si1  S //We assume that the frequencies of the items have ga,r  0,a  ISi , r  N ha,r  0,a  ISi , r  N for   Si do  been computed while creating the sample.  for a   do  ka,  number of items in  that come after a in the order <a ga,ka,  ga,ka, + 1 for j  1, . . . , ka, do ha,j  ha,j + 1 end  end a  max{r : ha,r > 0}  end //In the following expression, s is a symbol. w(s)   !  1 +  2min{r,ha,rj}  s2fS (a)  2n  e      X  aIS  ln  !  aX ga,rX q 2 ln(2/)  r=1  j=1  s  s  arg minsR+ w(s)   2 w(s) + |Si| until   /2 return FI(Si,I,   /2)  4.6 Selecting the sampling schedule  Any non-decreasing sequence (|Si|)i1 can act as a sam- pling schedule and Provost et al. [21] showed that a geo- metric sampling schedule (i.e., a schedule where Si = iS0 for some constant ) is asymptotically optimal when check- ing the stopping condition takes time O(|Si|). Nevertheless, even such a schedule requires the user to specify two param- eters: an initial sample size S0, and a growth rate  > 1. In our case it is possible to avoid forcing the choices of these parameters to the user, and instead allow the algori- thm to select an initial sample size and then choose succes- sive sample sizes based on the quality of the current sample. This has the net result of removing two parameters from the algorithm.  Choosing the initial sample size. We ask whether it is possible to choose the initial sample size wisely so that the algorithm does not waste time in creating and analyzing samples that are just too small for the stopping condition to be satised (exceeding in the other direction, i.e., hav- ing an initial sample size that is a bit too large, is not a signicant issue). In our case it is possible to compute the necessary initial sample size S 0, i.e., the minimum sample size which makes it possible for the stopping condition to be satised. In other words, for sample size smaller than S 0 it  is deterministically impossible that the stopping condition is satised, and therefore it is useless to create and analyze samples smaller than S 0.  Lemma 6. Let  0 = 8 ln(2/)   2  S  (6)  The stopping condition (5) from Thm. 4 can not be satised on samples with size smaller than S 0.  Proof. Assume that there exists a sample S of size smaller 0 for which the stopping condition (5) in Thm. 4 can  than S be satised. For such a sample, we have  r2 ln(2/)  |S|  >   2 .  From this and the fact that w(s)  0, we have that the stop- ping condition can not be satised, so we reached a contra- diction and the thesis holds.  Computing the size of the next sample. We can exploit the information obtained from mining the current sample to compute a meaningful sample size for the next iteration. Assume to be at iteration i  0 of the algorithm, and let i be the value of the l.h.s. of (5) at the end of the current iteration, and let |Si| be the size of the sample used in iteration i. Then at the next iteration i + 1 we use a sample of size |Si+1|, with |Si+1| =  (cid:17)2 |Si| .  (cid:16)2i  (7)    The intuition behind the above formula is that i is, through Thm. 1, an upper bound to the maximum deviation be- tween the frequency in Si of any itemset and the frequency of that itemset in the original dataset. There is a necessary quadratic dependency between this measure and the sample size [17], hence we can use i and |Si| to compute a sample size |Si+1| for which, everything else unvaried, the error al- lowed in a sample of that size (i.e., the l.h.s. of (5)) would be at most /2, as required by the stopping condition of our algorithm.  Although the method we just presented does not give any guarantee on the optimality of the schedule, our experimen- tal evaluation results (Sect. 5) show that is highly eective and results in a faster execution of the algorithm than us- ing a geometric sample schedule, thanks to the fact that intermediate sample sizes that are probably not sucient for computing an (, )-approximation are skipped. 4.7 Discussion  To the best of our knowledge, our algorithm improves over all progressive-sampling approaches previously presented in the literature [7, 8, 14, 18, 20, 21, 26] because it does not require the execution of any expensive Frequent Itemsets mining algorithm on each sample to check the stopping con- dition. Indeed the computation of the stopping condition only requires one scan of the sample. More straightforward stopping conditions with the same requirements are possi- ble: we explored a number of them, both empirically and theoretically, and found them substantially looser (i.e., sat- ised only at larger sample size) than the one presented in  1010 this work. We plan to include a presentation of these al- ternative sub-par stopping conditions, and the comparison of their performances with the one from our algorithm in the extended version of this work. It is indeed necessary to strike a delicate balance between the speed of checking the stopping condition and its strictness (i.e., how early it be- comes satised), otherwise the advantages of using sampling rather than analyzing the entire dataset are lost.  As the stopping condition does not depend in any way on , this parameter can be xed at a later stage. This is again a consequence of the fact that we do not need to run a mining algorithm on the sample to check the stopping condition.  We remark that the dependency on 1/2 of the sample  size can not be improved, as shown by Liberty et al. [17]. 4.8 Static-sampling variant  A variant of the approach presented in previous sections can be used in a static-sampling fashion. Consider the fol- lowing scenario: rather than having access to the entire dataset D and being able to sample from it as much as de- sired, we are given a single random sample S of the dataset of some size n, a xed parameter   (0, 1), and a mini- mum frequency threshold   (0, 1]. The task requires to compute an (, )-approximation to FI(D,I, ) for the best possible . No access to the dataset is possible and no other information about the dataset is available. This scenario is realistic and actually common, as it may be easy to create (and maintain) one single random sample of the dataset of a specic size while the dataset is created, while obtaining access to the entire dataset may not be feasible. Previous approaches like those presented by Riondato and Upfal [24] and Chakaravarthy et al. [6] rely on characteristic quantities of the dataset (e.g., the d-index [24], or the longest transac- tion in the dataset [6]) to compute a single sample size that allows to obtain the desired quality guarantees. Computing such quantities require scanning the entire dataset. Not only this may be extremely expensive for modern datasets, but it is not even possible in the scenario we just described. These approaches would then be useless in this scenario, as they have no information on the characteristic quantities they need. On the other hand, our approach only uses sample- dependent quantities (namely, the distribution of the sample frequencies of single items and related quantities), and can therefore compute the best (i.e., smallest)  obtainable from the given sample S. Indeed, it follows from Thm. 4 that such value is  r2 ln(2/) r2 ln(2/)  |Si|  (cid:19) (cid:19)  |Si|  (8)  from Thm. 5  .  .  (cid:18) (cid:18)   = 2  2 w(s  ) +   = 4  2 w(s  ) +  Similarly, to approximate TOPK(D,I, k):  we get  Even if we relax the scenario and assume that the algori- thm by Riondato and Upfal [24] (currently the best available for static sampling) has knowledge of an upper bound to the d-index of the dataset, the results of our experimental eval- uation (Sect. 5) show that the value for  computed by our approach using (8) is consistently (although not always) bet- ter (i.e., smaller) than the one computed by the algorithm in [24].  Riondato et al. [22] presented PARMA, a MapReduce al- gorithm for mining approximate collections of FIs. The variant presented in this section can be used in PARMA to obtain even higher-quality approximation or even smaller samples.  5. EXPERIMENTAL EVALUATION  We evaluate the performances of our algorithm by assess- ing the accuracy of the returned collection of FIs and by evaluating the algorithm runtime, comparing it with the time needed to extract the exact collection of FIs and to extract an approximate collection with the same guarantees using the algorithm by Riondato and Upfal [24] (from now on denoted as VC, as it is based on VC-dimension). We choose to compare to this static sampling approach rather than to other existing progressive sampling approaches due to the fact that no existing progressive sampling approach oers the same guarantees of our algorithm. Due to space limitations, we only report here a subset of the results. More results are available in the Appendix of the extended ver- sion [23].  Implementation, datasets, and parameters. We imple- mented our algorithm in C++11 and used the C implemen- tation by Grahne and Zhu [11] for the mining step. Our implementation is publicly available at http://cs.brown. edu/~matteo/radeprogrfi.tar.bz2. We use NLopt [15] to compute the minimum of w for the stopping condition (Thm. 4). We run the experiments on a machine with a quad-core AMD PhenomTM II X4 955 processor and 16GB of RAM, running GNU/Linux 3.2.0. We used datasets from the FIMI03 repository (http://fimi.ua.ac.be/data/) [10]. The characteristics of the datasets are reported in Table 1. Each dataset is replicated a number of times (between 200 and 1000) w.r.t. its FIMI03 version, so that its size is repre- sentative of modern datasets and the real-life distributions of the frequencies of the itemsets and of the transaction length are preserved. The d-bound d is a quantity used by VC to compute the sample size n as  (cid:16)  (cid:17)  .  n = 4 2  d + ln 1    It is, informally, the maximum index d for which the dataset contains at least d dierent transactions of length at least d [24, Sect. 4.1], and can be computed with a scan of the whole dataset.  Name accidents connect BMS-POS kosarak pumsb_star retail  Repl. factor 200 1000 200 200 1000 400  Size (|D|) 68036601 67557000 103119400 1980001400 49046000 35264804  |I| 468 129 1657 41270 2088 16470  d-bound [24] 46 43 81 443 59 58  Table 1: Dataset characteristics  In all our experiments we xed  = 0.1. The initial sam- ple sizes are computed according to (6). Except when oth- erwise specied, we used the automatic sampling schedule described in Sect. 4.6, i.e., we used (7) to compute the size of the sample to analyze at the next iteration. The value for  ranged in the set {0.01, 0.012, 0.015, 0.017, 0.02}. We  1011 run our algorithm ve times for each combination of pa- rameters, in order to evaluate uctuations in accuracy and running time due to the randomized nature of our algori- thm. Unless otherwise specied, the reported quantities are the averages over the runs.  Accuracy. We evaluate the accuracy of our algorithm in terms of the recall, precision, and error in frequency estima- tion for the output collection. Recall. The rst result of our experimental evaluation is that in all the hundreds of runs of our algorithm, for all datasets and combinations of parameters, the returned collection of itemsets always has the three properties from Def. 1, not just with probability 1  . This holds in par- ticular for the rst property (all itemsets in FI(D,I, ) are in the output collection), which means that the recall of our algorithm is always 100%. Precision. As for the precision of our algorithm, i.e., the ratio between the size of FI(D,I, ) and the output size, we remark that our algorithm gives no guarantee in this sense, as any itemsets with frequency in [  , ) may be in the output collection. Hence the precision depends on the dis- tribution of the dataset frequencies in this interval. In our experiments, it varied between 15% and 92%, depending on the parameters and on the dataset. We also measure the fraction of the itemsets with frequencies in [  , ) that are included in the output. This quantity ranges from 49% to a vanishingly small quantity when    (indeed in this case any itemset appearing in the dataset may be included in the output). We report the behavior of this quantity for various values of  in Figure 1, for the connect dataset at  = 0.72. In this gure we report the size of FI(D,I, ) (FI line), the number of possible False Positives (i.e., the num- ber of itemsets with frequencies in [  , ) in the dataset), the average number of FP in the output collection, and the ratio between this latter quantity to the former (aligned to the right vertical axis). We can see that the number of in- cluded FP grows slower than the number of possible FP, and therefore the ratio goes down. These False Positives are the price to pay when mining a sample of the dataset, and, by setting , the user understands that such False Positives are possible. In any case, our algorithm still returns a rela- tively compact collection of itemsets, rather than including any itemset that could theoretically be included (i.e., all the itemsets with frequencies in [ , )). Indeed the collection can still be used, in all cases, as a set of candidates from which to compute eciently the exact collection of FIs with a single linear scan of the dataset. The cost of this operation is almost always negligible. We remark once more that no itemset with frequency less than    was ever included in the output nor any itemset with frequency at least  was ever missed. Frequency Estimation. For every itemset A in the output collection, we measure the absolute frequency error errS(A) = |fS(A)  fD(A)|, where S is the last sample analyzed. The third property from Def. 1 requires errS(A) to be at most /2. Figure 2 shows the behavior of errS on the retail dataset, with  = 0.015. The behavior for other datasets and combination of parameters was similar and can be found in the extended online version [23]. We can see that the ab- solute error is almost an order of magnitude less than /2, both for the average and for the maximum error, and that it is very concentrated around the average. These low numbers  Figure 1: Precision for connect,  = 0.72.  are not due to the fact that many itemsets in the output col- lection have a low frequency in the dataset: we also measure the relative frequency error, dened as 100errS(A)/fD(A), and we report it in Figure 2, aligned to the right vertical axis. As we can see, this quantity was always less than 1.4%. In the future, we plan to develop an algorithm that gives guarantee on the relative frequency error, rather than on the absolute error.  Figure 2: Frequency error for retail,  = 0.015.  Discussion. The results of the accuracy experiments allow us to state that the algorithm performs in practice even better than what the theory guarantees. This is due to the fact that the theoretical analysis uses upper bounds that are developed for the worst case which almost never corresponds to naturally-arising datasets. The results also suggest that there is room for further improvements in the derivation of these bounds and their use in pattern mining.  Runtime. The main motivation of our work is that a FI mining algorithm based on progressive can be faster than one based on static sampling as it avoids the need to com- pute (or assume as known) characteristic quantities of the dataset which would require access to the entire dataset, and it can use properties of the sample to stop at smaller sample sizes. We compare the running time of our algori- thm to that of VC, to that of an exact algorithm for mining FI(D,I, ) from the whole dataset [11], and to the running time of our algorithm using a geometric sampling schedule |Si| = i|S0| for dierent values of  (in these cases, the initial sample size |S0| was still computed using (6) as in all our experiments). The results are reported for BMS-POS,  = 0.015 in Figure 3. Results for other datasets are sim-  0.0080.010.0120.0140.0160.0180.020.0E+05.0E+51.0E+61.5E+62.0E+62.5E+63.0E+63.5E+60.430.440.450.460.470.480.490.5FIpossible FPavg FPratioepsilonitemsetsratio0.0080.010.0120.0140.0160.0180.020.02200.00020.00040.00060.00080.0010.00120.001400.20.40.60.811.21.4maxavg + stdevavgmax relavg relepsilonabsolute freq. errorrelative freq. error1012 ilar and are not reported due to space limitations, but can be found in the extended version [23]. From the plot, it is possible to appreciate that our algorithm vastly outper- forms the exact algorithm and also VC. While the rst fact should be expected, the latter is due to VC having to scan the dataset in order to compute the d-bound, which can be relative expensive compared to our algorithm which needs no such computation. Moreover, as we discuss later, the sample size computed by VC is in most cases larger than the nal sample size used by our algorithm. We also report the running time for our algorithm using a geometric sample schedule with dierent values (2.0, 2.5, 3.0) for the scaling parameter . This allows us to evaluate the performances of the automatic sampling schedule described in Sect. 4.6. We can see that the automatic sampling schedule is more ecient as it allows our algorithm to run faster than with a geometric sample schedule by avoiding the creation and analysis of samples whose size is probably not sucient for the stopping condition to be satised, based on information obtained from the current sample. In almost all the runs of our algorithm, for all combinations of parameters and datasets, our algorithm stops after only two iterations (the only exception (3 iterations) happens for larger values of  on the kosarak dataset). This means that the information ob- tained at the minimum reasonable sample size (as computed by (6)) is extremely useful to compute a sucient sample size using (7). Instead, the runs using the geometric sample schedule stops after a variable number of iterations, which was not possible to predict in advance, and does not behave monotonically, as can be seen from Fig. 3. Hence, the use of the automatic sampling schedule is highly recommended, as it allows faster or comparable execution times and the re- moval of the parameter ), whose impact on the algorithm performances may not be clear a priori to the user.  Figure 3: Running time for BMS-POS,  = 0.015.  We also analyze the breakdown of the runtime of our algo- rithm, splitting it between time needed to sample the trans- action, time needed to evaluate the stopping condition, and time needed to perform the mining of the sample after the stopping condition is satised. The results are reported for the pumsb_star dataset,  = 0.32 in Figure 4. We can see that the runtime decreases as  grows. This is due to the sampling time and the mining time decreasing because the algorithm stops at smaller samples for larger values of . The fact that the mining time decreases as  increases is particu- larly interesting: the lowered frequency threshold   /2 at which the nal sample is mined is smaller for larger values of  and, on a sample of the same size, it would imply longer mining time than for lower values of . Instead the time saved due to the smaller sample dominates the impact of  the lower threshold. It is also clear that at small values of , the sampling time accounts for the majority of the running time. As expected, the sampling time depends quadratically on 1/ while the time needed to evaluate the stopping con- dition is almost constant. This suggests that it is indeed important to achieve a delicate balance between the cost of evaluating the stopping condition and the possibility that it is satised at smaller sample sizes. This was indeed one of our main guiding principles when designing our stopping condition.  Figure 4: Breakdown of runtime for pumsb_star,  = 0.32.  Static sampling. We also evaluate whether the static-sampling variant presented in Sect. 4.8 could outperform VC. We com- pared for a given sample size n, the value for  obtained using (8) on a sample of size n, to the value VC obtained using VC for the same sample size, which is  r  VC =  d + ln(1/)  n  ,  where d is the d-bound of the dataset (values in Table 1). In Figure 5 we show the results for the datasets kosarak and accidents. It is possible to see that VC is smaller than the one computed by our method at smaller sample size on the accidents dataset, but the  computed using (8) decreases faster as n grows and becomes smaller than VC at larger but reasonable sample sizes. On the other hand, on kosarak our method vastly outperforms VC, with a  that is half the one computed by VC. The datasets BMS-POS, pumsb- star, and retail showed results similar to those for kosarak, while the comparison for the dataset connect was similar to that on accidents. Looking at the characteristics of the datasets connect and accidents we noticed that they have a smaller number of items, a smaller d-bound, and more items with very high frequency than the other datasets. Of these characteristics, the last two are intuitively the ones with major impact on the results we see: a low d-bound results in a smaller VC, while high-frequency items will have a high frequency also in the sample, resulting in higher values for w(s), which depends on the items frequencies, and therefore in a higher . We are currently investigating how to improve our stopping condition in these cases.  We remark again that VC requires access to the entire dataset in order to compute d, which makes it unusable in some situation, as mentioned in Sect. 4.8. Moreover, com- puting d, as we showed when presenting the runtime results, can be extremely expensive, and the loss in terms of the ac- curacy parameter  may be traded o by the gain in speed.  0.0080.010.0120.0140.0160.0180.020.0E+02.0E+44.0E+46.0E+48.0E+41.0E+51.2E+51.4E+51.6E+5exactvcgeom-2.0geom-2.5geom-3.0avgepsilontotal runtime (ms)0.010.0120.0150.0170.020.0E+01.0E+42.0E+43.0E+44.0E+45.0E+46.0E+47.0E+48.0E+49.0E+41.0E+5miningstop. conditionsamplingepsilonruntime (ms)1013 Figure 5: Static sampling evaluation.  The comparison is therefore a slightly unfair to our meth- ods, given that VC is allowed to obtain crucial information by performing additional computation on the entire dataset. For these reasons, we consider our method more exible and more powerful than VC.  6. CONCLUSIONS  We present an algorithm for extracting a high-quality ap- proximation of the collection of FIs with probabilistic guar- antees. The algorithm employs progressive sampling with a stopping condition that relies on bounding the conditional Rademacher average of the problem using easy-to-compute characteristic quantities of the sample. The stopping con- dition can therefore be evaluated very eciently without the need to perform an expensive in-depth mining of the frequent itemsets in the sample at each step. To our knowl- edge this is the rst work that uses Rademacher averages in a knowledge discovery setting. The experimental results conrm that the algorithm is extremely successful at stop- ping fast at early iterations, and allows to extract very high- quality approximation of the collection of FIs. Among the possible directions for future work, it would be particularly interesting to better study the trade-o between the compu- tational complexity of the stopping condition and its ability to stop at small sample sizes. We are currently investigat- ing algorithms that give relative/multiplicative approxima- tion guarantees, and extensions of our work to additional signicance measures dierent from frequency [25, 26, 30].  7. '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= re.compile('(?<=INTRODUCTION)(.+)(?=ACKNOWLEDGMENTS)')\n",
    "p.search(re.sub('[\\s]',\" \",document)).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get CONCLUSION only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S  We present an algorithm for extracting a high-quality ap- proximation of the collection of FIs with probabilistic guar- antees. The algorithm employs progressive sampling with a stopping condition that relies on bounding the conditional Rademacher average of the problem using easy-to-compute characteristic quantities of the sample. The stopping con- dition can therefore be evaluated very eciently without the need to perform an expensive in-depth mining of the frequent itemsets in the sample at each step. To our knowl- edge this is the rst work that uses Rademacher averages in a knowledge discovery setting. The experimental results conrm that the algorithm is extremely successful at stop- ping fast at early iterations, and allows to extract very high- quality approximation of the collection of FIs. Among the possible directions for future work, it would be particularly interesting to better study the trade-o between the compu- tational complexity of the stopping condition and its ability to stop at small sample sizes. We are currently investigat- ing algorithms that give relative/multiplicative approxima- tion guarantees, and extensions of our work to additional signicance measures dierent from frequency [25, 26, 30].  7. '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= re.compile('(?<=CONCLUSION)(.+)(?=ACKNOWLEDGMENTS)')\n",
    "p.search(re.sub('[\\s]',\" \",document)).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get References only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  [1] R. Agrawal and R. Srikant. Fast algorithms for mining  association rules in large databases. VLDB 94.  [2] R. Agrawal, T. Imieliski, and A. Swami. Mining asso- ciation rules between sets of items in large databases. SIGMOD Rec., 22:207216, June 1993.  [3] B. Gu, B. Liu, F. Hu, and H. Liu. Eciently determin- ing the starting sample size for progressive sampling. ECML01.  [4] P. L. Bartlett, S. Boucheron, and G. Lugosi. Model selection and error estimation. Mach. Learn., 48:85 113, 2002.  [5] S. Boucheron, O. Bousquet, and G. Lugosi. Theory of classication : A survey of some recent advances. ESAIM: Probability and Statistics, 9:323375, 2005.  [6] V. T. Chakaravarthy, V. Pandit, and Y. Sabharwal. Analysis of sampling techniques for association rule mining. ICDT09.  [7] B. Chen, P. Haas, and P. Scheuermann. A new two- phase sampling based algorithm for discovering associ- ation rules. KDD02.  [8] K.-T. Chuang, M.-S. Chen, and W.-C. Yang. Progres- sive sampling for association rules based on sampling error estimation. PAKDD05.  [9] T. Elomaa and M. Kriinen. Progressive Rademacher  sampling. AAAI02.  [10] B. Goethals and M. J. Zaki. Advances in frequent itemset mining implementations: report on FIMI03. SIGKDD Explor. Newsl., 6(1):109117, June 2004.  [11] G. Grahne and J. Zhu. Eciently using prex-trees in  mining frequent itemsets. FIMI03.  [12] J. Han, J. Pei, and Y. Yin. Mining frequent patterns  without candidate generation. SIGMOD00.  [13] J. Han, H. Cheng, D. Xin, and X. Yan. Frequent pat- tern mining: current status and future directions. Data Mining and Knowl. Disc. , 15:5586, 2007.  [14] G. H. John and P. Langley. Static versus dynamic sam-  pling for data mining. KDD96.  [15] S. G. Johnson. The NLopt nonlinear-optimization pack-  age. URL http://ab-initio.mit.edu/nlopt.  [16] V. Koltchinskii. Rademacher penalties and structural IEEE Trans. Inf. Theory, 47(5):  risk minimization. 19021914, July 2001.  [17] E. Liberty, M. Mitzenmacher, J. Thaler, and J. Ullman. Space lower bounds for itemset frequency sketches. CoRR, 1407.3740, July 2014.  [18] S. Parthasarathy. Ecient progressive sampling for as-  sociation rules. ICDM02.  [19] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Dis- covering frequent closed itemsets for association rules. ICDT99.  [20] A. Pietracaprina, M. Riondato, E. Upfal,  and F. Vandin. Mining top-k frequent itemsets through pro- gressive sampling. Data Mining Knowl. Disc., 21(2): 310326, 2010.  [21] F. Provost, D. Jensen, and T. Oates. Ecient progres-  sive sampling. KDD99.  [22] M. Riondato, J. A. DeBrabant, R. Fonseca, and E. Up- fal. PARMA: A parallel randomized algorithm for as- sociation rules mining in MapReduce. CIKM12.  [23] M. Riondato and E. Upfal. Mining frequent itemsets through progressive sampling with Rademacher aver- ages. Extended Version. URL http://cs.brown.edu/ %7Ematteo/papers/progrsamplfi-ext.pdf.  [24] M. Riondato and E. Upfal. Ecient discovery of associ- ation rules and frequent itemsets through sampling with tight performance guarantees. ACM Trans. Knowl. Disc. from Data, 8(2), 2014.  [25] M. Riondato and F. Vandin. Finding the true frequent  [29] V. N. Vapnik. The Nature of Statistical Learning The- ory. Statistics for engineering and information science. Springer-Verlag, New York, NY, USA, 1999.  [30] G. I. Webb. Discovering signicant patterns. Mach.  Learn., 68(1):133, 2007.  itemsets. SDM14.  [26] T. Scheer and S. Wrobel. Finding the most interest- ing patterns in a database quickly by using sequential sampling. J. Mach. Learn. Res., 3:833862, Dec. 2002. [27] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cam- bridge University Press, 2014.  [28] H. Toivonen. Sampling large databases for association  rules. VLDB96.  0.0E+02.0E+64.0E+600.020.040.060.08kosarakVCThis worksample sizeepsilon0.0E+02.0E+64.0E+600.010.020.030.04accidentsVCThis worksample sizeepsilon1014 '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= re.compile('(?<=REFERENCES)(.+)')\n",
    "p.search(re.sub('[\\s]',\" \",document)).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to count the number of references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "create a match of all references by counting the number of integers enclosed by brackets (i.e. '[2]').  \n",
    "\n",
    "This is how references are labeled in the research papers\n",
    "'''\n",
    "\n",
    "len(re.findall('\\[(.*?)\\]',regexp.search(re.sub('[\\s]',\" \",document)).group(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
