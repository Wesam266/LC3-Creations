{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook works on extracting unique named entities and organizations from KDD papers and passing them lists.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import subprocess\n",
    "import unicodedata\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk import Tree\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import polyglot\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path        = os.path.abspath(os.getcwd())\n",
    "TESTDIR     = os.path.normpath(os.path.join(os.path.expanduser(\"~\"),\"projects\",\"LC3-Creations\", \"examples\",\"KDDsample\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "I experienced unicode problems early on.  Everytime I had an error, I scoured the internet for solutions. Here's the credit.\n",
    "\n",
    "\n",
    "\n",
    "- For Typeerror codes using subprocess to convert pdf2txt output to straight unicode --> http://stackoverflow.com/questions/33283603/python-popen-communicate-str-encodeencoding-utf-8-errors-ignore-cr\n",
    "- For problems with ASCII characters --> http://stackoverflow.com/questions/175240/how-do-i-convert-a-files-format-from-unicode-to-ascii-using-python\n",
    "- For unicode characters left in unicode converted to a string  --> http://stackoverflow.com/questions/8689795/how-can-i-remove-non-ascii-characters-but-leave-periods-and-spaces-using-python\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "a = unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,\"p1005.pdf\")))]),errors='ignore')\n",
    "document = filter(lambda x: x in string.printable,unicodedata.normalize('NFKD', a).encode('ascii','ignore').decode('unicode_escape').encode('ascii','ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mining Frequent Itemsets through Progressive Sampling\\n\\nwith Rademacher Averages\\n\\nMatteo Riondato\\n\\nDept. of Computer Science\\n\\nBrown University\\n\\nProvidence, RI 02912\\n\\nmatteo@cs.brown.edu\\n\\nEli Upfal\\n\\nDept. of Computer Science\\n\\nBrown University\\n\\nProvidence, RI 02912\\neli@cs.brown.edu\\n\\nABSTRACT\\nWe present an algorithm to extract an high-quality approx-\\nimation of the (top-k) Frequent itemsets (FIs) from ran-\\ndom samples of a transactional dataset. With high prob-\\nability the approximation is a superset of the FIs, and no\\nitemset with frequency much lower than the threshold is in-\\ncluded in it. The algorithm employs progressive sampling,\\nwith a stopping condition based on bounds to the empirical\\nRademacher average, a key concept from statistical learning\\ntheory. The computation of the bounds uses characteris-\\ntic quantities that can be obtained eciently with a sin-\\ngle scan of the sample. Therefore, evaluating the stopping\\ncondition is fast, and does not require an expensive mining\\nof each sample. Our experimental evaluation conrms the\\npracticality of our approach on real datasets, outperforming\\napproaches based on one-shot static sampling.\\n\\nCategories and Subject Descriptors\\nH.2.8 [Database Management]: Database Applications\\nData mining\\n\\nGeneral Terms\\nAlgorithms, Theory, Performance, Experimentation\\n\\nKeywords\\nFrequent Itemsets; Pattern Mining; Rademacher Averages;\\nSampling; Statistical Learning Theory\\n\\n1.\\n\\nINTRODUCTION\\n\\nThe task of Frequent Itemsets (FIs) mining is to extract all\\nsets of items that appear in at least a fraction  of a transac-\\ntional dataset D, or the k most frequent set of items [2]. It is\\na fundamental primitive of knowledge discovery and is use-\\nful, among the others, for market basket analysis, inference,\\nclassication, and network management [13]. Exact algo-\\nrithms to mine FIs have since long been available but their\\n\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided tha'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"returns named entity chunks in a given text\"\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(re.sub('[\\s]',\" \", document)))\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "# Another entity extractor\n",
    "st = StanfordNERTagger('/Users/linwood/stanford-corenlp-full-2015-04-20/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "       '/Users/linwood/stanford-corenlp-full-2015-04-20/stanford-corenlp-3.5.2.jar',\n",
    "       encoding='utf-8')\n",
    "tokenized_text = word_tokenize(re.sub('[\\s]',\" \", document))\n",
    "stanentities = st.tag(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I established two lists to hold the values that I extract from the text.  This itemgetter function will check for unique values.  First, I iterate over the extracted entities and see if the objects is a nltk.tree.Tree with a \"Person\" label.  If it is, and the length is equal to 1 (first or last name only), I append that value to the list. If it's larger, I iterate of the entity tree and pull out the first value only using itemgetter.  Then, I join the values from the list and append it to the destination list.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a list out of NLTK's standard NE chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Local Intrinsic Dimensionality Laurent Amsaleg Equipe LINKMEDIA', 'Rennes', 'Campus Universitaire', 'Rennes Cedex', 'Stphane Girard Equipe MISTIS', 'Furon Equipe LINKMEDIA', 'Oussama Chelly National Institute', 'Michael E. Houle National Institute', 'Michael Nett Google', 'Houle', 'Karger', 'Subject Descriptors', 'Parameter', 'Permissions', 'Keywords', 'Isometric Mapping', 'Linear Embedding', 'Principal', 'Analysis', 'Weibull', 'Maximum', 'Hill', 'Pr', 'Fisher', 'Tippett', 'Gnedenko', 'Given', 'Ruhls', 'J', 'M', 'Hein', 'P', 'D', 'Uniformly', 'Amsterdam Library', 'Object Images', 'Likewise', 'Faster', 'D IDMLE', 'Hein Takens', 'Dataset', 'Data', 'Secular', 'Residual Life Time', 'Cambridge University Press', 'J. Fauqueur', 'Pattern Recogn', 'Statistical Modeling', 'Extreme Values', 'Hero', 'Sys', 'Smallest Member', 'Math', 'Cambridge Phil', 'Fraga Alves', 'Portugalia Mathematica', 'Terme Maximum', 'Serie Aleatoire', 'Ann', 'Gupta', 'Audibert', 'Inlierness', 'Technical Report', 'Nett', 'Source Coding', 'Principal Component Analysis', 'Networks', 'Linear', 'Nonlinear Dimensionality Reduction', 'Locally Linear Embedding', 'Science', 'Rozza', 'Novel', 'Machine Learning Journal', 'Nonlinear Component Analysis', 'Neural Computation', 'Shaft', 'Database Syst.', 'Local Multidimensional Scaling', 'Neural Networks', 'Brunken']\n",
      "\n",
      "\n",
      "['INRIA Grenoble', 'Montbonnot', 'MLE', 'MoM', 'PWM', 'ACM', 'Principal', 'PCA', 'Component Analysis', 'ID', 'Fractal', 'Correlation Dimension', 'GED', 'MiND', 'LID', 'EVT', 'Weibull', 'Karamata', 'ED', 'CONTINUOUS', 'INTRINSIC', 'FX', 'Houle', 'IDX', 'xfX', 'EXTREME', 'Pareto', 'RV', 'GPD', 'Likelihood Estimation', 'ID2', 'ID2 X', 'Method', 'IDXk2', 'kxfX', 'Iverson', 'MiNDml1', 'max1jJ', 'IDXV', 'nFX', 'EXPERIMENTAL', 'IDRV', 'Parameters', 'CD', 'kNNG1', 'MiNDmli', 'Distance Distributions', 'Real Data', 'samplesIDMLEMoMRV', 'samplesMLEMoMRV', 'ALOI', 'MNIST', 'ANN', 'SIFT', 'PWMs', 'IDMNIST0K1K2K3K4K', 'IDANN_SIFT1B35 Dataset', 'IDMoM', 'IDRVE', 'kNNG2', 'IDMLE', 'ACKNOWLEDGMENTS', 'JST', 'JSPS Kakenhi Kiban', 'DistanceNeighbor', 'Neighbor', 'IDNeighborhood', 'Neighborhood', 'REFERENCES', 'Annals', 'Generic Image Retrieval', 'PAMI', 'Frequency Distribution', 'Largest', 'Methods of Stat.', 'Bounded Geometries', 'Density', 'Hubness', 'NII', 'Metrics', 'Document Recognition', 'IEEE', 'III', 'Kernel Eigenvalue Problem', 'ACM Trans', 'Intrinsic Dimensional Outlier']\n",
      "\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "persons = []\n",
    "organizations = []\n",
    "locations =[]\n",
    "\n",
    "for l in entities:\n",
    "    if isinstance(l,nltk.tree.Tree):\n",
    "        if l.label() == 'PERSON':\n",
    "            if len(l)== 1:\n",
    "                if l[0][0] in persons:\n",
    "                    pass\n",
    "                else:\n",
    "                    persons.append(l[0][0])\n",
    "            else:\n",
    "                if \" \".join(map(itemgetter(0), l)) in persons:\n",
    "                    pass\n",
    "                else:\n",
    "                    persons.append(\" \".join(map(itemgetter(0), l)))\n",
    "                    \n",
    "for o in entities:\n",
    "    if isinstance(o,nltk.tree.Tree):\n",
    "        if o.label() == 'ORGANIZATION':\n",
    "            if len(o)== 1:\n",
    "                if o[0][0] in organizations:\n",
    "                    pass\n",
    "                else:\n",
    "                    organizations.append(o[0][0])\n",
    "            else:\n",
    "                if \" \".join(map(itemgetter(0), o)) in organizations:\n",
    "                    pass\n",
    "                else:\n",
    "                    organizations.append(\" \".join(map(itemgetter(0), o)))\n",
    "                    \n",
    "for o in entities:\n",
    "    if isinstance(o,nltk.tree.Tree):\n",
    "        if o.label() == 'LOCATION':\n",
    "            if len(o)== 1:\n",
    "                if o[0][0] in locations:\n",
    "                    pass\n",
    "                else:\n",
    "                    locations.append(o[0][0])\n",
    "            else:\n",
    "                if \" \".join(map(itemgetter(0), o)) in locations:\n",
    "                    pass\n",
    "                else:\n",
    "                    locations.append(\" \".join(map(itemgetter(0), o)))\n",
    "                    \n",
    "                \n",
    "print persons\n",
    "print\n",
    "print\n",
    "print organizations\n",
    "print\n",
    "print\n",
    "print locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INRIA Grenoble\n",
      "France Inovalle\n",
      "Component Analysis\n",
      "Correlation Dimension\n",
      "Likelihood Estimation\n",
      "ID2 X\n",
      "Distance Distributions\n",
      "Real Data\n",
      "Swiss Roll\n",
      "IDANN_SIFT1B35 Dataset\n",
      "Real Data\n",
      "JSPS Kakenhi Kiban\n",
      "Generic Image Retrieval\n",
      "Asilomar Conf\n",
      "Frequency Distribution\n",
      "Methods of Stat.\n",
      "Bounded Geometries\n",
      "Distance Distributions\n",
      "Document Recognition\n",
      "Kernel Eigenvalue Problem\n",
      "ACM Trans\n",
      "Intrinsic Dimensional Outlier\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "for o in entities:\n",
    "    if isinstance(o,nltk.tree.Tree):\n",
    "        if o.label() == 'ORGANIZATION' or o.label() == 'GPE':\n",
    "            if len(o)>1:\n",
    "                print \" \".join(map(itemgetter(0), o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to iterate over the extracted list of entities to get a better break between person's and their university name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = [nltk.word_tokenize(l) for l in persons]\n",
    "fin = [nltk.chunk.ne_chunk(nltk.pos_tag(l)) for l in tokens]\n",
    "fin;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new =[word_tokenize(l) for l in persons]\n",
    "stan = [st.tag(l) for l in new]\n",
    "stan;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating lists of named entities from Stanford's NER model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function looks though an extracted stanford ner list, and finds continuous entitiy labels.  This should create first name, last name records of entities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_continuous_chunks(tagged_sent):\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for token, tag in tagged_sent:\n",
    "        if tag != \"O\":\n",
    "            current_chunk.append((token, tag))\n",
    "        else:\n",
    "            if current_chunk: # if the current chunk is not empty\n",
    "                continuous_chunk.append(current_chunk)\n",
    "                current_chunk = []\n",
    "    # Flush the final current_chunk into the continuous_chunk, if any.\n",
    "    if current_chunk:\n",
    "        continuous_chunk.append(current_chunk)\n",
    "    return continuous_chunk\n",
    "\n",
    "ne_tagged_sent = [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION')]\n",
    "\n",
    "named_entities = get_continuous_chunks(stanentities)\n",
    "named_entities_str = [\" \".join([token for token, tag in ne]) for ne in named_entities]\n",
    "named_entities_str_tag = [(\" \".join([token for token, tag in ne]), ne[0][1]) for ne in named_entities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Estimating Local Intrinsic Dimensionality Laurent Amsaleg Equipe LINKMEDIA',\n",
       "  u'ORGANIZATION'),\n",
       " (u'CNRSIRISA Rennes', u'LOCATION'),\n",
       " (u'France Campus Universitaire de Beaulieu 35042 Rennes Cedex',\n",
       "  u'ORGANIZATION'),\n",
       " (u'France', u'LOCATION'),\n",
       " (u'Stphane Girard Equipe MISTIS', u'PERSON'),\n",
       " (u'INRIA Grenoble', u'LOCATION'),\n",
       " (u'France Inovalle', u'LOCATION'),\n",
       " (u'France Teddy Furon Equipe LINKMEDIA', u'ORGANIZATION'),\n",
       " (u'INRIAIRISA Rennes', u'LOCATION'),\n",
       " (u'France Campus Universitaire de Beaulieu 35042 Rennes Cedex',\n",
       "  u'ORGANIZATION'),\n",
       " (u'France', u'LOCATION'),\n",
       " (u'Kawarabayashi National Institute of Informatics', u'ORGANIZATION'),\n",
       " (u'Japan', u'LOCATION'),\n",
       " (u'Hitotsubashi', u'LOCATION'),\n",
       " (u'Chiyoda-ku Tokyo', u'LOCATION'),\n",
       " (u'Japan', u'LOCATION'),\n",
       " (u'Oussama Chelly National Institute of Informatics', u'ORGANIZATION'),\n",
       " (u'Japan', u'LOCATION'),\n",
       " (u'Hitotsubashi', u'LOCATION'),\n",
       " (u'Chiyoda-ku Tokyo', u'LOCATION'),\n",
       " (u'Japan', u'LOCATION'),\n",
       " (u'Michael E. Houle National Institute of Informatics', u'ORGANIZATION'),\n",
       " (u'Japan', u'LOCATION'),\n",
       " (u'Hitotsubashi', u'LOCATION'),\n",
       " (u'Chiyoda-ku Tokyo', u'LOCATION'),\n",
       " (u'Japan', u'LOCATION'),\n",
       " (u'Michael Nett Google', u'PERSON'),\n",
       " (u'Japan', u'LOCATION'),\n",
       " (u'Tokyo', u'LOCATION'),\n",
       " (u'Japan', u'LOCATION'),\n",
       " (u'Houle', u'PERSON'),\n",
       " (u'Karger', u'PERSON'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'2015', u'DATE'),\n",
       " (u'$ 15.00', u'MONEY'),\n",
       " (u'August 10-13', u'DATE'),\n",
       " (u'2015', u'DATE'),\n",
       " (u'Sydney', u'LOCATION'),\n",
       " (u'Australia', u'LOCATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'Weibull', u'PERSON'),\n",
       " (u'EVT', u'ORGANIZATION'),\n",
       " (u'Karamata', u'LOCATION'),\n",
       " (u'Weibull', u'PERSON'),\n",
       " (u'EVT', u'ORGANIZATION'),\n",
       " (u'Weibull', u'PERSON'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'Houle', u'PERSON'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'Houle', u'PERSON'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'Fisher', u'PERSON'),\n",
       " (u'Tippett', u'PERSON'),\n",
       " (u'Balkema', u'PERSON'),\n",
       " (u'Haan', u'PERSON'),\n",
       " (u'Pickands', u'ORGANIZATION'),\n",
       " (u'Haan', u'PERSON'),\n",
       " (u'Pickands', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'Haan', u'PERSON'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'GPD', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'Fisher', u'PERSON'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'n2k', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'Iverson', u'PERSON'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'Karger', u'PERSON'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'S1', u'ORGANIZATION'),\n",
       " (u'GED', u'ORGANIZATION'),\n",
       " (u'S1', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'Hein', u'PERSON'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'3 4 5 6 7 8 9 10a 10b 10c 11 12 13 10 3 4 4 2 6 2 12 20 10 17 24 2 20 1 11',\n",
       "  u'DATE'),\n",
       " (u'Mobius', u'PERSON'),\n",
       " (u'7 8 9 10 11 12', u'DATE'),\n",
       " (u') 30 32 34 36 38 40 42 44 46', u'DATE'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'ALOI ( Amsterdam Library of Object Images', u'ORGANIZATION'),\n",
       " (u'ANN SIFT1B', u'PERSON'),\n",
       " (u'ANN SIFT1B', u'PERSON'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'0 5 10 15 20 25 30', u'DATE'),\n",
       " (u'0 5 10 15 20 25 30 350K2K4K6K8K10KEstimated IDQueryMNIST 0 5 10 15 20 25 30 350K2K4K6K8K10KEstimated IDQueryANN_SIFT1BABDEFCG0K1K2K3K4K 0 5 10 15 20 25 30',\n",
       "  u'DATE'),\n",
       " (u'0 5 10 15 20 25 30', u'DATE'),\n",
       " (u'0 5 10 15 20 25 30', u'DATE'),\n",
       " (u'12.29 12 12.39 20 7.39 10 24 14.05 2.49 2 20 12.48 11 5 6 3 72 20 11 25 3 20',\n",
       "  u'DATE'),\n",
       " (u'1.96 13.72 12 14.47 20 8.20 10 16.66 24 1.99 2 20 15.46 11 5 6 3 72 20 11 25 3 20',\n",
       "  u'DATE'),\n",
       " (u'44.17 12 20 -21.77 21.46 10 7.98 24 32.16 2 20 -22.83 11 5 6 3 72 20 11 25 3 20',\n",
       "  u'DATE'),\n",
       " (u'Hein Takens', u'PERSON'),\n",
       " (u'1000', u'DATE'),\n",
       " (u'Hein Takens', u'PERSON'),\n",
       " (u'Hein Takens', u'PERSON'),\n",
       " (u'-21.77 20 21.46 10 24 8.04 32.16 2 20 -22.83 11 5 6 3 72 20 11 25 3 20',\n",
       "  u'DATE'),\n",
       " (u'Hein Takens', u'PERSON'),\n",
       " (u'44.17 12 -21.77 20 21.46 10 8.04 24 31.66 2 20 -22.83 11 5 6 3 72 20 11 25 3 20',\n",
       "  u'DATE'),\n",
       " (u'Hein Takens', u'PERSON'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'EVT', u'ORGANIZATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'1000', u'DATE'),\n",
       " (u'L. Amsaleg', u'PERSON'),\n",
       " (u'T. Furon', u'ORGANIZATION'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'K. Kawarabayashi', u'PERSON'),\n",
       " (u'JST ERATO Kawarabaya- shi Project', u'ORGANIZATION'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'JSPS Kakenhi Kiban ( A ) Research Grant 25240036', u'ORGANIZATION'),\n",
       " (u'0 5 10 15 20 25 30 35', u'DATE'),\n",
       " (u'A. Balkema', u'PERSON'),\n",
       " (u'L. de Haan', u'PERSON'),\n",
       " (u'1974', u'DATE'),\n",
       " (u'] N. Bingham', u'PERSON'),\n",
       " (u'C. Goldie', u'ORGANIZATION'),\n",
       " (u'Cambridge University Press', u'ORGANIZATION'),\n",
       " (u'1989', u'DATE'),\n",
       " (u'] N. Boujemaa', u'PERSON'),\n",
       " (u'J. Fauqueur', u'PERSON'),\n",
       " (u'M. Ferecatu', u'PERSON'),\n",
       " (u'F. Fleuret', u'PERSON'),\n",
       " (u'V. Gouet', u'PERSON'),\n",
       " (u'B. LeSaux', u'PERSON'),\n",
       " (u'H. Sahbi', u'PERSON'),\n",
       " (u'Interactive Specic and Generic Image Retrieval', u'ORGANIZATION'),\n",
       " (u'2001', u'DATE'),\n",
       " (u'] C. Bouveyron', u'PERSON'),\n",
       " (u'G. Celeux', u'PERSON'),\n",
       " (u'S. Girard', u'PERSON'),\n",
       " (u'] J. Bruske', u'PERSON'),\n",
       " (u'G. Sommer', u'PERSON'),\n",
       " (u'PAMI', u'ORGANIZATION'),\n",
       " (u'] F. Camastra', u'ORGANIZATION'),\n",
       " (u'A. Vinciarelli', u'PERSON'),\n",
       " (u'PAMI', u'ORGANIZATION'),\n",
       " (u'] S. Coles', u'PERSON'),\n",
       " (u'Statistical Modeling of Extreme Values', u'ORGANIZATION'),\n",
       " (u'] J. Costa', u'ORGANIZATION'),\n",
       " (u'Asilomar Conf', u'ORGANIZATION'),\n",
       " (u'2003', u'DATE'),\n",
       " (u'T. de Vries', u'ORGANIZATION'),\n",
       " (u'S. Chawla', u'PERSON'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'2010', u'DATE'),\n",
       " (u'] R. A. Fisher', u'PERSON'),\n",
       " (u'L. H. C. Tippett', u'PERSON'),\n",
       " (u'Cambridge Phil', u'LOCATION'),\n",
       " (u'1928', u'DATE'),\n",
       " (u'] M. I. Fraga Alves', u'PERSON'),\n",
       " (u'L. de Haan', u'PERSON'),\n",
       " (u'T. Lin', u'PERSON'),\n",
       " (u'] M. I. Fraga Alves', u'PERSON'),\n",
       " (u'M. I. Gomes', u'PERSON'),\n",
       " (u'L. de Haan', u'PERSON'),\n",
       " (u'Portugalia Mathematica', u'ORGANIZATION'),\n",
       " (u'2003', u'DATE'),\n",
       " (u'B. V. Gnedenko', u'PERSON'),\n",
       " (u'Distribution Limite du Terme Maximum', u'ORGANIZATION'),\n",
       " (u'1943', u'DATE'),\n",
       " (u'A. Gupta', u'PERSON'),\n",
       " (u'R. Krauthgamer', u'PERSON'),\n",
       " (u'J. R. Lee', u'PERSON'),\n",
       " (u'2003', u'DATE'),\n",
       " (u'] M. Hein', u'PERSON'),\n",
       " (u'2005', u'DATE'),\n",
       " (u'B. M. Hill', u'PERSON'),\n",
       " (u'1975', u'DATE'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'Discriminability , Density & Distance Distributions', u'ORGANIZATION'),\n",
       " (u'2013', u'DATE'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'Outlierness', u'LOCATION'),\n",
       " (u'Hubness', u'PERSON'),\n",
       " (u'Extreme-Value-Theoretic Foundation', u'ORGANIZATION'),\n",
       " (u'NII', u'ORGANIZATION'),\n",
       " (u'2015', u'DATE'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'H. Kashima', u'PERSON'),\n",
       " (u'M. Nett', u'PERSON'),\n",
       " (u'2012', u'DATE'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'X. Ma', u'PERSON'),\n",
       " (u'M. Nett', u'PERSON'),\n",
       " (u'V. Oria', u'PERSON'),\n",
       " (u'Multi-Step Similarity Search', u'ORGANIZATION'),\n",
       " (u'2012', u'DATE'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'X. Ma', u'PERSON'),\n",
       " (u'V. Oria', u'PERSON'),\n",
       " (u'2014', u'DATE'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'M. Nett', u'PERSON'),\n",
       " (u'PAMI', u'ORGANIZATION'),\n",
       " (u'2015', u'DATE'),\n",
       " (u'] H. Jegou', u'PERSON'),\n",
       " (u'R. Tavenard', u'PERSON'),\n",
       " (u'M. Douze', u'PERSON'),\n",
       " (u'L. Amsaleg', u'PERSON'),\n",
       " (u'Source Coding', u'ORGANIZATION'),\n",
       " (u'2011', u'DATE'),\n",
       " (u'I. Jollie', u'PERSON'),\n",
       " (u'1986', u'DATE'),\n",
       " (u'] D. R. Karger', u'PERSON'),\n",
       " (u'M. Ruhl', u'PERSON'),\n",
       " (u'2002', u'DATE'),\n",
       " (u'J. Karhunen', u'PERSON'),\n",
       " (u'J. Joutsensalo', u'PERSON'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'Neural Networks', u'ORGANIZATION'),\n",
       " (u'1994', u'DATE'),\n",
       " (u'] Y. LeCun', u'PERSON'),\n",
       " (u'L. Bottou', u'PERSON'),\n",
       " (u'Y. Bengio', u'PERSON'),\n",
       " (u'P. Haner', u'PERSON'),\n",
       " (u'1998', u'DATE'),\n",
       " (u'J. Pickands', u'ORGANIZATION'),\n",
       " (u'Statistical Inference Using Extreme Order Statistics', u'ORGANIZATION'),\n",
       " (u'1975', u'DATE'),\n",
       " (u'C. R. Rao', u'PERSON'),\n",
       " (u'1973', u'DATE'),\n",
       " (u'S. T. Roweis', u'PERSON'),\n",
       " (u'L. K. Saul', u'PERSON'),\n",
       " (u'2000', u'DATE'),\n",
       " (u'A. Rozza', u'PERSON'),\n",
       " (u'G. Lombardi', u'PERSON'),\n",
       " (u'C. Ceruti', u'PERSON'),\n",
       " (u'E. Casiraghi', u'PERSON'),\n",
       " (u'P. Campadelli', u'PERSON'),\n",
       " (u'2012', u'DATE'),\n",
       " (u'B. Scholkopf', u'PERSON'),\n",
       " (u'A. J. Smola', u'PERSON'),\n",
       " (u'K.-R. Muller', u'PERSON'),\n",
       " (u'Kernel Eigenvalue Problem', u'ORGANIZATION'),\n",
       " (u'1998', u'DATE'),\n",
       " (u'U. Shaft', u'PERSON'),\n",
       " (u'R. Ramakrishnan', u'PERSON'),\n",
       " (u'2006', u'DATE'),\n",
       " (u'] F. Takens', u'ORGANIZATION'),\n",
       " (u'1985', u'DATE'),\n",
       " (u'J. Tenenbaum', u'PERSON'),\n",
       " (u'V. D. Silva', u'PERSON'),\n",
       " (u'J. Langford', u'PERSON'),\n",
       " (u'2000', u'DATE'),\n",
       " (u'B. Tenenbaum', u'PERSON'),\n",
       " (u'V. De Silva', u'ORGANIZATION'),\n",
       " (u'J. C. Langford', u'PERSON'),\n",
       " (u'2000', u'DATE'),\n",
       " (u'J. Venna', u'PERSON'),\n",
       " (u'S. Kaski', u'PERSON'),\n",
       " (u'Neural Networks', u'ORGANIZATION'),\n",
       " (u'2006', u'DATE'),\n",
       " (u'P. Verveer', u'PERSON'),\n",
       " (u'R. Duin', u'PERSON'),\n",
       " (u'PAMI', u'ORGANIZATION'),\n",
       " (u'1995', u'DATE'),\n",
       " (u'] J. von Brunken', u'PERSON'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'A. Zimek', u'PERSON'),\n",
       " (u'High-Dimensional Data', u'ORGANIZATION'),\n",
       " (u'NII', u'ORGANIZATION'),\n",
       " (u'2015', u'DATE')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_entities = get_continuous_chunks(stanentities)\n",
    "named_entities_str = [\" \".join([token for token, tag in ne]) for ne in named_entities]\n",
    "named_entities_str_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Stphane Girard Equipe MISTIS',\n",
       " u'Michael Nett Google',\n",
       " u'Houle',\n",
       " u'Karger',\n",
       " u'Weibull',\n",
       " u'Fisher',\n",
       " u'Tippett',\n",
       " u'Balkema',\n",
       " u'Haan',\n",
       " u'Iverson',\n",
       " u'Hein',\n",
       " u'Mobius',\n",
       " u'ANN SIFT1B',\n",
       " u'Hein Takens',\n",
       " u'L. Amsaleg',\n",
       " u'M. E. Houle',\n",
       " u'K. Kawarabayashi',\n",
       " u'A. Balkema',\n",
       " u'L. de Haan',\n",
       " u'  N. Bingham',\n",
       " u'  N. Boujemaa',\n",
       " u'J. Fauqueur',\n",
       " u'M. Ferecatu',\n",
       " u'F. Fleuret',\n",
       " u'V. Gouet',\n",
       " u'B. LeSaux',\n",
       " u'H. Sahbi',\n",
       " u'  C. Bouveyron',\n",
       " u'G. Celeux',\n",
       " u'S. Girard',\n",
       " u'  J. Bruske',\n",
       " u'G. Sommer',\n",
       " u'A. Vinciarelli',\n",
       " u'  S. Coles',\n",
       " u'S. Chawla',\n",
       " u'  R. A. Fisher',\n",
       " u'L. H. C. Tippett',\n",
       " u'  M. I. Fraga Alves',\n",
       " u'T. Lin',\n",
       " u'M. I. Gomes',\n",
       " u'B. V. Gnedenko',\n",
       " u'A. Gupta',\n",
       " u'R. Krauthgamer',\n",
       " u'J. R. Lee',\n",
       " u'  M. Hein',\n",
       " u'B. M. Hill',\n",
       " u'Hubness',\n",
       " u'H. Kashima',\n",
       " u'M. Nett',\n",
       " u'X. Ma',\n",
       " u'V. Oria',\n",
       " u'  H. Jegou',\n",
       " u'R. Tavenard',\n",
       " u'M. Douze',\n",
       " u'I. Jollie',\n",
       " u'  D. R. Karger',\n",
       " u'M. Ruhl',\n",
       " u'J. Karhunen',\n",
       " u'J. Joutsensalo',\n",
       " u'  Y. LeCun',\n",
       " u'L. Bottou',\n",
       " u'Y. Bengio',\n",
       " u'P. Haner',\n",
       " u'C. R. Rao',\n",
       " u'S. T. Roweis',\n",
       " u'L. K. Saul',\n",
       " u'A. Rozza',\n",
       " u'G. Lombardi',\n",
       " u'C. Ceruti',\n",
       " u'E. Casiraghi',\n",
       " u'P. Campadelli',\n",
       " u'B. Scholkopf',\n",
       " u'A. J. Smola',\n",
       " u'K.-R. Muller',\n",
       " u'U. Shaft',\n",
       " u'R. Ramakrishnan',\n",
       " u'J. Tenenbaum',\n",
       " u'V. D. Silva',\n",
       " u'J. Langford',\n",
       " u'B. Tenenbaum',\n",
       " u'J. C. Langford',\n",
       " u'J. Venna',\n",
       " u'S. Kaski',\n",
       " u'P. Verveer',\n",
       " u'R. Duin',\n",
       " u'  J. von Brunken',\n",
       " u'A. Zimek']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare=[]\n",
    "for l,m in named_entities_str_tag:\n",
    "    l=re.sub('(\\])',\" \",l)\n",
    "    if m == 'PERSON':\n",
    "        if l in compare:\n",
    "            pass\n",
    "        else:\n",
    "            compare.append(l)\n",
    "    else:\n",
    "        pass\n",
    "compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(compare) & set(p29pdf_allauthors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p29pdf_allauthors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list1 = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list2 = [i for i in xrange(7,17,1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(list1) & set(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parts_of_speech(corpus):\n",
    "    \"returns named entity chunks in a given text\"\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(corpus))\n",
    "    entities = nltk.chunk.ne_chunk(tagged)\n",
    "    # Another entity extractor\n",
    "    st = StanfordNERTagger('/Users/linwood/stanford-corenlp-full-2015-04-20/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "           '/Users/linwood/stanford-corenlp-full-2015-04-20/stanford-corenlp-3.5.2.jar',\n",
    "           encoding='utf-8')\n",
    "    tokenized_text = word_tokenize(corpus)\n",
    "    stanentities = st.tag(tokenized_text)\n",
    "    return entities\n",
    "def find_entities(chunks):\n",
    "    \"given list of tagged parts of speech, returns unique named entities\"\n",
    "\n",
    "    def traverse(tree):\n",
    "        \"recursively traverses an nltk.tree.Tree to find named entities\"\n",
    "        entity_names = []\n",
    "    \n",
    "        if hasattr(tree, 'node') and tree.node:\n",
    "            if tree.node == 'NE':\n",
    "                entity_names.append(' '.join([child[0] for child in tree]))\n",
    "            else:\n",
    "                for child in tree:\n",
    "                    entity_names.extend(traverse(child))\n",
    "    \n",
    "        return entity_names\n",
    "    \n",
    "    named_entities = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        entities = sorted(list(set([word for tree in chunk\n",
    "                            for word in traverse(tree)])))\n",
    "        for e in entities:\n",
    "            if e not in named_entities:\n",
    "                named_entities.append(e)\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting entities and creating lists using Polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from polyglot.text import Text\n",
    "e=Text(re.sub('[\\s]',\" \",document[:10000])).entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code iterates over the polyglot extracted entities and creates a list of person, locations, and organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import unicodedata\n",
    "\n",
    "def extraction(corpus):\n",
    "    \n",
    "    # extract entities from a single string; remove whitespace characters\n",
    "    try:\n",
    "        e = Text(re.sub('[\\s]',\" \",corpus)).entities\n",
    "    except:\n",
    "        pass #e = Text(re.sub(\"(r'(x0)',\" \",\"(re.sub('[\\s]',\" \",corpus)))).entities\n",
    "    \n",
    "    current_person =[]\n",
    "    persons =[]\n",
    "    current_org=[]\n",
    "    organizations=[]\n",
    "    current_loc=[]\n",
    "    locations=[]\n",
    "\n",
    "    for l in e:\n",
    "        if l.tag == 'I-PER':\n",
    "            for m in l:\n",
    "                current_person.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_person: # if the current chunk is not empty\n",
    "                        persons.append(\" \".join(current_person))\n",
    "                        current_person = []\n",
    "        elif l.tag == 'I-ORG':\n",
    "            for m in l:\n",
    "                current_org.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_org: # if the current chunk is not empty\n",
    "                        organizations.append(\" \".join(current_org))\n",
    "                        current_org = []\n",
    "        elif l.tag == 'I-LOC':\n",
    "            for m in l:\n",
    "                current_loc.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_loc: # if the current chunk is not empty\n",
    "                        locations.append(\" \".join(current_loc))\n",
    "                        current_loc = []\n",
    "    results = {}\n",
    "    results['persons']=persons\n",
    "    results['organizations']=organizations\n",
    "    results['locations']=locations\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extraction(document)['persons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regexp = re.compile(\"REFERENCES(.*)$\")\n",
    "references = Text(regexp.search(re.sub('[\\s]',\" \",document)).group(1)).entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regexp1 = re.compile(\"REFERENCES(.*)$\")\n",
    "references = Text(regexp.search(re.sub('[\\s]',\" \",document)).group(1)).entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(extraction(regexp.search(re.sub('[\\s]',\" \",document)).group(1))['persons'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truth Sets to test extraction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 authors\n",
      "\n",
      "There are 3 author organizations\n",
      "\n",
      "There are 7 author locations\n",
      "\n",
      "There are 152 authors in the references\n"
     ]
    }
   ],
   "source": [
    "#p19.pdf\n",
    "\n",
    "p19pdf_authors=['Tim Althoff','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang']\n",
    "p19pdf_author_organizations=['Computer Science Department','Stanford University','Google']\n",
    "p19pdf_author_locations=['Stanford, CA','Stanford','CA','Google','1600 Amphitheatre Parkway, Mountain View, CA 94043','1600 Amphitheatre Parkway','Mountain View']\n",
    "\n",
    "p19pdf_references_authors =['A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']\n",
    "p19pdf_allauthors=['Tim Althoff','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang','A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']\n",
    "\n",
    "print \"There are %r authors\" % len(p19pdf_authors)\n",
    "print  # white space\n",
    "print \"There are %r author organizations\" %len(p19pdf_author_organizations)\n",
    "print \n",
    "print \"There are %r author locations\" % len(p19pdf_author_locations)\n",
    "print  \n",
    "print \"There are %r authors in the references\" %len(p19pdf_references_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 authors\n",
      "\n",
      "There are 6 author organizations\n",
      "\n",
      "There are 8 author locations\n",
      "\n",
      "There are 106 authors in the references\n"
     ]
    }
   ],
   "source": [
    "#p29.pdf\n",
    "\n",
    "p29pdf_authors=['Laurent Amsaleg','Stéphane Girard','Oussama Chelly','Teddy Furon','Michael E. Houle','Ken-ichi Kawarabayashi',\n",
    "               'Michael Nett']\n",
    "p29pdf_author_organizations=['Equipe LINKMEDIA','Campus Universitaire de Beaulieu','CNRS/IRISA Rennes','National Institute of Informatics',\n",
    "                             'Equipe MISTIS INRIA','Google']\n",
    "p29pdf_author_locations=['Campus Universitaire de Beaulieu','35042 Rennes Cedex, France','France','-1-2 Hitotsubashi, Chiyoda-ku Tokyo 101-8430, Japan',\n",
    "                        'Japan','6-10-1 Roppongi, Minato-ku Tokyo 106-6126','Inovallée, 655, Montbonnot 38334 Saint-Ismier Cedex','Tokyo']\n",
    "\n",
    "p29pdf_references_authors =['A. A. Balkema','L. de Haan','N. Bingham', 'C. Goldie','J. Teugels','N. Boujemaa', 'J. Fauqueur', 'M. Ferecatu', 'F. Fleuret',\n",
    "                            'V. Gouet', 'B. LeSaux','H. Sahbi','C. Bouveyron', 'G. Celeux', 'S. Girard','J. Bruske', 'G. Sommer',\n",
    "                           'F. Camastra','A. Vinciarelli','S. Coles','J. Costa' ,'A. Hero','T. de Vries', 'S. Chawla','M. E. Houle',\n",
    "                           'R. A. Fisher','L. H. C. Tippett','M. I. Fraga Alves', 'L. de Haan','T. Lin','M. I. Fraga Alves', 'M. I. Gomes','L. de Haan',\n",
    "                           'B. V. Gnedenko',' A. Gupta', 'R. Krauthgamer','J. R. Lee','A. Gupta', 'R. Krauthgamer','J. R. Lee','M. Hein','J.-Y. Audibert',\n",
    "                           'B. M. Hill','M. E. Houle','M. E. Houle','M. E. Houle','M. E. Houle', 'H. Kashima', 'M. Nett','M. E. Houle', 'X. Ma', 'M. Nett',\n",
    "                            'V. Oria','M. E. Houle', 'X. Ma', 'V. Oria','J. Sun','M. E. Houle','M. Nett','H. Jegou', 'R. Tavenard', 'M. Douze','L. Amsaleg',\n",
    "                           'I. Jollie','D. R. Karger','M. Ruhl','J. Karhunen','J. Joutsensalo','Y. LeCun', 'L. Bottou', 'Y. Bengio', 'P. Haner',\n",
    "                           'J. Pickands, III','C. R. Rao','S. T. Roweis','L. K. Saul','A. Rozza', 'G. Lombardi', 'C. Ceruti', 'E. Casiraghi', 'P. Campadelli',\n",
    "                           'B. Scholkopf', 'A. J. Smola','K.-R. Muller','U. Shaft','R. Ramakrishnan',' F. Takens','J. Tenenbaum', 'V. D. Silva','J. Langford',\n",
    "                           'J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. Venna','S. Kaski',\n",
    "                           'P. Verveer','R. Duin','J. von Brunken', 'M. E. Houle', 'A. Zimek','J. von Brunken', 'M. E. Houle','A. Zimek']\n",
    "\n",
    "p29pdf_allauthors=['Laurent Amsaleg','Stéphane Girard','Oussama Chelly','Teddy Furon','Michael E. Houle','Ken-ichi Kawarabayashi',\n",
    "               'Michael Nett','A. A. Balkema','L. de Haan','N. Bingham', 'C. Goldie','J. Teugels','N. Boujemaa', 'J. Fauqueur', 'M. Ferecatu', 'F. Fleuret',\n",
    "                            'V. Gouet', 'B. LeSaux','H. Sahbi','C. Bouveyron', 'G. Celeux', 'S. Girard','J. Bruske', 'G. Sommer',\n",
    "                           'F. Camastra','A. Vinciarelli','S. Coles','J. Costa' ,'A. Hero','T. de Vries', 'S. Chawla','M. E. Houle',\n",
    "                           'R. A. Fisher','L. H. C. Tippett','M. I. Fraga Alves', 'L. de Haan','T. Lin','M. I. Fraga Alves', 'M. I. Gomes','L. de Haan',\n",
    "                           'B. V. Gnedenko',' A. Gupta', 'R. Krauthgamer','J. R. Lee','A. Gupta', 'R. Krauthgamer','J. R. Lee','M. Hein','J.-Y. Audibert',\n",
    "                           'B. M. Hill','M. E. Houle','M. E. Houle','M. E. Houle','M. E. Houle', 'H. Kashima', 'M. Nett','M. E. Houle', 'X. Ma', 'M. Nett',\n",
    "                            'V. Oria','M. E. Houle', 'X. Ma', 'V. Oria','J. Sun','M. E. Houle','M. Nett','H. Jegou', 'R. Tavenard', 'M. Douze','L. Amsaleg',\n",
    "                           'I. Jollie','D. R. Karger','M. Ruhl','J. Karhunen','J. Joutsensalo','Y. LeCun', 'L. Bottou', 'Y. Bengio', 'P. Haner',\n",
    "                           'J. Pickands, III','C. R. Rao','S. T. Roweis','L. K. Saul','A. Rozza', 'G. Lombardi', 'C. Ceruti', 'E. Casiraghi', 'P. Campadelli',\n",
    "                           'B. Scholkopf', 'A. J. Smola','K.-R. Muller','U. Shaft','R. Ramakrishnan',' F. Takens','J. Tenenbaum', 'V. D. Silva','J. Langford',\n",
    "                           'J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. Venna','S. Kaski',\n",
    "                           'P. Verveer','R. Duin','J. von Brunken', 'M. E. Houle', 'A. Zimek','J. von Brunken', 'M. E. Houle','A. Zimek']\n",
    "\n",
    "\n",
    "print \"There are %r authors\" % len(p29pdf_authors)\n",
    "print  # white space\n",
    "print \"There are %r author organizations\" %len(p29pdf_author_organizations)\n",
    "print \n",
    "print \"There are %r author locations\" % len(p29pdf_author_locations)\n",
    "print  \n",
    "print \"There are %r authors in the references\" %len(p29pdf_references_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.sub('[\\s]',\" \",document)[9300:10500]\n",
    "#regexp.search(re.sub('[\\s]',\" \",document)).group(1)[4900:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extraction(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class entities(object):\n",
    "  def __init__(self):\n",
    "    self.persons = extraction(document)['persons']\n",
    "    self.organizations = extraction(document)['organizations']\n",
    "\n",
    "my_shape = entities()\n",
    "my_shape.persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to extract emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import emailextractor\n",
    "from emailextractor import file_to_str, get_emails\n",
    "tuple(get_emails(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get only the Title and Author Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Estimating Local Intrinsic Dimensionality  Laurent Amsaleg Equipe LINKMEDIA,  CNRS/IRISA Rennes, France  Campus Universitaire de  Beaulieu  35042 Rennes Cedex, France laurent.amsaleg@irisa.fr  Stphane Girard Equipe MISTIS, INRIA  Grenoble, France  Inovalle, 655, Montbonnot 38334 Saint-Ismier Cedex, stephane.girard@inria.fr  France  Teddy Furon  Equipe LINKMEDIA,  INRIA/IRISA Rennes, France  Campus Universitaire de  Beaulieu  35042 Rennes Cedex, France  teddy.furon@inria.fr  Ken-ichi Kawarabayashi  National Institute of Informatics, Japan 2-1-2 Hitotsubashi,  Chiyoda-ku  Tokyo 101-8430, Japan k_keniti@nii.ac.jp  Oussama Chelly National Institute of Informatics, Japan 2-1-2 Hitotsubashi,  Chiyoda-ku  Tokyo 101-8430, Japan  chelly@nii.ac.jp Michael E. Houle National Institute of Informatics, Japan 2-1-2 Hitotsubashi,  Chiyoda-ku  Tokyo 101-8430, Japan  meh@nii.ac.jp Michael Nett Google, Japan  6-10-1 Roppongi, Minato-ku  Tokyo 106-6126, Japan mnett@google.com  '"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=re.compile('(.*)(?=ABSTRACT)')\n",
    "p.search(re.sub('[\\s]',\" \",document)).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get Title Only (or most of it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Estimating Local Intrinsic Dimensionality'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=re.compile('(.+)(\\\\n\\\\n)')\n",
    "q=re.compile('(?<=\\\\n\\\\n)(.+?)(?=\\\\n\\\\n)')\n",
    "#p.search(document).group(1)+\" \"+q.search(document).group(1)\n",
    "p.search(document).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get Abstract only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p= re.compile('(?<=ABSTRACT)(.+)(?=Categories and Subject Descriptors)')\n",
    "p.search(re.sub('[\\s]',\" \",document)).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get keywords only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Frequent Itemsets; Pattern Mining; Rademacher Averages; Sampling; Statistical Learning Theory  1.  '"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= re.compile('(?<=Keywords)(.+)(?=INTRODUCTION)')\n",
    "p.search(re.sub('[\\s]',\" \",document)).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Database Management']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= re.compile('(?<=Categories and Subject Descriptors)(.+)(?=Keywords)')\n",
    "re.findall('\\[(.*?)\\]',p.search(re.sub('[\\s]',\" \",document)).group(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get Body only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  In an attempt to improve the discriminability of similar- ity measures, and the scalability of methods that depend on them, much attention has been given in the areas of machine learning, databases, and data mining to the development of dimensional reduction techniques. Linear techniques for di- mensionality reduction include Principal Component Anal- ysis (PCA) and its variants [4, 24]. Non-linear dimension- ality reduction methods  also known as manifold learn- ing techniques  include Isometric Mapping [36], Multi- Dimensional Scaling [35,37], Locally Linear Embedding and its variants [30], and Non-Linear Component Analysis [32]. Most reduction techniques require that a target dimension be provided by the user, although some attempt to deter- mine the dimension automatically. Ideally, the supplied di- mension should depend on the intrinsic dimensionality (ID) of the data. This has served to motivate the development of models of ID, as well as accurate estimators.  Over the past few decades, many practical models of the intrinsic dimensionality of data sets have been proposed. Examples include the previously mentioned Principal Com- ponent Analysis and its variants [4, 24], as well as several manifold learning techniques [26, 30, 32, 37]. Topological ap- proaches to ID estimate the basis dimension of the tangent space of the data manifold from local samples [5,38]. Fractal methods such as the Correlation Dimension (CD) estimate an intrinsic dimension from the space-lling capacity of the data [6, 14]. Graph-based methods use the k-nearest neigh- bors graph along with density in order to estimate ID [8].  The aforementioned intrinsic dimensionality measures can be described as global, in that they consider the dimension-  29 ality of a given set as a whole, without any individual object being given a special role. In contrast, local ID measures are dened in this paper as those that involve only the k- nearest neighbor distances of a specic location in the space. Several local intrinsic dimensionality models have been pro- posed recently, such as the expansion dimension (ED) [25], the generalized expansion dimension (GED) [19], the min- imum neighbor distance (MiND) [31], and local continuous intrinsic dimension (which we will refer to here as LID) [17]. These models quantify ID in terms of the rate at which the number of encountered objects grows as the considered range of distances expands from a reference location.  Local approaches can be very useful when data is com- posed of heterogeneous manifolds. In addition to applica- tions in manifold learning, measures of local ID have been used in the context of similarity search, where they are used to assess the complexity of a search query [22, 25], or to control the early termination of search [20, 21]. They have also found applications in outlier detection, in the analysis of a projection-based heuristic [9], and in the estimation of local density [39]. The eciency and eectiveness of the algorithmic applications of intrinsic dimensional estimation (such as [20, 21]) depends greatly on the quality of of the estimators employed.  Distances from a query point can be seen as realizations of a continuous positive random variable. In this case, the smallest distances encountered would be extreme events associated with the lower tail of the underlying distance dis- tribution. In Extreme Value Theory (EVT), a discipline of statistics concerned with the study of tails of continu- ous probability distributions, the random variable associated with nearest neighbor distances can be assumed to follow a power-law distribution [7]. Continuous lower-bounded ran- dom variables are known to asymptotically converge to the Weibull distribution as the sample size grows, regardless of the original distance measure and its distribution. In an equivalent formulation of EVT due to Karamata, the cu- mulative distribution function of a tail distribution can be represented as a regularly-varying (RV) function whose dom- inant factor is a polynomial in the distance [7,18]; the degree (or index) of this polynomial factor determines the shape parameter of the associated Weibull distribution, or equiva- lently the exponent of the associated power law. The index has been interpreted as a form of intrinsic dimension [7]. Maximum likelihood estimation of the index leads to the well-known Hill estimator for power-law distributions [16]. While EVT provides an asymptotic description of tail dis- tributions, in the case of continuous distance distributions, the distribution can be exactly characterized in terms of LID [18]. The LID model introduces a function that assesses the discriminative power of the distribution at any given dis- tance value [17, 18]. A distance measure is described as dis- criminative when an expansion in the distance results in a relatively small increase in the number of observations. This function is shown to fully characterize the cumulative dis- tribution function without the explicit involvement of the probability density [18]. The limit of this function yields the skewness of the Weibull distribution (or equivalently, the Karamata representation index, or power law exponent) associated with the lower tail. It is the estimation of this limit that is the main focus of this paper.  In addition to the more traditional applications stated ear- lier, LID has the potential for wide application in many ma-  chine learning and data mining contexts, as it makes no as- sumptions on the nature of the data distribution other than continuity.  The main original contributions of this paper are:  a framework for the estimation of local continuous in- trinsic dimension (LID) using well-established tech- niques: the maximum likelihood estimation (MLE), the method of moments (MoM), and the method of probability-weighted moments (PWM). In particular, we verify that applying MLE to LID leads to the well- known Hill estimator [16].   a new family of estimators based on the extreme-value- theoretic notion of regularly varying functions. Several existing dimensionality models (ED, GED, and MiND) are shown to be special cases of this family.   condence intervals for the variance and convergence  of the estimators we propose.   an experimental study using articial data and syn- thetic distance distributions, in which we compare our estimators with state-of-the-art global and local esti- mators. We also show that the empirical variance and convergence rates of the MLE (Hill) and MoM estima- tors are superior to those of the other local estimators studied.   experiments showing that local estimators are more ro- bust than global ones in the presence of noise in non- linear manifolds. Our experiments show that our ap- proaches are very competitive in this regard with other methods, both local and global.   proles of several real-world data sets in terms of LID, illustrating the degree of variability of complexity from region to region within a dataset. The proles demon- strate that a single global ID value is in general not sucient to fully characterize the complexity of real- world data.  2. CONTINUOUS INTRINSIC DIMENSION LID [17] aims to quantify the local ID of a feature space exclusively in terms of the distribution of inter-point dis- tances. Formally, let (Rm, d) be a domain equipped with a non-negative distance function d. Let us consider the distribution of distances within the domain with respect to some xed point of reference. We model this distribution in terms of a random variable X with support [0,). X is said to have probability density fX, where fX is a non-negative Lebesgue-integrable function, if and only if  Pr[a  X  b] =  fX(x) dx,  (cid:90) b  x=a  (cid:90) x  for any a, b  [0,) such that a  b. The corresponding cumulative density function FX is canonically dened as  FX(x) = Pr[X  x] =  fX(u) du.  u=0  Accordingly, whenever X is absolutely continuous at x, FX is dierentiable at x and its rst-order derivative is fX(x).  Definition 1  (Houle [17]). Given an absolutely con- tinuous random distance variable X, for any distance thresh- old x such that FX(x) > 0, the local continuous intrinsic  30 dimension of X at distance x is given by  IDX(x) (cid:44) lim0+  ln FX ((1 + )x)  ln FX(x)  ln(1 + )  wherever the limit exists.  With respect to the generalized expansion dimension [19], a precursor of LID, the above denition of IDX(x) is the out- come of a dimensional test of neighborhoods of radii x and (1+)x in which the neighborhood cardinalities are replaced by the expected number of neighbors. LID also turns out to be equivalent to a formulation of the (lack of) discriminative power of a distance measure, as both formulations have the same closed form:  Theorem 1  (Houle [17]). Let X be an absolutely con- tinuous random distance variable. If FX is both positive and dierentiable at x, then  IDX(x) =  xfX(x) FX(x)  .  3. EXTREME VALUE THEORY  Extreme value theory is concerned with the modeling of what can be regarded as the extreme behavior of stochastic processes. Its best known theorem, attributed in parts to Fisher and Tippett [10], and Gnedenko [13], states that the maximum of N independent identically-distributed random variables (after proper renormalization) converges in distri- bution to a generalized extreme value distribution as N goes to innity. 3.1 Threshold excesses  Consider the following two denitions. Definition 2. Let   R and  > 0. The family of gen- eralized Pareto distributions is dened by its cumulative dis- tribution function:  (cid:18)  (cid:19) 1    .  FX (x) = 1   1 +  x   Definition 3. Let X be a random variable whose distri- bution FX has the upper endpoint x+  R  {}. Given w < x+, the conditional excess distribution FX,w of X is the distribution of X  w conditioned on the event X > w:  FX,w (x) =  FX(w + x)  FX(w)  1  FX(w)  .  We are now in a position to introduce a powerful theorem due to Balkema and de Haan [1], and Pickands [28], which can be regarded as the counterpart to the central limit the- orem for extremal statistics.  Theorem 2  (Balkema-de Haan [1], Pickands [28]). Let (Xi)iN be a sequence of independent random variables with identical distribution function FX satisfying the condi- tions of the Fisher-Tippett-Gnedenko Theorem. As w  x+, FX,w (x) converges to a distribution in FGPD.  In the following we demonstrate a direct relation between local ID and extreme value theory, which arises as an im- plication of Theorem 2. Note that any choice of distance threshold w corresponds to a neighborhood of radius w based  at the reference point, or equivalently, to the tail of the dis- tribution of distances on [0, w). As discussed in [7], Theo- rem 2 also applies to lower tails: one can reason about min- ima using the transformation Y = X. The distribution of the excess Y  (w) (conditioned on Y > w) then tends to a distribution in FGPD, as w tends to the lower endpoint of FX located at zero. Accordingly, as w tends to zero, the distribution in the tail [0, w) can be restated as follows [7]. Lemma 1. Let X be an absolutely continuous random dis- tance variable with support [0,) and cumulative distribu- tion function FX such that FX(x) > 0 if x > 0. Let c  (0, 1) be an arbitrary constant. Let w > 0 be a distance threshold, and consider x restricted to the range [cw, w). As w tends to zero, the distribution of X restricted to the tail [cw, w) satises, for some xed  < 0:   1    (x/w) FX,w (x)   1  Note that the distribution of excess distance w  X is bounded from above by w which, according to [7], enforces that  < 0.  To summarize, whenever Theorem 2 applies to a distance variable X, the cumulative distribution of distances within a radius-w neighborhood is asymptotically determined by a single parameter  < 0. We can prove the following state- ment concerning LID.  Theorem 3. Let X be an absolutely continuous random distance variable with support [0,), satisfying the condi- tions of Theorem 2, and w > 0 be a distance threshold. Then, as w tends to zero,  IDX(w)   1   =: IDX.  Proof. Omitted due to space limitations.  Note that together Lemma 1 and Theorem 3 allow us to restate the asymptotic cumulative distribution of distances in the tail [cw, w) as  (x/w)IDX FX,w (x)   1.  (1)  3.2 Regularly-varying functions  The Fisher-Tippett-Gnedenko Theorem and the Pickands- Balkema-de Haan Theorem have been shown to be equiva- lent to a third characterization of the tail behavior, in terms of regularly-varying (RV) functions. The asymptotic cumu- lative distribution of X in the tail [0, w) can be expressed as FX(x) = x(cid:96)X(1/x), where (cid:96)X is dierentiable and slowly varying; that is, for all c > 0, (cid:96)X satises  lim t  (cid:96)X(ct) (cid:96)X(t)  = 1.  FX restricted to [0, w) is itself said to be regularly varying with index . In particular, a cumulative distribution F  FGEV has  < 0 if and only if F is RV and has a nite endpoint. Note that the slowly-varying component (cid:96)X(1/x) of FX is not necessarily constant as x tends to zero. For a detailed account of RV functions, we refer the reader to [2]. The following corollary is a straightforward extension of  the examples given in Section 2.  Corollary 1. Let X be a random distance variable re- stricted to [0, w) with distribution FX(x) = x(cid:96)X(1/x). As w tends to zero, the index  converges to IDX.  31 4. ESTIMATION  This section is concerned with practical methods for the estimation of the local intrinsic dimension of a random dis- tance variable X. In particular, we adapt known GPD pa- rameter estimators such as the maximum-likelihood estima- tor (in Section 4.1) and moment based estimators (in Sec- tions 4.2 and 4.3), and propose a new estimator based on regularly varying functions (in Section 4.4).  For the remainder of this discussion we assume that we are given a sequence x1, . . . , xn of observations of a random distance variable X with support [0, w), in ascending order  that is, x1  x2    xn. 4.1 Maximum Likelihood Estimation  Using the asymptotic expression of the distance distribu- tion given in Equation 1, we see that the log-likelihood of IDX for the sample is L(IDX) = n ln  n(cid:88) Accordingly, the maximum-likelihood estimate (cid:99)IDX is  + n ln IDX + (IDX  1)  FX,w (w)  xi w  i=1  ln  w  .  (cid:99)IDX =   (cid:18) 1  (cid:88)n  n  i=1  ln  xi w  (cid:19)1  ,  which follows the form of the well-known Hill estimator for the scaling exponent of a power-law tail distribution [16].  The MLE model ensures the usual regularity conditions that guarantee the consistency, the asymptotic normality and the eciency of this estimator. The variance is asymp- totically given by the inverse of the Fisher information de- ned as:  (cid:20)  I = E   2L(IDX)   ID2 X  =  n ID2 X  ,  where E[] denotes the expectation. Therefore, if the number / n). Accordingly, with probability 1  , a sample of n dis-  of samples n is suciently large, we have(cid:99)IDX  N (IDX, ID2 tances in [0, w) provides an estimate (cid:99)IDX lying within  X  (cid:21)  (cid:18)  IDX  IDX n  1    1   2  In other words, the 1   condence interval is  (cid:34)  (cid:99)IDX  .  (cid:19) (cid:99)IDX  1 + n1/21(1  /2)  ,  1  n1/21(1  /2)  (cid:35)  .  4.2 Method of Moments  For any choice of k  N, the k-th order non-central mo-  ment k of the random distance X is  k = E  =  xkfX(x) dx = wk  (cid:104)  Xk(cid:105)  (cid:90) w  x=0  IDX  IDX + k  .  ,  (cid:17)  (cid:16) k (cid:80)n  wk  Solving for the intrinsic dimension gives  IDX = k  k  k  wk = g  with g(x) = k x by its empirical counterpart k = 1 n E[k] = k and E[2  1x . When estimating the order-k moment i , we see that  r] = (n2k + n(n  1)2  k)n2, so that  i=1 xk  Var[2  k] =  2k  2  k  n  =  w2kIDXk2  n(IDX + 2k)(IDX + k)2 .  mk,l,m = E  (cid:18) IDX  Therefore, the distribution of k with  wk is asymptotically normal  k  wk  N  ;  IDXk2  n(IDX + 2k)(IDX + k)2  IDX + k  According to [29, Th. 6a2.9], if x  N (; 2n1) asymp- totically, then g(x)  N (g(); 2n1g(cid:48)()2), where g(cid:48) is the rst-order derivative of g. Therefore, asymptotically  (cid:18)  (cid:99)IDX  N  (cid:18)  (cid:19)  .  (cid:19)(cid:19)  .  IDX;  ID2 X n  1 +  (k/IDX)2  ID2  X(1 + 2k/IDX)  This variance is monotonically increasing in k/IDX, which indicates that we should use moments of small order k. When k/IDX tends to zero, the variance converges to ID2 X/n, the variance of the maximum-likelihood estimator (see Sec- tion 4.1). Note that an upper bound on IDX implies that the variance is bounded. In this case we can derive condence intervals similar to Section 4.1. 4.3 Probability-Weighted Moments  General probability-weighted moments are dened as  (cid:104)  FX(x)k(1  FX(x))lXm(cid:105) (cid:105)  (cid:90) w  .  We restrict here our attention to a subfamily: for any choice of k  N, k is dened as FX(x)kX  FX(x)kxfX(x) dx  k (cid:44) E  =  (cid:104)  =  IDX w  IDX k + IDX + 1  x=0  ;  solving for the intrinsic dimension yields  IDX =  k  w  k(k + 1)  = h  (cid:17)  ,  (cid:16) k  w  x  1(k+1)x .  where h(x) = 4.4 Estimation Using Regularly Varying Func-  tions  In this section we introduce an ad hoc estimator for the intrinsic dimensionality based on the characterization of dis- tribution tails as regularly varying functions (as discussed in Section 3). Consider the empirical distribution function FX, dened as  FX(x) =  where (cid:74)(cid:75) refers to the Iverson bracket which evaluates to  1 if  is true, and 0 otherwise. We propose the following estimator for the index  of FX.  (cid:88)n j=1(cid:74)xj < x(cid:75) ,  1 n  Definition 4. Let X be an absolutely continuous random distance variable restricted to [0, w). The local intrinsic di- mension IDX can be estimated as  (cid:104) FX((1 + jn)xn)/ FX(xn) (cid:80)J  j=1 j ln(1 + jn)  (cid:105)  ,  (cid:99)IDX =  =  (cid:80)J  j=1 j ln  under the assumption that xn, n  0 as n  , where (j)1jJ and (j)1jJ are sequences.  We will refer to this family of estimators as RV, for reg- ularly varying. Note that since RV estimators involve only the products jn for 1  j  J, we may assume without loss of generality that 1 +  + J = 1. The estimators are based on the observation that, for all 1  j  J,  32 ln [FX((1 + jn)xn)/FX(xn)] =  ln(1 + jn) + ln [(cid:96)X((1 + jn)xn)/(cid:96)X(xn)] (cid:39)  ln(1 + jn).  The RV family covers several of the known local estima- tors of intrinsic dimensionality. For the parameter choices J = 1 and  =  n, the RV estimator reduces to the GED formulation proposed in [19]:  (cid:104) FX((1 + )xn)/ FX(xn)  (cid:105)  ln  (cid:99)IDX =  ln(1 + )  ,  By setting  = 1, Karger & Ruhls expansion dimension is obtained, while by setting xn as the distance to the k- nearest neighbor and  such as (1 + )xn as the distance to the nearest neighbor, we nd a special case of the MiND family (MiNDml1) [31]. Alternatively, by setting J = n, i = 1 for all i  [1..n], , the RV  and choosing the vector  such that 1 + in = xi xn estimator becomes  (cid:80)n (cid:80)n  (cid:99)IDX =  j=1 ln [j/n] j=1 ln [xj/xn]  (cid:80)n   ln   2n  n j=1 ln [xj/xn]  As n  , this converges to the MLE (Hill) estimator pre- sented in Section 4.1, with w = xn.  We now turn our attention to an analysis of the variation of RV estimators. First, we introduce an auxiliary function which drives the speed of convergence of the estimator pro- posed in Denition 4. For x  R let X(x) be dened as  X(x) (cid:44) x(cid:96)(cid:48)  X(x) (cid:96)X(x)  .  In [11, 12], the auxiliary function is assumed to be regu- larly varying, and the estimation of the corresponding reg- ular variation index is addressed. Within this article, so as to prove the following results, we limit ourselves to the assumption that X is ultimately non-increasing.  Theorem 4. Let X be a random distance variable over [0, w) with distribution function FX(x) = x(cid:96)X(1/x), and let max (cid:44) max1jJ j. Furthermore, let n, xn  0 so that  n FX(xn)n   and(cid:112)nFX(xn)nX(1/[(1+maxn)xn])  ultimately non-increasing, then (cid:112)nFX(xn)n [IDX (cid:99)IDX]  0 as n approaches innity. If the auxiliary function X is  converges to a centered Gaussian with variance  IDXV, = IDX  (cid:62)S ((cid:62) )2 ,  where Sa,b = (|a|  |b|)(cid:74)ab > 0(cid:75) for (a, b)  {1, . . . , J}2.  (A  B denotes the minimum of A and B.)  Note that the requirement nFX(xn)n   can be inter- preted as a necessary and sucient condition for the almost sure presence of at least one distance sample in the interval [xn, (1 + jn)xn)]. In addition, the condition  (cid:112)nFX(xn)nX(1/[rn(1 + maxn)])  0  enforces that the approximation bias X(1/[(1 + n)xn]) is negligible compared to the standard deviation of the esti-  mate, 1/(cid:112)nFX(xn)n. We continue the analysis by propos-  ing choices of  that minimize the variance in Theorem 4.  Lemma 2. The weight vector  = (1, . . . , J )(cid:62) mini- mizing V, is proportional to 0 = S1 = (1, 0, . . . , 0)(cid:62), and the associated optimal variance is given by V0( ) =  (cid:0)(cid:62)S1(cid:1)1  .  Proof. Omitted due to space limitations. For the case J = 1, we see that  = (1)(cid:62) and V0(1) = 1. This indicates that the GED minimizes the variance of es- timation. However, dierent choices can be made regarding the weight vector  and regarding the criterion to use in or- der to optimize the choice of . Minimizing variance is one choice explored in this paper, but other criteria can be used. In general, however, the following condence interval holds for RV estimators:  Lemma 3. Let   (0, 1), and assume that the assump- tions of Theorem 4 hold with  = S1 . Let u = 1((1 + )/2), where  is the cumulative distribution function of the standard Gaussian distribution. Then  (cid:16) nnV0( )(cid:99)IDX FX(xn)  (cid:17)1/2  IDX  u  are the boundaries of the asymptotic condence interval of  level  for (cid:99)IDX.  Proof. Lemma 3 is a direct consequence of the asymp- totic distribution established in Theorem 4 and the conver- gence of FX(xn) to FX(xn) as n  . 5. EXPERIMENTAL FRAMEWORK 5.1 Methods  The methods used in this study include MLE, MoM, PWM, and RV. The RV estimators are evaluated for the choices J = 1 and J = 2, as follows:  (cid:40) ln nln(cid:98)n/2(cid:99)  ln xnln x(cid:98)n/2(cid:99) , ln(cid:98)n/j(cid:99)(p1) ln(cid:98)i/j(cid:99) ln xn/xj +(p1) ln xi/xj  if J = 1  ,  if J = 2,  (cid:99)IDRV =  where p = (xi  2xj + xn)/(xn  xj), i = (cid:98)n/2(cid:99), and j = (cid:98)3n/4(cid:99). Note that the estimator RV for J = 1 is a form of generalized expansion dimension (GED) [19]. For every dataset, we report the average of ID estimates across all the points in the dataset. All estimators in our study can be computed in time linear in the number of sample points.  Parameters threshold = 0.025 k = 100,  = 1, M = 1, N = 10 k = 100,  = 1, M = 10, N = 1  Method PCA kNNG1 kNNG2 MiNDml1 None MiNDmli  k = 100  Table 1: Parameter choices used in the experiments.  Our experimental framework includes several state-of-the- art intrinsic dimensionality measures. The global estima- tors consist of a projection method (PCA), fractal methods (CD [6], Hein [15], Takens [34]), and graph-based methods (kNNG1, kNNG2 [8]). The local distance-based estimators are MiNDml1 and MiNDmli [31]. Table 1 summarizes the parameter choices for every method, except for the fractal methods, which do not involve any parameter.  The MiND variants makes more restrictive assumptions than our methods: they assume the data to be uniformly dis- tributed on a hypersphere, with a locally isometric smooth  33 (a) ID = 2  (b) ID = 8  (c) ID = 32  (d) ID = 128  Figure 1: Comparison of the mean and standard deviation of LID estimates provided by MLE, MoM and RV (for J = 1 and J = 2) on increasingly large samples drawn from articially-generated distance distributions. The results cover target dimensionality values of 2, 8, 32, and 128. The values are marked in the corresponding plots.  map between the hypersphere and the representational space. MiND uses only the two extreme samples (smallest and largest), and requires knowledge of the dimension of the space (D). In contrast, our approach assumes only that the nearest neighbor distances are in the lower tail of the dis- tance distribution, where EVT estimation can be performed. 5.2 Articial Distance Distributions  In the following we propose a set of experiments concern- ing articial data, and describe the method employed for the generation of test data.  First, consider a point P drawn uniformly at random from within the m-dimensional unit sphere, for some choice of m  N. According to the method of normal variates, we dene P = Z1/mY(cid:107)Y(cid:107)1, where Z is uniformly distributed on [0, 1], and Y is a random vector in Rm whose coecients follow the standard normal distribution. The distance of P, with respect to our choice of reference point at location 0  Rm, is distributed as follows. (cid:107)Z1/mY(cid:107)  X =  (cid:107)Y(cid:107) = Z1/m.  Note that, by measuring LID purely based on distance values with respect to a reference point, the model does not require that the data have an underlying spatial representation. As such, non-integer values of m  R can be selected for the generation of distances, if desired. For choices of m  {2, 8, 32, 128}, we draw 100 indepen- dent sequences of sample distance values from the distribu- tion described above, and record the estimates produced by each of our methods for sample sizes n between 10 and 104. 5.3 Articial Data  The data sets used in our experiments have been proposed in [31]. They consist of 15 manifolds of various stuctures and intrinsic dimensionalities (d) represented in spaces of dierent dimensions (D). They are summarized in Table 2. These datasets were generated in dierent sizes (103, 104, and 105 points) in order to evaluate the eect of the num-  Manifold  d D Description  1 2 3  4 5 6 7 8 9  10a 10b 10c 11 12 13  10 3 4  4 2 6 2 12 20 10 17 24 2 20 1  11 Uniformly sampled sphere.  5 Ane space. 6 Concentrated gure  confusable with a 3d one.  8 Non-linear manifold. 3  2-d Helix  36 Non-linear manifold.  3  Swiss-Roll.  72 Non-linear manifold. 20 Ane space. 11 Uniformly sampled hypercube. 18 Uniformly sampled hypercube. 25 Uniformly sampled hypercube. 3 Mobius band 10-times twisted.  Isotropic multivariate Gaussian.  20 13 Curve.  Table 2: Articial datasets used in the experiments.  ber of points on the quality of the dierent estimators. For each dataset and for each of the three sizes, we average the estimates over 20 instances.  In order to evaluate the robustness of the estimators, we also prepared versions of these datasets with noise added. For each attribute f , we added normally-distributed noise with mean equal to zero and standard deviation n = p  f where f is the standard deviation of the attribute itself, and p  {0.01, 0.04, 0.16, 0.64}. For attributes with f = 0, the noise was generated with standard deviation n = p  f where  f is the minimum of the nonzero standard deviations over all attributes.  5.4 Real Data  Not only can a reliable estimation of ID greatly benet the practical performance of many applications, it also serves as a characterization of high-dimensional data sets and the po- tential problems associated with their use in practice. To this end, we investigate the distribution of LID estimates on   1.8 2 2.2 2.4 2.6 2.8 3 3.2 3.4 3.6101102103104Mean Estimated IDNumber of samplesIDMLEMoMRV (J=1)RV (J=2) 0 0.2 0.4 0.6 0.8 1 1.2101102103104Standard Deviation / IDNumber of samplesMLEMoMRV (J=1)RV (J=2) 7 8 9 10 11 12 13101102103104Mean Estimated IDNumber of samplesIDMLEMoMRV (J=1)RV (J=2) 0 0.2 0.4 0.6 0.8 1 1.2101102103104Standard Deviation / IDNumber of samplesMLEMoMRV (J=1)RV (J=2) 30 32 34 36 38 40 42 44 46 48101102103104Mean Estimated IDNumber of samplesIDMLEMoMRV (J=1)RV (J=2) 0 0.2 0.4 0.6 0.8 1 1.2101102103104Standard Deviation / IDNumber of samplesMLEMoMRV (J=1)RV (J=2) 120 130 140 150 160 170 180 190 200 210 220101102103104Mean Estimated IDNumber of samplesIDMLEMoMRV (J=1)RV (J=2) 0 0.2 0.4 0.6 0.8 1 1.2101102103104Standard Deviation / IDNumber of samplesMLEMoMRV (J=1)RV (J=2)34 Figure 2: Plots of the distribution of LID values across 104 distinct query locations for each data set. The LID values were obtained using the MLE estimator on the size-1000 neighborhoods of the individual reference points.  Figure 3: Histograms of LID values across 104 distinct query locations for each data set, obtained using the MLE estimator on the size-1000 neighborhoods of the individual reference points.  the following data sets, each taken from a real-world appli- cation scenario.  The ALOI (Amsterdam Library of Object Images) data set contains a total of 110250 color photos of 1000 dier- ent objects taken from varying viewpoints under various il- lumination conditions. Each image is described by a 641- dimensional vector of color and texture features [3].  The MNIST database [27] contains of 70000 recordings of handwritten digits. The images have been normalized and discretized to a 28  28-pixel grid. The gray-scale values of the resulting 784 pixels are used to form the feature vectors. The ANN SIFT1B data set consists of 128-dimensional SIFT descriptors extracted from a collection of  109 im- ages. This set has been created for the evaluation of nearest- neighbor search strategies at very large scales [23].  For each data set, we estimate LID with respect to 104 dis- tinct reference points, based on the distribution of distances to their respective 103-nearest neighbors. For ANN SIFT1B we use a selection of 104 query points that is provided with the data. In the case of ALOI and MNIST, we computed distance samples with respect to 104 points selected uni- formly at random.  6. EXPERIMENTAL RESULTS 6.1 Articial Distance Distributions  We begin our experimental study with an assessment  in terms of bias, variance, and convergence  of the ability of each estimator to identify the ID of a sample of distance values generated according to dierent choices of target ID. Note that for these trials, the distributional model asserted in Lemma 1 holds everywhere on the range [0, w) by con- struction (with w = 1).  Fig. 1 shows the behavior of MLE, MoM, and RV (for choices of J = 1 and J = 2). The convergence to the tar- get ID value observed in every case empirically conrms the consistency of these estimators. Likewise, PWM is consis-  tent however, one should beware of PWMs susceptibility to the eects of numerical instability.  We also note that the RV estimator with J = 1 (GED)  which asymptotically minimizes variance according to Lemma 2  is not the choice that minimizes variance when the number of samples is limited. Faster initial convergence favors the choice of MLE and MoM for applications where the number of available query-to-neighbor distances is lim- ited, or where time complexity is an issue. 6.2 Articial Data  In Tables 3 and 4, due to space limitations, we present only a representative selection of the experimental results, averaged over 20 runs each. It should be noted that as PCA and MiNDmli estimates are restricted to integer values, their bias is lower for examples having integer ground-truth intrin- sic dimension, especially when this dimensionality is small. Also, unlike the other estimators tested, MiND estimators also require that an upper bound on the ID be supplied (set to D in these experiments). PCA requires a threshold parameter to be supplied, the value of which can greatly inuence the estimation.  The experimental results indicate that local estimators tend to over-estimate dimensionality in the case of non-linear manifolds (sets m3, m4, m5, m6, m7, m8, m11 and m13) and to under-estimate it in the case of linear manifolds (sets m1, m2, m9, m10a, m10b, m10c and m12). For highly non-linear manifolds, such as the Swiss Roll (m7), global estimators have diculty in identifying the intrinsic dimension. The experimental results with higher sampling rates conrm the reduction in bias that would be expected with smaller k- nearest-neighbor distances, as the local manifold structure more closely approximates the tangent space.  To show the eects of noise on the estimators, we display in Tables 5, 6 and 7 for each method the deviation of every estimate in the presence of noise as a proportion of the estimate obtained in the absence of noise. On the one hand, we note that global methods, k-NNG in particular, are   0 5 10 15 20 25 30 350K2K4K6K8K10KEstimated IDQueryALOI 0 5 10 15 20 25 30 350K2K4K6K8K10KEstimated IDQueryMNIST 0 5 10 15 20 25 30 350K2K4K6K8K10KEstimated IDQueryANN_SIFT1BABDEFCG0K1K2K3K4K 0 5 10 15 20 25 30 35Abs. FrequencyEstimated IDALOI0K1K2K3K4K 0 5 10 15 20 25 30 35Abs. FrequencyEstimated IDMNIST0K1K2K3K4K 0 5 10 15 20 25 30 35Abs. FrequencyEstimated IDANN_SIFT1B35 Dataset  m1 m2 m3 m7 m8 m9  m10a m10c m11 m12  d D IDMLE 8.07 10 3 2.67 3.56 4 2.49 2 12.29 12 12.39 20 7.39 10 24 14.05 2.49 2 20 12.48  11 5 6 3 72 20 11 25 3 20  Dataset  m1 m2 m3 m7 m8 m9  m10a m10c m11 m12  d D IDMLE 9.04 10 2.88 3 3.86 4 2 1.96 13.72 12 14.47 20 8.20 10 16.66 24 1.99 2 20 15.46  11 5 6 3 72 20 11 25 3 20  Dataset  m1 m2 m3 m7 m8 m9  m10a m10c m11 m12  d D IDMLE -10.07 10 2.43 3 -30.83 4 -8.67 2 44.17 12 20 -21.77 21.46 10 7.98 24 32.16 2 20 -22.83  11 5 6 3 72 20 11 25 3 20  IDMoM IDPWM IDGED 7.91 2.65 3.55 3.22 11.97 11.96 7.28 13.52 3.05 11.85  8.14 2.68 3.59 3.04 12.51 12.50 7.47 14.22 2.94 12.43  8.08 2.67 3.56 2.80 12.33 12.40 7.40 14.07 2.74 12.46  IDRVE MiNDml1 MiNDmli 8.95 3.00 4.00 2.00 13.00 13.50 8.00 15.35 2.00 14.00  7.79 2.60 3.49 3.12 11.79 11.79 7.16 13.32 2.97 11.67  9.50 2.94 3.88 2.00 13.49 15.03 8.50 17.69 2.01 16.79  CD Hein Takens 9.44 9.24 2.87 2.91 3.66 3.63 1.95 1.95 11.85 11.00 14.68 12.84 8.45 8.42 16.82 16.90 2.00 1.99 13.69 13.64  5.35 2.75 3.70 1.90 3.60 4.30 8.15 6.05 2.70 3.70  kNNG1 7.96 2.53 4.00 3.10 14.28 19.68 10.69 17.31 2.83 11.71  kNNG2 7.02 2.52 2.88 2.86 12.56 10.84 6.65 29.77 2.59 5.13  Table 3: ID estimates for 1000 points.  IDMoM IDPWM IDGED 9.06 2.90 3.92 1.99 13.91 14.41 8.21 16.54 2.04 15.23  9.32 2.94 3.97 2.02 14.50 15.08 8.43 17.45 2.06 16.03  9.10 2.90 3.90 1.99 13.86 14.56 8.25 16.77 2.03 15.54  IDRVE MiNDml1 MiNDmli 9.00 3.00 4.00 2.00 14.00 15.00 8.00 17.00 2.00 16.00  8.92 2.85 3.85 1.95 13.69 14.18 8.08 16.28 2.00 15.00  9.61 2.96 3.92 1.99 12.91 15.95 8.86 18.50 1.99 17.74  CD Hein Takens 9.59 9.56 2.98 3.08 3.76 3.75 1.97 1.98 11.92 11.95 15.74 15.69 8.92 8.87 18.13 18.08 2.00 1.99 15.04 15.00  8.95 3.55 3.90 1.95 8.10 2.65 9.10 10.90 2.00 3.70  kNNG1 9.20 2.77 3.94 1.83 14.08 10.11 6.55 15.00 1.84 37.63  kNNG2 9.87 2.44 3.94 1.83 14.08 10.11 6.55 15.00 1.84 37.63  Table 4: Dimensionality estimates for 10000 points.  IDMoM IDPWM IDGED -11.81 -3.10 -33.16 -15.58 35.44 -24.01 20.83 6.83 28.43 -23.90  -10.55 -1.03 -32.05 -14.57 43.00 -22.25 21.45 7.87 29.56 -23.10  -11.80 -3.06 -33.25 -16.34 39.79 -24.34 21.59 7.45 28.64 -24.52  IDRVE MiNDml1 MiNDmli -2.78 -11.88 0.00 -3.51 -25.00 -33.25 0.00 -15.90 60.71 35.65 -23.98 -17.00 25.00 20.92 11.76 6.88 0.00 28.50 -23.93 -19.69  -1.56 36.49 -23.47 34.17 115.49 -9.97 22.12 14.76 47.74 -16.52  CD -11.82 12.01 -22.13 14.21 86.53 -22.12 9.02 -2.99 41.21 -16.22  Hein Takens -12.10 23.15 -22.34 9.60 85.99 -22.62 8.07 -3.75 40.50 -16.27  -38.55 -18.31 -41.03 10.26 25.93 167.92 -64.29 -74.31 10.00 13.51  kNNG1 -62.17 16.97 -35.79 -44.81 93.68 157.17 338.17 -177.73 195.65 -84.45  kNNG2 -64.74 32.79 -35.79 -44.81 93.68 157.17 338.17 -177.73 195.65 -84.45  Table 5: Deviation of dimensionality estimates for 10000 manifold points with added noise (p=0.01).  Dataset  m1 m2 m3 m7 m8 m9  m10a m10c m11 m12  d D IDMLE -10.18 10 3 2.43 -30.57 4 -8.67 2 44.24 12 -21.77 20 21.46 10 24 8.04 32.16 2 20 -22.83  11 5 6 3 72 20 11 25 3 20  IDMoM IDPWM IDGED -11.92 -3.45 -33.16 -15.58 35.59 -24.01 20.83 6.83 28.43 -23.90  -10.66 -1.03 -32.05 -14.57 43.07 -22.25 21.45 7.87 29.56 -23.10  -11.91 -3.06 -33.25 -16.83 39.86 -24.27 21.59 7.51 28.64 -24.52  IDRVE MiNDml1 MiNDmli -2.78 -12.00 -3.51 0.00 -25.00 -33.25 0.00 -15.90 60.71 35.72 -17.33 -23.91 25.00 20.79 6.94 11.76 0.00 28.50 -23.93 -19.37  -1.87 37.16 -23.47 34.17 116.42 -10.22 21.78 14.49 46.73 -16.18  CD -17.05 18.83 -26.40 15.74 86.69 -22.31 3.04 -7.85 40.20 -16.16  Hein Takens -12.20 22.82 -22.07 11.62 86.16 -22.74 7.96 -3.53 39.00 -16.33  -63.69 -9.86 -42.31 7.69 46.30 132.08 -48.35 -59.17 37.50 33.78  kNNG1 -341.09 -7.94 -31.47 -38.25 9.52 15.73 25.65 -18.80 255.43 -174.25  kNNG2 -324.72 4.51 -31.47 -38.25 9.52 15.73 25.65 -18.80 255.43 -174.25  Table 6: Deviation of dimensionality estimates for 10000 manifold points with added noise (p=0.04).  Dataset  m1 m2 m3 m7 m8 m9  m10a m10c m11 m12  d D IDMLE -10.18 10 2.43 3 -30.83 4 2 -8.67 44.17 12 -21.77 20 21.46 10 8.04 24 31.66 2 20 -22.83  11 5 6 3 72 20 11 25 3 20  IDMoM IDPWM IDGED -11.81 -3.10 -33.42 -15.58 35.44 -24.01 20.83 6.89 28.43 -23.90  -10.66 -1.03 -32.05 -14.57 43.00 -22.25 21.45 7.93 29.06 -23.17  -11.80 -3.06 -33.25 -16.34 39.79 -24.27 21.59 7.51 28.64 -24.52  IDRVE MiNDml1 MiNDmli -2.78 -11.88 0.00 -3.51 -25.00 -33.25 -15.90 0.00 59.64 35.57 -17.00 -23.98 25.00 20.79 11.76 6.88 0.00 28.50 -23.93 -19.69  -1.77 37.16 -22.96 34.17 115.72 -9.66 21.22 14.43 46.73 -16.52  CD -16.95 19.48 -31.20 19.29 85.94 -22.12 9.02 -2.71 27.64 -16.16  Hein Takens -12.10 23.49 -22.34 15.15 85.65 -22.68 8.18 -3.42 39.00 -16.27  -35.75 -18.31 -35.90 18.46 -11.11 100.00 -39.56 -30.73 10.00 6.76  kNNG1 -37.61 -24.19 -35.03 4.37 -11.93 -907.22 34.35 -610.20 3811.41 -835.80  kNNG2 -41.84 -13.93 -35.03 4.37 -11.93 -907.22 34.35 -610.20 3811.41 -835.80  Table 7: Deviation of dimensionality estimates for 10000 manifold points with added noise (p=0.16).  PCA 11.00 3.00 5.30 3.00 24.00 20.00 10.00 24.00 3.00 20.00  PCA 11.00 3.00 5.05 3.00 24.00 20.00 10.00 24.00 3.00 20.00  PCA -22.73 -33.33 -60.40 -66.67 95.21 -31.75 10.00 4.17 -35.00 -26.00  PCA -23.18 -33.33 -60.40 -66.67 95.21 -31.75 10.00 4.17 -35.00 -26.00  PCA -22.73 -33.33 -60.40 -66.67 95.21 -31.75 10.00 4.17 -35.00 -26.00  36 (a) Illustration of the distribution of k-nearest neighbor distances for k  [1, 1000] with respect to 7 points of interest.  (b) Distribution of LID estimates based on k-nearest neighbor sets for k  [10, 1000] with respect to 7 points of interest.  Figure 4: Distribution of IDMLE estimates and distance values across neighborhoods around the points of interest.  signicantly aected by noise: their estimates diverge very quickly as noise is being introduced. On the other hand, the local estimators display more resistance to noise in the case of non-linear manifolds; among the local estimators, our EVT estimators tend to outperform the MiND variants.  We note that the additive noise considered in this experi- ment does not drastically impact the intrinsic dimensional- ity in the case of hypercubes. (sets m10a, m10b and m10c). That explains why PCA appears resistant to noise for the sets m10a, m10b and m10c.  6.3 Real Data  Based on our experiments on synthetic data, we expect the performance of our proposed estimators to be largely in agreement with one another. Accordingly, for clarity of presentation, for the experimentation on real data, we show results only for the MLE estimator.  Fig. 2 illustrates the distribution of LID estimates across reference points for all three data sets. The scatter plot for the ANN SIFT1B data set furthermore contains several points of interest annotated with their LID values, corre- sponding to objects of interest which we discuss later. First, we clearly observe dierences in the location of the distri- bution of LID values among the three data sets; for exam- ple, the mean value and standard deviation of the LID esti- mates for ALOI are considerably lower than those obtained for ANN SIFT1B. More specically, we observe mean val- ues of ALOI  2.2, MNIST  6.3, and ANN SIFT1B  12.3, with the corresponding standard deviations of ALOI  1.9, MNIST  2.7, and ANN SIFT1B  3.0. It should be noted that the measured ID within the neighborhoods that were tested is far smaller than the dimension of the full feature spaces. By plotting the same data as histograms in Fig. 3, we can furthermore see that the individual distributions of LID values dier in kurtosis and skewness as well.  The most striking dierence between the individual points of interest are the distances to their respective k-nearest neighbors. Fig. 4a displays for each point of interest the specic distribution of neighbor-distances for all values of k between 1 and 1000. Interestingly, the ID measured at the points of interest appears to be associated with other prop- erties of the respective objects. For example, distribution of neighbor-distances for objects with high corresponding di- mensionality (D, E and F ) indicate that these points are  in some sense outliers. On the other hand, despite their distance distributions being quite dissimilar, the LID values measured at A, B, and C are nearly identical.  7. CONCLUSION  Our experimental results on synthetic data show that the estimation of LID stabilizes for sample sizes on the order of 100. However, for Theorem 2 to be applicable, one must set a suciently small threshold on the lower tail of the distribution, which may severely limit the number of data objects falling within the tail. Although there is a conict between the accuracy of the estimator and the validity of the model, this conict is resolved as the size of the dataset scales upward; it is in precisely such situations where the applications of ID have the most impact.  Estimates of local ID constitute a measure of the complex- ity of data. Along with other indicators such as contrast [33], LID could give researchers and practitioners more insight into the nature of their data, and therefore help them im- prove the eciency and ecacy of their applications. As a tool for guiding learning processes, the proposed estima- tors could serve in many ways. Data collected during the retrieval processes could be automatically ltered out as noise, whenever they are associated with an unusually high ID value. In this way, the quality of query results may be enhanced as well.  The performance of content-based retrieval systems is usu- ally assessed in terms of the precision and recall of queries on a ground truth data set. However, in high-dimensional set- tings it is often the case that some points are much less likely to appear in a query result than others. Unlike LID, conven- tional measures of complexity or performance do not account for this diculty. LID has therefore the potential to aid in the design of fair benchmarks that truly reect the power of retrieval systems, according to a sound, mathematically- grounded procedure.  8. '"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= re.compile('(?<=INTRODUCTION)(.+)(?=ACKNOWLEDGMENTS)')\n",
    "p.search(re.sub('[\\s]',\" \",document)).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get CONCLUSION only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S  We present an algorithm for extracting a high-quality ap- proximation of the collection of FIs with probabilistic guar- antees. The algorithm employs progressive sampling with a stopping condition that relies on bounding the conditional Rademacher average of the problem using easy-to-compute characteristic quantities of the sample. The stopping con- dition can therefore be evaluated very eciently without the need to perform an expensive in-depth mining of the frequent itemsets in the sample at each step. To our knowl- edge this is the rst work that uses Rademacher averages in a knowledge discovery setting. The experimental results conrm that the algorithm is extremely successful at stop- ping fast at early iterations, and allows to extract very high- quality approximation of the collection of FIs. Among the possible directions for future work, it would be particularly interesting to better study the trade-o between the compu- tational complexity of the stopping condition and its ability to stop at small sample sizes. We are currently investigat- ing algorithms that give relative/multiplicative approxima- tion guarantees, and extensions of our work to additional signicance measures dierent from frequency [25, 26, 30].  7. '"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= re.compile('(?<=CONCLUSION)(.+)(?=ACKNOWLEDGMENTS)')\n",
    "p.search(re.sub('[\\s]',\" \",document)).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get References only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  [1] R. Agrawal and R. Srikant. Fast algorithms for mining  association rules in large databases. VLDB 94.  [2] R. Agrawal, T. Imieliski, and A. Swami. Mining asso- ciation rules between sets of items in large databases. SIGMOD Rec., 22:207216, June 1993.  [3] B. Gu, B. Liu, F. Hu, and H. Liu. Eciently determin- ing the starting sample size for progressive sampling. ECML01.  [4] P. L. Bartlett, S. Boucheron, and G. Lugosi. Model selection and error estimation. Mach. Learn., 48:85 113, 2002.  [5] S. Boucheron, O. Bousquet, and G. Lugosi. Theory of classication : A survey of some recent advances. ESAIM: Probability and Statistics, 9:323375, 2005.  [6] V. T. Chakaravarthy, V. Pandit, and Y. Sabharwal. Analysis of sampling techniques for association rule mining. ICDT09.  [7] B. Chen, P. Haas, and P. Scheuermann. A new two- phase sampling based algorithm for discovering associ- ation rules. KDD02.  [8] K.-T. Chuang, M.-S. Chen, and W.-C. Yang. Progres- sive sampling for association rules based on sampling error estimation. PAKDD05.  [9] T. Elomaa and M. Kriinen. Progressive Rademacher  sampling. AAAI02.  [10] B. Goethals and M. J. Zaki. Advances in frequent itemset mining implementations: report on FIMI03. SIGKDD Explor. Newsl., 6(1):109117, June 2004.  [11] G. Grahne and J. Zhu. Eciently using prex-trees in  mining frequent itemsets. FIMI03.  [12] J. Han, J. Pei, and Y. Yin. Mining frequent patterns  without candidate generation. SIGMOD00.  [13] J. Han, H. Cheng, D. Xin, and X. Yan. Frequent pat- tern mining: current status and future directions. Data Mining and Knowl. Disc. , 15:5586, 2007.  [14] G. H. John and P. Langley. Static versus dynamic sam-  pling for data mining. KDD96.  [15] S. G. Johnson. The NLopt nonlinear-optimization pack-  age. URL http://ab-initio.mit.edu/nlopt.  [16] V. Koltchinskii. Rademacher penalties and structural IEEE Trans. Inf. Theory, 47(5):  risk minimization. 19021914, July 2001.  [17] E. Liberty, M. Mitzenmacher, J. Thaler, and J. Ullman. Space lower bounds for itemset frequency sketches. CoRR, 1407.3740, July 2014.  [18] S. Parthasarathy. Ecient progressive sampling for as-  sociation rules. ICDM02.  [19] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Dis- covering frequent closed itemsets for association rules. ICDT99.  [20] A. Pietracaprina, M. Riondato, E. Upfal,  and F. Vandin. Mining top-k frequent itemsets through pro- gressive sampling. Data Mining Knowl. Disc., 21(2): 310326, 2010.  [21] F. Provost, D. Jensen, and T. Oates. Ecient progres-  sive sampling. KDD99.  [22] M. Riondato, J. A. DeBrabant, R. Fonseca, and E. Up- fal. PARMA: A parallel randomized algorithm for as- sociation rules mining in MapReduce. CIKM12.  [23] M. Riondato and E. Upfal. Mining frequent itemsets through progressive sampling with Rademacher aver- ages. Extended Version. URL http://cs.brown.edu/ %7Ematteo/papers/progrsamplfi-ext.pdf.  [24] M. Riondato and E. Upfal. Ecient discovery of associ- ation rules and frequent itemsets through sampling with tight performance guarantees. ACM Trans. Knowl. Disc. from Data, 8(2), 2014.  [25] M. Riondato and F. Vandin. Finding the true frequent  [29] V. N. Vapnik. The Nature of Statistical Learning The- ory. Statistics for engineering and information science. Springer-Verlag, New York, NY, USA, 1999.  [30] G. I. Webb. Discovering signicant patterns. Mach.  Learn., 68(1):133, 2007.  itemsets. SDM14.  [26] T. Scheer and S. Wrobel. Finding the most interest- ing patterns in a database quickly by using sequential sampling. J. Mach. Learn. Res., 3:833862, Dec. 2002. [27] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cam- bridge University Press, 2014.  [28] H. Toivonen. Sampling large databases for association  rules. VLDB96.  0.0E+02.0E+64.0E+600.020.040.060.08kosarakVCThis worksample sizeepsilon0.0E+02.0E+64.0E+600.010.020.030.04accidentsVCThis worksample sizeepsilon1014 '"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= re.compile('(?<=REFERENCES)(.+)')\n",
    "p.search(re.sub('[\\s]',\" \",document)).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to count the number of references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "create a match of all references by counting the number of integers enclosed by brackets (i.e. '[2]').  \n",
    "\n",
    "This is how references are labeled in the research papers\n",
    "'''\n",
    "\n",
    "len(re.findall('\\[(.*?)\\]',regexp.search(re.sub('[\\s]',\" \",document)).group(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Database Management']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\[(.*?)\\]',catsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
