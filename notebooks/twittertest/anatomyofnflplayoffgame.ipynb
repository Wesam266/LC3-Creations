{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Information\n",
    "###############################################################################\n",
    "# Created by Linwood Creekmore \n",
    "\n",
    "# https://github.com/linwoodc3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Admin work; creating a normalized path to work on any OS for calls to keys or files\n",
    "###############################################################################\n",
    "import geohash\n",
    "import os\n",
    "path = os.path.normpath(os.path.join(os.path.normpath(os.path.expanduser(\"~\")),\"projects\",\"LC3-Creations\"))\n",
    "\n",
    "#**********************************************************************\n",
    "# This block of imports eliminates an ipython error associated with\n",
    "# 'import sys' where the print statement goes to the console\n",
    "# and not the ipython notebook.  For more information on this workaround,\n",
    "# see https://github.com/ipython/ipython/issues/8354\n",
    "#**********************************************************************\n",
    "\n",
    "import sys\n",
    "default_stdout = sys.stdout\n",
    "default_stderr = sys.stderr\n",
    "sys.path.append(os.path.join(path,\"timehash\"))\n",
    "import timehash\n",
    "reload(sys)\n",
    "sys.stdout = default_stdout\n",
    "sys.stderr = default_stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Class to load json stored in .txt files from disc into python json/dicts for analysis\n",
    "#**********************************************************************\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "#shameless copy paste from json/decoder.py\n",
    "FLAGS = re.VERBOSE | re.MULTILINE | re.DOTALL\n",
    "WHITESPACE = re.compile(r'[ \\t\\n\\r]*', FLAGS)\n",
    "\n",
    "class ConcatJSONDecoder(json.JSONDecoder):\n",
    "    def decode(self, s, _w=WHITESPACE.match):\n",
    "        s_len = len(s)\n",
    "\n",
    "        objs = []\n",
    "        end = 0\n",
    "        while end != s_len:\n",
    "            obj, end = self.raw_decode(s, idx=_w(s, end).end())\n",
    "            end = _w(s, end).end()\n",
    "            objs.append(obj)\n",
    "        return objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Turn Twitter hashtags into a string for NLP or analysis\n",
    "#**********************************************************************\n",
    "\n",
    "def hashtag_getter(tweet):\n",
    "    try:\n",
    "        # do a test to see if the length of the entities is greater than zero, if not skip\n",
    "        if len([l['text'] for l in tweet['entities']['hashtags']])>0:\n",
    "            \n",
    "            # join all hashtags into a list, split by \",\" and whitespace\n",
    "            hashtags = [(\", \".join([l['text'] for l in tweet['entities']['hashtags']]))]\n",
    "            hashtags_string = \", \".join(hashtags)\n",
    "        else:\n",
    "            hashtags = \"\"\n",
    "            hashtags_string = \"\"\n",
    "            \n",
    "    # if we don't have hashtags, this exception prevents an error hangup   \n",
    "    except:\n",
    "        pass\n",
    "    return hashtags,hashtags_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Nunn, Transportation, VeteranJob, Job, Jobs, Hiring, CareerArc'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_getter(nflgame[4])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# IMPORTANT Admin step; getting Twitter API keys; DO THIS!!!!!\n",
    "#**********************************************************************\n",
    "'''\n",
    "Before the next step, follow the instructions at this link, Section 1 only\n",
    "--> http://socialmedia-class.org/twittertutorial.html\n",
    "\n",
    "Copy your four (4) keys to a .txt file or wordpad (somewhere you can copy and paste easily)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# IMPORTANT Admin step; Obfuscating your keys...example you can reuse\n",
    "#**********************************************************************\n",
    "\n",
    "'''\n",
    "A complaint I have with data scientists, computer scientists, data analysts\n",
    "or people who do this programming in general, is their inability to explain things.\n",
    "Like any field, they assume everyone is \"on their level\". \n",
    "\n",
    "One thing they told me was, \"Hide your api keys!\"  But no one ever showed me how.\n",
    "By how, I mean a way to hide them but not have to always cut and paste.\n",
    "It's insane!  I had to reinvent the wheel myself because out of all my googling,\n",
    "I never found a good example.  I'm solving that problem for you. This is one \n",
    "clear example you can reuse anywhere and anytime you need to add an api to your\n",
    "code.\n",
    "\n",
    "This block of code below (under this rant) requires you to cut and paste\n",
    "your keys into a nested dictionary.  We make simple key:value pairs for our api key\n",
    "combinations for every service.  Then, we convert our nested dictionary to a .json\n",
    "file using \"json.dumps\".  Finally, we write that json file to disc as a .txt file, \n",
    "with the intention of NEVER adding that file to git.  You could store EVERY api key\n",
    "or consumer secret or consumer key for ANY service in this file. \n",
    "You will NEVER cut and paste a key into your notebooks or code again, but just \n",
    "load the .txt file and retrieve the key:value pair you need.  Make sure to clear your\n",
    "cut and pasted key from the code/notebook after writing the .txt to disc!!!\n",
    "I explain how to add new key:value pairs below as well. \n",
    "'''\n",
    "\n",
    "#**********************************************************************\n",
    "# Here is the code; remove multiline comment quotes to use\n",
    "#**********************************************************************\n",
    "\n",
    "\n",
    "'''\n",
    "apikeys = {}\n",
    "apikeys['service']= {\"keyname\": \"key\"}\n",
    "\n",
    "import json\n",
    "\n",
    "# writing to directory two steps above current; in this case, to the repository's base directory\n",
    "with open('../../apikeys.txt', 'w+') as outfile:\n",
    "    json.dump(apikeys, outfile)\n",
    "outfile.closed\n",
    "\n",
    "#**********************************************************************\n",
    "# Have new keys to add? Easy, just follow this process\n",
    "# To add new keys to your file, \n",
    "# just open the .txt in a text editor like notepad, wordpad, sublime, etc\n",
    "# And the new key manually using this example below\n",
    "# anything with \"new\" below would just be added in manually\n",
    "#**********************************************************************\n",
    "\n",
    "apikeys = {\"newservice\": {\"newconsumerkey\": \"newkey\", \"newconsumersecret\": \"newkey\"},\"existingservice\":{\"consumerkey\":\"key\", \"consumersecret\":\"key}}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# How to load your obfuscated keys from a local \".txt\" file\n",
    "# The text file is from the json file you stored above\n",
    "#**********************************************************************\n",
    "\n",
    "# I created a nested dictionary with my API keys, then wrote that json to disk.  Now, I load the json and just pass the keys into the application\n",
    "\n",
    "oauth = json.load(open(os.path.join(path,\"apikeys.txt\")), cls=ConcatJSONDecoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# How to retrieve your key for a specific service\n",
    "#**********************************************************************\n",
    "\n",
    "# If you have NOT created the apikeys json file and saved to disc, this will error\n",
    "oauth[0]['openmapquest']['consumerkey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Passing in oauth values; you obviscate by loading your locally stored json\n",
    "#**********************************************************************\n",
    "\n",
    "# Import the necessary package to process data in JSON format\n",
    "try:\n",
    "    import json\n",
    "except ImportError:\n",
    "    import simplejson as json\n",
    "\n",
    "# Import the necessary methods from \"twitter\" library\n",
    "from twitter import Twitter, OAuth, TwitterHTTPError, TwitterStream\n",
    "\n",
    "# Variables that contains the user credentials to access Twitter API \n",
    "# If you have not followed the obfuscate process above, this will be empty and error out\n",
    "ACCESS_TOKEN = oauth[0]['twitter']['accesstoken']\n",
    "ACCESS_SECRET = oauth[0]['twitter']['accesssecret']\n",
    "CONSUMER_KEY = oauth[0]['twitter']['consumerkey']\n",
    "CONSUMER_SECRET = oauth[0]['twitter']['consumersecret']\n",
    "\n",
    "oauth = OAuth(ACCESS_TOKEN, ACCESS_SECRET, CONSUMER_KEY, CONSUMER_SECRET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Code to pull tweets from twitter stream\n",
    "\n",
    "# I pulled this code from http://socialmedia-class.org/twittertutorial.html.  \n",
    "# I used the locations filter, but you can alter to get a sample or pull \n",
    "# specific keywords.  Use the link above.  The only line you would alter is:\n",
    "# iterator = twitter_stream.statuses.filter()\n",
    "# Want to know what arguments to pass in? visit https://dev.twitter.com/streaming/reference/post/statuses/filter\n",
    "#**********************************************************************\n",
    "\n",
    "\n",
    "# Initiate the connection to Twitter Streaming API\n",
    "twitter_stream = TwitterStream(auth=oauth)\n",
    "\n",
    "# Filter the public data following through Twitter; the format is \"long,lat , long,lat\" with southwest corner first\n",
    "iterator = twitter_stream.statuses.filter(locations = '-105.024513,39.741353, -105.014846,39.747408 ')\n",
    "\n",
    "# Print each tweet in the stream to the screen \n",
    "# Here we set it to stop after getting 1000 tweets. \n",
    "# You don't have to set it to stop, but can continue running \n",
    "# the Twitter API to collect data for days or even longer. \n",
    "\n",
    "with open('tweetstream.txt', 'w+') as outfile:\n",
    "    for tweet in iterator:\n",
    "        \n",
    "        # Twitter Python Tool wraps the data returned by Twitter \n",
    "        # as a TwitterDictResponse object.\n",
    "        # We convert it back to the JSON format to print/score\n",
    "        #print json.dumps(tweet)  \n",
    "\n",
    "        # The command below will do pretty printing for JSON data, try it out\n",
    "        # print json.dumps(tweet, indent=4)\n",
    "        json.dump(tweet, outfile)\n",
    "    outfile.closed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# After the tweet streaming data collection is complete, this is how to load the file\n",
    "#**********************************************************************\n",
    "\n",
    "nflgame = json.load(open(os.path.join(path,\"notebooks\",\"twittertest\",\"pats_vs_broncos.txt\")), cls=ConcatJSONDecoder)\n",
    "nflgame2 = json.load(open(os.path.join(path,\"notebooks\",\"twittertest\",\"chiefs_vs_pats.txt\")), cls=ConcatJSONDecoder)\n",
    "nflgame3 = json.load(open(os.path.join(path,\"notebooks\",\"twittertest\",\"skins_vs_pack.txt\")), cls=ConcatJSONDecoder)\n",
    "nflgame4 = [json.loads(l) for l in json.load(open(os.path.join(path,\"notebooks\",\"twittertest\",\"viks_vs_hawks.txt\")), cls=ConcatJSONDecoder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1469"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#**********************************************************************\n",
    "# Check for the number of tweets you collected\n",
    "#**********************************************************************\n",
    "\n",
    "len(nflgame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#**********************************************************************\n",
    "# Code to extract values from json tweet and write to csv\n",
    "# http://blog.appliedinformaticsinc.com/how-to-parse-and-convert-json-to-csv-using-python/\n",
    "#**********************************************************************\n",
    "import csv\n",
    "import datetime\n",
    "import re\n",
    "import collections\n",
    "\n",
    "row = collections.OrderedDict({\"tweetdetails\":{}})\n",
    "polygons = 0\n",
    "points = 0\n",
    "game_data = row['tweetdetails']\n",
    "\n",
    "# create a folder to store data\n",
    "if not os.path.exists('./output'):\n",
    "    os.makedirs('./output')\n",
    "\n",
    "# open a file for writing\n",
    "playoffs = open('./output/playoffs.csv', 'w+')\n",
    "\n",
    "# create the csv writer object\n",
    "csvwriter = csv.writer(playoffs)\n",
    "count = 0\n",
    "passed = 0\n",
    "for l in nflgame:\n",
    "    try:\n",
    "        row['tweetdetails']['message_id']= str(l['id'])\n",
    "        row['tweetdetails']['epochtime']= str(float(l['timestamp_ms'])/float(1000))\n",
    "        row['tweetdetails']['timehash']= timehash.encode(int(l['timestamp_ms'])/1000)\n",
    "        row['tweetdetails']['polygon']=[(m[0],m[1]) for m in (l['place']['bounding_box']['coordinates'][0])]\n",
    "        row['tweetdetails']['screen_name']= str(l['user']['screen_name'])\n",
    "        row['tweetdetails']['user_id']= l['user']['id_str']\n",
    "        row['tweetdetails']['tweet']= re.sub(\"[;,]\",\"\",str(l['text'])) # replacing characters that could delimit text\n",
    "        row['tweetdetails']['hashtags']= hashtag_getter(l)[1]\n",
    "        row['tweetdetails']['time'] = datetime.datetime.fromtimestamp((int(l['timestamp_ms'])/1000)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        row['tweetdetails']['geohash']= str(int(geohash.encode(l['coordinates']['coordinates'][1],l['coordinates']['coordinates'][0])))\n",
    "        row['tweetdetails']['usermentions']=(\", \".join(map(str,[(m['screen_name'],m['id_str']) for m in l['entities']['user_mentions']])))\n",
    "        polygons += 1\n",
    "        try:\n",
    "            if l['geo']['type'] == 'Point':\n",
    "                row['tweetdetails']['Latitude']=l['geo']['coordinates'][0]\n",
    "                row['tweetdetails']['Longitude']=l['geo']['coordinates'][1]\n",
    "                points += 1\n",
    "                \n",
    "        except:\n",
    "            row['tweetdetails']['Latitude']=\"\"\n",
    "            row['tweetdetails']['Longitude']=\"\"\n",
    "            \n",
    "        \n",
    "        \n",
    "        header = row['tweetdetails'].keys()\n",
    "        \n",
    "        if count == 0:\n",
    "            csvwriter.writerow(header)\n",
    "            count += 1\n",
    "        csvwriter.writerow(row['tweetdetails'].values())\n",
    "            \n",
    "    except:\n",
    "        passed += 1\n",
    "        \n",
    "        \n",
    "playoffs.close() \n",
    "print passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'QgsPoint(-117.523867, 33.85216)', u'QgsPoint(-117.523867, 39.79369)', u'QgsPoint(-84.097028, 39.79369)', u'QgsPoint(-84.097028, 33.85216)']]\n"
     ]
    }
   ],
   "source": [
    "#**********************************************************************\n",
    "# Retrieve polygon points from tweet \"place\", \"bounding_box\" tag\n",
    "# return in format well known text, qgis, or plain points\n",
    "#**********************************************************************\n",
    "\n",
    "\n",
    "for l in nflgame[18:19]:\n",
    "    tweetpoly = [(m[0],m[1]) for m in (l['place']['bounding_box']['coordinates'][0])]\n",
    "    polypoints = \", \".join(map(str,[(m[0],m[1]) for m in (l['place']['bounding_box']['coordinates'][0])]))\n",
    "    tweetpoly_qgisformat = \",\".join(map(str,[[[ u\"QgsPoint\"+str(l) for l in tweetpoly]]]))\n",
    "    tweetpoly_wkt = \n",
    "    print tweetpoly_qgisformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "(u'Gothbrookz', u'37138910')\n",
      "\n",
      "(u'newsmanone', u'16336228')\n",
      "(u'NBC10', u'16521206')\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#**********************************************************************\n",
    "# Retrieve user mentions to build graph of relationships\n",
    "#**********************************************************************\n",
    "\n",
    "\n",
    "for l in nflgame[15:24]:\n",
    "    try:\n",
    "        usermentions = \", \".join(map(str,[(m['screen_name'],m['id_str']) for m in l['entities']['user_mentions']]))\n",
    "        \n",
    "    \n",
    "    except:\n",
    "        usermentions = \"\"\n",
    "        entry = \"\"\n",
    "    \n",
    "    print usermentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'contributors': None,\n",
       " u'coordinates': None,\n",
       " u'created_at': u'Sat Jan 16 21:33:54 +0000 2016',\n",
       " u'entities': {u'hashtags': [],\n",
       "  u'symbols': [],\n",
       "  u'urls': [],\n",
       "  u'user_mentions': [{u'id': 240833391,\n",
       "    u'id_str': u'240833391',\n",
       "    u'indices': [0, 13],\n",
       "    u'name': u'Travis Vilandre',\n",
       "    u'screen_name': u'DirtyHolland'},\n",
       "   {u'id': 59475498,\n",
       "    u'id_str': u'59475498',\n",
       "    u'indices': [14, 27],\n",
       "    u'name': u'Andy Gresh',\n",
       "    u'screen_name': u'TheRealGresh'},\n",
       "   {u'id': 78350811,\n",
       "    u'id_str': u'78350811',\n",
       "    u'indices': [28, 39],\n",
       "    u'name': u'Scott Zolak',\n",
       "    u'screen_name': u'scottzolak'},\n",
       "   {u'id': 19541697,\n",
       "    u'id_str': u'19541697',\n",
       "    u'indices': [40, 49],\n",
       "    u'name': u'CBS Radio',\n",
       "    u'screen_name': u'CBSRadio'}]},\n",
       " u'favorite_count': 0,\n",
       " u'favorited': False,\n",
       " u'filter_level': u'low',\n",
       " u'geo': None,\n",
       " u'id': 688474279508312065,\n",
       " u'id_str': u'688474279508312065',\n",
       " u'in_reply_to_screen_name': u'DirtyHolland',\n",
       " u'in_reply_to_status_id': 688470789415391232,\n",
       " u'in_reply_to_status_id_str': u'688470789415391232',\n",
       " u'in_reply_to_user_id': 240833391,\n",
       " u'in_reply_to_user_id_str': u'240833391',\n",
       " u'is_quote_status': False,\n",
       " u'lang': u'en',\n",
       " u'place': {u'attributes': {},\n",
       "  u'bounding_box': {u'coordinates': [[[-73.508143, 41.187054],\n",
       "     [-73.508143, 42.886811],\n",
       "     [-69.858861, 42.886811],\n",
       "     [-69.858861, 41.187054]]],\n",
       "   u'type': u'Polygon'},\n",
       "  u'country': u'United States',\n",
       "  u'country_code': u'US',\n",
       "  u'full_name': u'Massachusetts, USA',\n",
       "  u'id': u'cd450c94084cbf9b',\n",
       "  u'name': u'Massachusetts',\n",
       "  u'place_type': u'admin',\n",
       "  u'url': u'https://api.twitter.com/1.1/geo/id/cd450c94084cbf9b.json'},\n",
       " u'retweet_count': 0,\n",
       " u'retweeted': False,\n",
       " u'source': u'<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>',\n",
       " u'text': u'@DirtyHolland @TheRealGresh @scottzolak @CBSRadio me too!',\n",
       " u'timestamp_ms': u'1452980034125',\n",
       " u'truncated': False,\n",
       " u'user': {u'contributors_enabled': False,\n",
       "  u'created_at': u'Sat Feb 05 19:15:19 +0000 2011',\n",
       "  u'default_profile': True,\n",
       "  u'default_profile_image': False,\n",
       "  u'description': None,\n",
       "  u'favourites_count': 197,\n",
       "  u'follow_request_sent': None,\n",
       "  u'followers_count': 41,\n",
       "  u'following': None,\n",
       "  u'friends_count': 259,\n",
       "  u'geo_enabled': True,\n",
       "  u'id': 247874205,\n",
       "  u'id_str': u'247874205',\n",
       "  u'is_translator': False,\n",
       "  u'lang': u'en',\n",
       "  u'listed_count': 0,\n",
       "  u'location': u'Wales',\n",
       "  u'name': u'Jake Vilandre',\n",
       "  u'notifications': None,\n",
       "  u'profile_background_color': u'C0DEED',\n",
       "  u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  u'profile_background_tile': False,\n",
       "  u'profile_banner_url': u'https://pbs.twimg.com/profile_banners/247874205/1377949683',\n",
       "  u'profile_image_url': u'http://pbs.twimg.com/profile_images/683637510254280704/FxIij8EJ_normal.jpg',\n",
       "  u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/683637510254280704/FxIij8EJ_normal.jpg',\n",
       "  u'profile_link_color': u'0084B4',\n",
       "  u'profile_sidebar_border_color': u'C0DEED',\n",
       "  u'profile_sidebar_fill_color': u'DDEEF6',\n",
       "  u'profile_text_color': u'333333',\n",
       "  u'profile_use_background_image': True,\n",
       "  u'protected': False,\n",
       "  u'screen_name': u'JVilandre',\n",
       "  u'statuses_count': 2758,\n",
       "  u'time_zone': None,\n",
       "  u'url': None,\n",
       "  u'utc_offset': None,\n",
       "  u'verified': False}}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nflgame2[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1469"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'polygon',\n",
       " 'tweet',\n",
       " 'hashtags',\n",
       " 'epochtime',\n",
       " 'timehash',\n",
       " 'geohash',\n",
       " 'Longitude',\n",
       " 'Latitude',\n",
       " 'message_id',\n",
       " 'screen_name']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['tweetdetails'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "game_data = row['tweetdetails']\n",
    "\n",
    "# create a folder to store data\n",
    "if not os.path.exists('./output'):\n",
    "    os.makedirs('./output')\n",
    "\n",
    "# open a file for writing\n",
    "playoffs = open('./output/playoffs.csv', 'w+')\n",
    "\n",
    "# create the csv writer object\n",
    "csvwriter = csv.writer(playoffs)\n",
    "\n",
    "count = 0\n",
    "header = row['tweetdetails'].keys()\n",
    "csvwriter.writerow(header)\n",
    "csvwriter.writerow(row['tweetdetails'].values())\n",
    "playoffs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Nunn, Transportation, VeteranJob, Job, Jobs, Hiring, CareerArc']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_getter(nflgame[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(float(l['timestamp_ms'])/float(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(u'107886768', u'barb_hill'),\n",
       " (u'1254995785', u'DCBarno'),\n",
       " (u'131951051', u'rian5ca'),\n",
       " (u'1356310075', u'clozoya13'),\n",
       " (u'141302910', u'Kindred_Jobs'),\n",
       " (u'15999904', u'WilmingtonWX'),\n",
       " (u'1948302668', u'littlehotmess77'),\n",
       " (u'1950302455', u'Lorenzo_1599'),\n",
       " (u'2587789764', u'WorkWithSHC'),\n",
       " (u'2706556174', u'Crp94'),\n",
       " (u'27585679', u'gerrypizza'),\n",
       " (u'3345561723', u'ACM_Nicky'),\n",
       " (u'33978500', u'AaronMatas'),\n",
       " (u'356840344', u'PracticeWithUs'),\n",
       " (u'365012829', u'PTK473'),\n",
       " (u'546225760', u'kmgcareers'),\n",
       " (u'61101107', u'CurtNickisch'),\n",
       " (u'911570634', u'Jason6440')}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#**********************************************************************\n",
    "# Testing for common users at games\n",
    "#**********************************************************************\n",
    "\n",
    "users3 = []\n",
    "for l in nflgame4:\n",
    "    try:\n",
    "        users3.append((l['user']['id_str'],l['user']['screen_name']))\n",
    "    except:\n",
    "        pass\n",
    "print len(users3)\n",
    "\n",
    "users1 = [(l['user']['id_str'],l['user']['screen_name']) for l in nflgame]\n",
    "users2 = [(l['user']['id_str'],l['user']['screen_name']) for l in nflgame2]\n",
    "users4 = [(l['user']['id_str'],l['user']['screen_name']) for l in nflgame3]\n",
    "\n",
    "a =set(users1) & set(users2)\n",
    "b = set(users2) & set(users4)\n",
    "c = set(users1) & set(users4)\n",
    "d = set(users3) & set(users4)\n",
    "e = set(users3) & set(users2)\n",
    "f = set(users3) & set(users1)\n",
    "\n",
    "\n",
    "#**********************************************************************\n",
    "# Super set of profiles who participated in at least two events\n",
    "#**********************************************************************\n",
    "((((a.union(b)).union(c)).union(d)).union(e)).union(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nflgame[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Geo point locations of tweets; store separately\n",
    "#**********************************************************************\n",
    "\n",
    "count = 0\n",
    "for l in nflgame:\n",
    "    try:\n",
    "        if l['geo']['type'] == 'Point':\n",
    "            print l['geo']['coordinates']\n",
    "            count += 1\n",
    "            print count\n",
    "    except:\n",
    "        pass\n",
    "print float(count)/float(len(nflgame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count =0\n",
    "for l in nflgame[1:100]:\n",
    "    try:\n",
    "        print l['geo']\n",
    "        count += 1\n",
    "        print count\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "googlegeolocator.reverse([nflgame[4]['coordinates']['coordinates'][1],nflgame[4]['coordinates']['coordinates'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "osmgeolocator.reverse((nflgame[111]['coordinates']['coordinates'][1],nflgame[111]['coordinates']['coordinates'][0]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use regex to return only text seperated by , (city, state) or text  by itself; ignores symbols\n",
    "\n",
    "import re\n",
    "\n",
    "for l in nflgame[80:115]:\n",
    "    try:\n",
    "        print re.search('[ ]?[A-Za-z]+[ ]?([A-Za-z]+)?[ ]?([A-Za-z]+)?(,)?[ ]?([A-Za-z]+)?((\\.)[^ ])?([A-Za-z]+)?((\\.)[^ ])?',l['user']['location']).group(0).strip()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "datetime.datetime.fromtimestamp((int(nflgame[20]['timestamp_ms'])/1000)).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use regex to return only text seperated by , (city, state) or text  by itself; ignores symbols\n",
    "\n",
    "import re\n",
    "\n",
    "for l in nflgame[1800:2000]:\n",
    "    try:\n",
    "        print l['text']\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Nunn, Transportation, VeteranJob, Job, Jobs, Hiring, CareerArc']\n",
      "[u'mountmorrisonsummit, mountainevans']\n",
      "[u'broncos']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop over the original json\n",
    "row = {\"tweetdetails\":{}}\n",
    "hashtags=[]\n",
    "for h in nflgame[1:10]:\n",
    "    try:\n",
    "        # do a test to see if the length of the entities is greater than zero, if not skip\n",
    "        if len(\", \".join([l['text'] for l in h['entities']['hashtags']])) >0:\n",
    "\n",
    "            # print the hashtags as a string; each hashtag is split by a space\n",
    "            print [(\", \".join([l['text'] for l in h['entities']['hashtags']]))]         \n",
    "            \n",
    "    # if we don't have hashtags, this exception prevents an error hangup   \n",
    "    except:\n",
    "        pass\n",
    "hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Extract home locations of user as string, geocode, cache name and latitude\n",
    "# and then store latitude and longitude of event for csv writing\n",
    "#**********************************************************************\n",
    "\n",
    "\n",
    "\n",
    "def geocoding(tweetlocation):\n",
    "    test = {\"cache_details\":{}}\n",
    "    try:\n",
    "        from geopy.geocoders import GoogleV3, Bing, GeoNames, Nominatim\n",
    "        google= GoogleV3(api_key=oauth[0]['google']['serverkey'])\n",
    "        bing= Bing(api_key=oauth[0]['bing']['key'])\n",
    "        geonames = GeoNames(username = oauth[0]['geonames']['username'])\n",
    "        osm = Nominatim()\n",
    "    except:\n",
    "        print \"You need to install the \\'geopy\\' module or edit input parameters\"\n",
    "        \n",
    "    try:\n",
    "        import re\n",
    "    except:\n",
    "        print \"You need to install the \\'re\\' module\"\n",
    "        \n",
    "    try: \n",
    "        import time\n",
    "    except:\n",
    "        print \"You need to install the \\'time\\' module\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    locations =[]\n",
    "    for l in nflgame[18:35]:\n",
    "        try:\n",
    "            locations.append(re.search('[ ]?[A-Za-z]+[ ]?([A-Za-z]+)?[ ]?([A-Za-z]+)?(,)?[ ]?([A-Za-z]+)?((\\.)[^ ])?([A-Za-z]+)?((\\.)[^ ])?',l['user']['location']).group(0).strip())\n",
    "        except:\n",
    "            pass\n",
    "    test['cache_details']['locations']=locations\n",
    "    for l in locations:\n",
    "        if l in cachedlocs[0].keys():\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                answer = geonames.geocode(l)\n",
    "                cachedlocs.append({l:{\"realname\":answer[0],\"latitude\":answer[1][0],\"longitude\":answer[1][1]}})\n",
    "                \n",
    "                time.sleep(1)\n",
    "            except: \n",
    "                pass\n",
    "    return cachedlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc = nflgame[1:20]\n",
    "test = {\"cache_details\":{}}\n",
    "try:\n",
    "    from geopy.geocoders import GoogleV3, Bing, GeoNames, Nominatim\n",
    "    google= GoogleV3(api_key=oauth[0]['google']['serverkey'])\n",
    "    bing= Bing(api_key=oauth[0]['bing']['key'])\n",
    "    geonames = GeoNames(username = oauth[0]['geonames']['username'])\n",
    "    osm = Nominatim()\n",
    "except:\n",
    "    print \"You need to install the \\'geopy\\' module or edit input parameters\"\n",
    "\n",
    "try:\n",
    "    import re\n",
    "except:\n",
    "    print \"You need to install the \\'re\\' module\"\n",
    "\n",
    "try: \n",
    "    import time\n",
    "except:\n",
    "    print \"You need to install the \\'time\\' module\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for l in loc:\n",
    "    try:\n",
    "        \n",
    "        m = locations.append(re.search('[ ]?[A-Za-z]+[ ]?([A-Za-z]+)?[ ]?([A-Za-z]+)?(,)?[ ]?([A-Za-z]+)?((\\.)[^ ])?([A-Za-z]+)?((\\.)[^ ])?',l['user']['location']).group(0).strip())\n",
    "         \n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "test['cache_details']['locations']=locations\n",
    "test['cache_details']['cache']={}\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Riverside, CA'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nflgame[18]['user']['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['cache_details']['locations'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from geopy.geocoders import GoogleV3, Bing, GeoNames,Nominatim\n",
    "\n",
    "google= GoogleV3(api_key=oauth[0]['google']['serverkey'])\n",
    "bing= Bing(api_key=oauth[0]['bing']['key'])\n",
    "geonames = GeoNames(username='linwoodc3')\n",
    "osm = Nominatim()\n",
    "#geonames.geocode('kansas city metro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = google.geocode('Riverside, CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'Riverside, CA, USA', (33.9533487, -117.3961564))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0],b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name = str(nflgame[18]['user']['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test['cache_details']['cache'][name] = {\"Name\":b[0],\"Point\":b[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-47aa38edee87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cache_details'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'locations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test['cache_details']['locations']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Extract home locations of user as string, geocode, cache name and latitude\n",
    "# and then store latitude and longitude of event for csv writing\n",
    "#**********************************************************************\n",
    "\n",
    "\n",
    "for l in nflgame[20:550]:\n",
    "    try:  \n",
    "        m = re.search('[ ]?[A-Za-z]+[ ]?([A-Za-z]+)?[ ]?([A-Za-z]+)?(,)?[ ]?([A-Za-z]+)?((\\.)[^ ])?([A-Za-z]+)?((\\.)[^ ])?',l['user']['location']).group(0).strip()\n",
    "        \n",
    "        if m.lower() in test['cache_details']['locations'].lower():\n",
    "            print \"It's there!!\"\n",
    "            pass\n",
    "        else:\n",
    "            test['cache_details']['locations'].append(m)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'providence, ri',\n",
       " u'adelaide, australia',\n",
       " u'california',\n",
       " u'denver, co',\n",
       " u'hq aspen beverly hills',\n",
       " u'islamic republic of',\n",
       " u'chicago, il',\n",
       " u'battle creek, mi',\n",
       " u'charlotte, nc',\n",
       " u'the internet',\n",
       " u'colorado, usa',\n",
       " u'houston, tx',\n",
       " u'the',\n",
       " u'colorado springs, co',\n",
       " u'coloroado',\n",
       " u'riverside, ca',\n",
       " u'mars',\n",
       " u'instagram',\n",
       " u'canto seven, ca',\n",
       " u'lamar, co',\n",
       " u'staywithme',\n",
       " u'aspen, co',\n",
       " u'littleton, co',\n",
       " u'denver',\n",
       " u'colorado, ny',\n",
       " u'colorado, with',\n",
       " u'denver,co',\n",
       " u'morrison co',\n",
       " u'centennial, colorado',\n",
       " u'flyin high in the',\n",
       " u'denver, colorado',\n",
       " u'mile high city',\n",
       " u'houstatlantavegas',\n",
       " u'bridgetown,',\n",
       " u'boulder, co',\n",
       " u'northern colorado',\n",
       " u'denver co',\n",
       " u'colorado',\n",
       " u'c',\n",
       " u'fort collins, co',\n",
       " u'ft',\n",
       " u'aurora, co',\n",
       " u'frisco, tx',\n",
       " u'riverside,ca',\n",
       " u'straight outta tha',\n",
       " u'queen city of the',\n",
       " u'broncoscountry denver,co',\n",
       " u'west palm beach, fl',\n",
       " u'atlanta, ga',\n",
       " u'detroit, denver',\n",
       " u'rva',\n",
       " u'es denver',\n",
       " u'csusb zta',\n",
       " u'riverside',\n",
       " u'slayin, tx',\n",
       " u'orion nebula',\n",
       " u'boston, ma',\n",
       " u'usa',\n",
       " u'denvahh via sandy eggo',\n",
       " u'parts unknown',\n",
       " u'alamosa',\n",
       " u'serendipity',\n",
       " u'worldwide',\n",
       " u'greeley, co',\n",
       " u'keller, tx',\n",
       " u'most dope family',\n",
       " u'psalms',\n",
       " u'a clawfoot bathtub in',\n",
       " u'orange county, ca',\n",
       " u'nola',\n",
       " u'tri',\n",
       " u'elizabeth',\n",
       " u'boston at heart',\n",
       " u'los angeles ca',\n",
       " u'colorado springs',\n",
       " u'tripl',\n",
       " u'denver, san',\n",
       " u'venice',\n",
       " u'kearney, ne',\n",
       " u'washington, dc',\n",
       " u'boston',\n",
       " u'hudson, co',\n",
       " u'longmont, co',\n",
       " u'miami, fl',\n",
       " u'san antonio,tx',\n",
       " u'aurora, colorado',\n",
       " u'co via nm',\n",
       " u'w',\n",
       " u'colorado livin',\n",
       " u'aco',\n",
       " u'denver, bolorado',\n",
       " u'cincinnati, oh',\n",
       " u'bennett, co',\n",
       " u'la',\n",
       " u'co',\n",
       " u'monroe',\n",
       " u'austin, tx',\n",
       " u't',\n",
       " u'lakewood, colorado',\n",
       " u'chino, ca',\n",
       " u'with mines',\n",
       " u'columbus, ohio',\n",
       " u'trying to find myself',\n",
       " u'aurora,colorado',\n",
       " u'wwf',\n",
       " u'the u.s',\n",
       " u'hays, ks',\n",
       " u'hinkley hs',\n",
       " u'brighton, co',\n",
       " u'california, usa',\n",
       " u'co,wy',\n",
       " u'united states',\n",
       " u'fort collins rock city',\n",
       " u'here, there',\n",
       " u'lincoln, ne',\n",
       " u'ovo',\n",
       " u'cleveland, ohio',\n",
       " u'nashville',\n",
       " u'the mile hi city']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test['cache_details']['locations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(test['cache_details']['locations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = [{\"first\":\"linwood\",\"last\":\"creekmore\"}]\n",
    "\n",
    "bob = [\"eit\"]\n",
    "\n",
    "for l in bob:\n",
    "    if l in d[0].keys():\n",
    "        print \"What\"\n",
    "    else:\n",
    "        print \"No\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caching(\"Colorado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cachedlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from geopy.geocoders import GoogleV3, Bing, GeoNames,Nominatim\n",
    "googlegeolocator = GoogleV3()\n",
    "binggeolocator = Bing(api_key=oauth[0]['bing']['key'])\n",
    "geonamesgeolocator = GeoNames(username='linwoodc3')\n",
    "osmgeolocator = Nominatim()\n",
    "osmgeolocator.geocode('Richmond, Virginia')\n",
    "\n",
    "\n",
    "cities=[]\n",
    "for l in viks_vs_hawks[800:820]:\n",
    "    try:\n",
    "        cities.append(re.search('^ ?[^ ]?[A-Za-z]+ ?(,)?[ ]?[A-Za-z]+',l['user']['location']).group(0).strip())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/linwood/projects/LC3-Creations/timehash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import timehash\n",
    "\n",
    "print timehash.encode(int(nflgame[400]['timestamp_ms'])/1000), timehash.encode(int(nflgame[401]['timestamp_ms'])/1000) ,timehash.encode(int(nflgame[300]['timestamp_ms'])/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geohash.encode_uint64(l['coordinates']['coordinates'][1],l['coordinates']['coordinates'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import geohash\n",
    "for l in nflgame[200:225]:\n",
    "    try:\n",
    "        \n",
    "        print l['geo']['coordinates']\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l in set(skins_vs_pack_locs):\n",
    "    try:\n",
    "        print geolocator.geocode(l)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from geopy.geocoders import GeoNames\n",
    "geolocator = GeoNames(username='linwoodc3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b= geolocator.geocode('Twin Cities')\n",
    "print latitude\n",
    "print longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skins_vs_pack_truelocs = []\n",
    "for l['user']['location'] in b:\n",
    "    try:\n",
    "        if len(geolocator.geocode(l['user']['location'])) > 0:\n",
    "            skins_vs_pack_truelocs.append(l['user']['location'])\n",
    "    except:\n",
    "        pass   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(skins_vs_pack_truelocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l in b[1:10]:\n",
    "    print l['user']['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "23.539129, -120.429975\n",
    "62.138919, -57.846612\n",
    " \n",
    "map = Basemap(projection='merc', lat_0 = 23, lon_0 = -120,\n",
    "    resolution = 'h', area_thresh = 0.1,\n",
    "    llcrnrlon=-120.429975, llcrnrlat=23.539129,\n",
    "    urcrnrlon=-57.846612, urcrnrlat=62.138919) \n",
    " \n",
    "map.drawcoastlines()\n",
    "map.drawcountries()\n",
    "map.fillcontinents(color = 'coral')\n",
    "map.drawmapboundary()\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc[0]['user']['location']={\"Name\":b[0],\"Point\":b[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc[1]['user']['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string = '#Brady&ampCo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Brady&Co'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.sub(\"&amp\",\"&\",string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reduce expected at least 2 arguments, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-afdbde4d6c86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: reduce expected at least 2 arguments, got 1"
     ]
    }
   ],
   "source": [
    "reduce(map(str,string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
