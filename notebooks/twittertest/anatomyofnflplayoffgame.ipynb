{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Information\n",
    "###############################################################################\n",
    "# Created by Linwood Creekmore \n",
    "\n",
    "# https://github.com/linwoodc3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Admin work; creating a normalized path to work on any OS for calls to keys or files\n",
    "###############################################################################\n",
    "import geohash\n",
    "import os\n",
    "path = os.path.normpath(os.path.join(os.path.normpath(os.path.expanduser(\"~\")),\"projects\",\"LC3-Creations\"))\n",
    "import sys\n",
    "sys.path.append(os.path.join(path,\"timehash\"))\n",
    "import timehash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "default_stdout = sys.stdout\n",
    "default_stderr = sys.stderr\n",
    "reload(sys)\n",
    "sys.stdout = default_stdout\n",
    "sys.stderr = default_stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Function to load our .txt files from disc into python json/dicts for analysis\n",
    "#**********************************************************************\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "#shameless copy paste from json/decoder.py\n",
    "FLAGS = re.VERBOSE | re.MULTILINE | re.DOTALL\n",
    "WHITESPACE = re.compile(r'[ \\t\\n\\r]*', FLAGS)\n",
    "\n",
    "class ConcatJSONDecoder(json.JSONDecoder):\n",
    "    def decode(self, s, _w=WHITESPACE.match):\n",
    "        s_len = len(s)\n",
    "\n",
    "        objs = []\n",
    "        end = 0\n",
    "        while end != s_len:\n",
    "            obj, end = self.raw_decode(s, idx=_w(s, end).end())\n",
    "            end = _w(s, end).end()\n",
    "            objs.append(obj)\n",
    "        return objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Turn Twitter hashtags into a string for NLP or analysis\n",
    "#**********************************************************************\n",
    "\n",
    "def hashtag_getter(tweet):\n",
    "    try:\n",
    "        # do a test to see if the length of the entities is greater than zero, if not skip\n",
    "        if len([l['text'] for l in tweet['entities']['hashtags']])>0:\n",
    "            \n",
    "            # join all hashtags into a list, split by \",\" and whitespace\n",
    "            hashtags = [(\", \".join([l['text'] for l in tweet['entities']['hashtags']]))]\n",
    "        else:\n",
    "            hashtags = \"\"\n",
    "            \n",
    "    # if we don't have hashtags, this exception prevents an error hangup   \n",
    "    except:\n",
    "        pass\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# How to obfuscate API keys; store as json, run, then clear. DON'T add file to GITHUB \n",
    "\n",
    "# Everyone always said \"hide your keys\" but never say how; this is a way\n",
    "#**********************************************************************\n",
    "'''\n",
    "apikeys = {}\n",
    "apikeys['service']= {\"keyname\": \"key\"}\n",
    "\n",
    "import json\n",
    "\n",
    "# writing to directory two steps above current; in this case, to the repository's base directory\n",
    "with open('../../apikeys.txt', 'w+') as outfile:\n",
    "    json.dump(apikeys, outfile)\n",
    "outfile.closed\n",
    "\n",
    "#**********************************************************************\n",
    "# Have new keys to add? Easy, just follow this process\n",
    "# To add new keys to your file, \n",
    "# just open the .txt in a text editor like notepad, wordpad, sublime, etc\n",
    "# And the new key manually using this example below\n",
    "# anything with \"new\" below would just be added in manually\n",
    "#**********************************************************************\n",
    "\n",
    "apikeys = {\"newservice\": {\"newconsumerkey\": \"newkey\", \"newconsumersecret\": \"newkey\"},\"existingservice\":{\"consumerkey\":\"key\", \"consumersecret\":\"key}}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# How to load your keys from text file; never add this to git\n",
    "#**********************************************************************\n",
    "\n",
    "# I created a nested dictionary with my API keys, then wrote that json to disk.  Now, I load the json and just pass the keys into the application\n",
    "\n",
    "oauth = json.load(open(os.path.join(path,\"apikeys.txt\")), cls=ConcatJSONDecoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# How to retrieve your key for a specific service\n",
    "#**********************************************************************\n",
    "\n",
    "# If you have NOT created the apikeys json file and saved to disc, this will error\n",
    "oauth[0]['openmapquest']['consumerkey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Passing in oauth values; you obviscate by loading your locally stored json\n",
    "#**********************************************************************\n",
    "\n",
    "# Import the necessary package to process data in JSON format\n",
    "try:\n",
    "    import json\n",
    "except ImportError:\n",
    "    import simplejson as json\n",
    "\n",
    "# Import the necessary methods from \"twitter\" library\n",
    "from twitter import Twitter, OAuth, TwitterHTTPError, TwitterStream\n",
    "\n",
    "# Variables that contains the user credentials to access Twitter API \n",
    "# If you have not followed the obfuscate process above, this will be empty and error out\n",
    "ACCESS_TOKEN = oauth[0]['twitter']['accesstoken']\n",
    "ACCESS_SECRET = oauth[0]['twitter']['accesssecret']\n",
    "CONSUMER_KEY = oauth[0]['twitter']['consumerkey']\n",
    "CONSUMER_SECRET = oauth[0]['twitter']['consumersecret']\n",
    "\n",
    "oauth = OAuth(ACCESS_TOKEN, ACCESS_SECRET, CONSUMER_KEY, CONSUMER_SECRET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Code to pull tweets from twitter stream\n",
    "\n",
    "# I pulled this code from http://socialmedia-class.org/twittertutorial.html.  \n",
    "# I used the locations filter, but you can alter to get a sample or pull \n",
    "# specific keywords.  Use the link above.  The only line you would alter is:\n",
    "# iterator = twitter_stream.statuses.filter(\n",
    "#**********************************************************************\n",
    "\n",
    "\n",
    "# Initiate the connection to Twitter Streaming API\n",
    "twitter_stream = TwitterStream(auth=oauth)\n",
    "\n",
    "# Filter the public data following through Twitter\n",
    "iterator = twitter_stream.statuses.filter(locations = '-105.024513,39.741353, -105.014846,39.747408 ')\n",
    "\n",
    "# Print each tweet in the stream to the screen \n",
    "# Here we set it to stop after getting 1000 tweets. \n",
    "# You don't have to set it to stop, but can continue running \n",
    "# the Twitter API to collect data for days or even longer. \n",
    "\n",
    "with open('tweetstream.txt', 'w+') as outfile:\n",
    "    for tweet in iterator:\n",
    "        \n",
    "        # Twitter Python Tool wraps the data returned by Twitter \n",
    "        # as a TwitterDictResponse object.\n",
    "        # We convert it back to the JSON format to print/score\n",
    "        #print json.dumps(tweet)  \n",
    "\n",
    "        # The command below will do pretty printing for JSON data, try it out\n",
    "        # print json.dumps(tweet, indent=4)\n",
    "        json.dump(tweet, outfile)\n",
    "    outfile.closed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# After the tweet streaming data collection is complete, this is how to load the file\n",
    "#**********************************************************************\n",
    "\n",
    "nflgame = json.load(open(os.path.join(path,\"notebooks\",\"twittertest\",\"pats_vs_broncos.txt\")), cls=ConcatJSONDecoder)\n",
    "nflgame2 = json.load(open(os.path.join(path,\"notebooks\",\"twittertest\",\"chiefs_vs_pats.txt\")), cls=ConcatJSONDecoder)\n",
    "nflgame3 = json.load(open(os.path.join(path,\"notebooks\",\"twittertest\",\"skins_vs_pack.txt\")), cls=ConcatJSONDecoder)\n",
    "nflgame4 = [json.loads(l) for l in json.load(open(os.path.join(path,\"notebooks\",\"twittertest\",\"viks_vs_hawks.txt\")), cls=ConcatJSONDecoder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1469"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nflgame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Code to extract values from json tweet and write to csv\n",
    "# http://blog.appliedinformaticsinc.com/how-to-parse-and-convert-json-to-csv-using-python/\n",
    "#**********************************************************************\n",
    "import csv\n",
    "row = {\"tweetdetails\":{}}\n",
    "polygons = 0\n",
    "points = 0\n",
    "game_data = row['tweetdetails']\n",
    "\n",
    "# create a folder to store data\n",
    "if not os.path.exists('./output'):\n",
    "    os.makedirs('./output')\n",
    "\n",
    "# open a file for writing\n",
    "playoffs = open('./output/playoffs.csv', 'w+')\n",
    "\n",
    "# create the csv writer object\n",
    "csvwriter = csv.writer(playoffs)\n",
    "count = 0\n",
    "for l in nflgame:\n",
    "    try:\n",
    "        row['tweetdetails']['message_id']= l['id']\n",
    "        row['tweetdetails']['epochtime']= int(float(l['timestamp_ms'])/float(1000))\n",
    "        row['tweetdetails']['timehash']= timehash.encode(int(l['timestamp_ms'])/1000)\n",
    "        row['tweetdetails']['polygon']=[(m[0],m[1]) for m in (l['place']['bounding_box']['coordinates'][0])]\n",
    "        row['tweetdetails']['screen_name']= l['user']['screen_name']\n",
    "        row['tweetdetails']['user_id']= l['user']['id_str']\n",
    "        row['tweetdetails']['tweet']= l['text']\n",
    "        row['tweetdetails']['hashtags']= hashtag_getter(l)\n",
    "        polygons += 1\n",
    "        try:\n",
    "            if l['geo']['type'] == 'Point':\n",
    "                row['tweetdetails']['Latitude']=l['geo']['coordinates'][0]\n",
    "                row['tweetdetails']['Longitude']=l['geo']['coordinates'][1]\n",
    "                row['tweetdetails']['geohash']=geohash.encode_uint64(l['coordinates']['coordinates'][1],l['coordinates']['coordinates'][0])\n",
    "                points += 1\n",
    "                \n",
    "        except:\n",
    "            row['tweetdetails']['Latitude']=\"\"\n",
    "            row['tweetdetails']['Longitude']=\"\"\n",
    "            row['tweetdetails']['geohash']=\"\"\n",
    "        \n",
    "        \n",
    "        header = row['tweetdetails'].keys()\n",
    "        \n",
    "        if count == 0:\n",
    "            csvwriter.writerow(header)\n",
    "            count += 1\n",
    "        csvwriter.writerow(row['tweetdetails'].values())\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "playoffs.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1469"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'polygon',\n",
       " 'tweet',\n",
       " 'hashtags',\n",
       " 'epochtime',\n",
       " 'timehash',\n",
       " 'geohash',\n",
       " 'Longitude',\n",
       " 'Latitude',\n",
       " 'message_id',\n",
       " 'screen_name']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['tweetdetails'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "game_data = row['tweetdetails']\n",
    "\n",
    "# create a folder to store data\n",
    "if not os.path.exists('./output'):\n",
    "    os.makedirs('./output')\n",
    "\n",
    "# open a file for writing\n",
    "playoffs = open('./output/playoffs.csv', 'w+')\n",
    "\n",
    "# create the csv writer object\n",
    "csvwriter = csv.writer(playoffs)\n",
    "\n",
    "count = 0\n",
    "header = row['tweetdetails'].keys()\n",
    "csvwriter.writerow(header)\n",
    "csvwriter.writerow(row['tweetdetails'].values())\n",
    "playoffs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Nunn, Transportation, VeteranJob, Job, Jobs, Hiring, CareerArc']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_getter(nflgame[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(float(l['timestamp_ms'])/float(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(u'107886768', u'barb_hill'),\n",
       " (u'1254995785', u'DCBarno'),\n",
       " (u'131951051', u'rian5ca'),\n",
       " (u'1356310075', u'clozoya13'),\n",
       " (u'141302910', u'Kindred_Jobs'),\n",
       " (u'15999904', u'WilmingtonWX'),\n",
       " (u'1948302668', u'littlehotmess77'),\n",
       " (u'1950302455', u'Lorenzo_1599'),\n",
       " (u'2587789764', u'WorkWithSHC'),\n",
       " (u'2706556174', u'Crp94'),\n",
       " (u'27585679', u'gerrypizza'),\n",
       " (u'3345561723', u'ACM_Nicky'),\n",
       " (u'33978500', u'AaronMatas'),\n",
       " (u'356840344', u'PracticeWithUs'),\n",
       " (u'365012829', u'PTK473'),\n",
       " (u'546225760', u'kmgcareers'),\n",
       " (u'61101107', u'CurtNickisch'),\n",
       " (u'911570634', u'Jason6440')}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#**********************************************************************\n",
    "# Testing for common users at games\n",
    "#**********************************************************************\n",
    "\n",
    "users3 = []\n",
    "for l in nflgame4:\n",
    "    try:\n",
    "        users3.append((l['user']['id_str'],l['user']['screen_name']))\n",
    "    except:\n",
    "        pass\n",
    "print len(users3)\n",
    "\n",
    "users1 = [(l['user']['id_str'],l['user']['screen_name']) for l in nflgame]\n",
    "users2 = [(l['user']['id_str'],l['user']['screen_name']) for l in nflgame2]\n",
    "users4 = [(l['user']['id_str'],l['user']['screen_name']) for l in nflgame3]\n",
    "\n",
    "a =set(users1) & set(users2)\n",
    "b = set(users2) & set(users4)\n",
    "c = set(users1) & set(users4)\n",
    "d = set(users3) & set(users4)\n",
    "e = set(users3) & set(users2)\n",
    "f = set(users3) & set(users1)\n",
    "\n",
    "\n",
    "#**********************************************************************\n",
    "# Super set of profiles who participated in at least two events\n",
    "#**********************************************************************\n",
    "((((a.union(b)).union(c)).union(d)).union(e)).union(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nflgame[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Geo point locations of tweets; store separately\n",
    "#**********************************************************************\n",
    "\n",
    "count = 0\n",
    "for l in nflgame:\n",
    "    try:\n",
    "        if l['geo']['type'] == 'Point':\n",
    "            print l['geo']['coordinates']\n",
    "            count += 1\n",
    "            print count\n",
    "    except:\n",
    "        pass\n",
    "print float(count)/float(len(nflgame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count =0\n",
    "for l in nflgame[1:100]:\n",
    "    try:\n",
    "        print l['geo']\n",
    "        count += 1\n",
    "        print count\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "googlegeolocator.reverse([nflgame[4]['coordinates']['coordinates'][1],nflgame[4]['coordinates']['coordinates'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "osmgeolocator.reverse((nflgame[111]['coordinates']['coordinates'][1],nflgame[111]['coordinates']['coordinates'][0]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use regex to return only text seperated by , (city, state) or text  by itself; ignores symbols\n",
    "\n",
    "import re\n",
    "\n",
    "for l in nflgame[80:115]:\n",
    "    try:\n",
    "        print re.search('[ ]?[A-Za-z]+[ ]?([A-Za-z]+)?[ ]?([A-Za-z]+)?(,)?[ ]?([A-Za-z]+)?((\\.)[^ ])?([A-Za-z]+)?((\\.)[^ ])?',l['user']['location']).group(0).strip()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l in nflgame[18:23]:\n",
    "    print len(([(m[0],m[1]) for m in (l['place']['bounding_box']['coordinates'][0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "datetime.datetime.fromtimestamp((int(nflgame[20]['timestamp_ms'])/1000)).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use regex to return only text seperated by , (city, state) or text  by itself; ignores symbols\n",
    "\n",
    "import re\n",
    "\n",
    "for l in nflgame[1800:2000]:\n",
    "    try:\n",
    "        print l['text']\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Nunn, Transportation, VeteranJob, Job, Jobs, Hiring, CareerArc']\n",
      "[u'mountmorrisonsummit, mountainevans']\n",
      "[u'broncos']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop over the original json\n",
    "row = {\"tweetdetails\":{}}\n",
    "hashtags=[]\n",
    "for h in nflgame[1:10]:\n",
    "    try:\n",
    "        # do a test to see if the length of the entities is greater than zero, if not skip\n",
    "        if len(\", \".join([l['text'] for l in h['entities']['hashtags']])) >0:\n",
    "\n",
    "            # print the hashtags as a string; each hashtag is split by a space\n",
    "            print [(\", \".join([l['text'] for l in h['entities']['hashtags']]))]         \n",
    "            \n",
    "    # if we don't have hashtags, this exception prevents an error hangup   \n",
    "    except:\n",
    "        pass\n",
    "hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Extract home locations of user as string, geocode, cache name and latitude\n",
    "# and then store latitude and longitude of event for csv writing\n",
    "#**********************************************************************\n",
    "\n",
    "\n",
    "\n",
    "def caching(loc):\n",
    "    test = {\"cache_details\":{}}\n",
    "    try:\n",
    "        from geopy.geocoders import GoogleV3, Bing, GeoNames, Nominatim\n",
    "        google= GoogleV3(api_key=oauth[0]['google']['serverkey'])\n",
    "        bing= Bing(api_key=oauth[0]['bing']['key'])\n",
    "        geonames = GeoNames(username = oauth[0]['geonames']['username'])\n",
    "        osm = Nominatim()\n",
    "    except:\n",
    "        print \"You need to install the \\'geopy\\' module or edit input parameters\"\n",
    "        \n",
    "    try:\n",
    "        import re\n",
    "    except:\n",
    "        print \"You need to install the \\'re\\' module\"\n",
    "        \n",
    "    try: \n",
    "        import time\n",
    "    except:\n",
    "        print \"You need to install the \\'time\\' module\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    locations =[]\n",
    "    for l in nflgame[18:35]:\n",
    "        try:\n",
    "            locations.append(re.search('[ ]?[A-Za-z]+[ ]?([A-Za-z]+)?[ ]?([A-Za-z]+)?(,)?[ ]?([A-Za-z]+)?((\\.)[^ ])?([A-Za-z]+)?((\\.)[^ ])?',l['user']['location']).group(0).strip())\n",
    "        except:\n",
    "            pass\n",
    "    test['cache_details']['locations']=locations\n",
    "    for l in locations:\n",
    "        if l in cachedlocs[0].keys():\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                answer = geonames.geocode(l)\n",
    "                cachedlocs.append({l:{\"realname\":answer[0],\"latitude\":answer[1][0],\"longitude\":answer[1][1]}})\n",
    "                \n",
    "                time.sleep(1)\n",
    "            except: \n",
    "                pass\n",
    "    return cachedlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc = nflgame[1:20]\n",
    "test = {\"cache_details\":{}}\n",
    "try:\n",
    "    from geopy.geocoders import GoogleV3, Bing, GeoNames, Nominatim\n",
    "    google= GoogleV3(api_key=oauth[0]['google']['serverkey'])\n",
    "    bing= Bing(api_key=oauth[0]['bing']['key'])\n",
    "    geonames = GeoNames(username = oauth[0]['geonames']['username'])\n",
    "    osm = Nominatim()\n",
    "except:\n",
    "    print \"You need to install the \\'geopy\\' module or edit input parameters\"\n",
    "\n",
    "try:\n",
    "    import re\n",
    "except:\n",
    "    print \"You need to install the \\'re\\' module\"\n",
    "\n",
    "try: \n",
    "    import time\n",
    "except:\n",
    "    print \"You need to install the \\'time\\' module\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for l in loc:\n",
    "    try:\n",
    "        \n",
    "        m = locations.append(re.search('[ ]?[A-Za-z]+[ ]?([A-Za-z]+)?[ ]?([A-Za-z]+)?(,)?[ ]?([A-Za-z]+)?((\\.)[^ ])?([A-Za-z]+)?((\\.)[^ ])?',l['user']['location']).group(0).strip())\n",
    "         \n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "test['cache_details']['locations']=locations\n",
    "test['cache_details']['cache']={}\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Riverside, CA'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nflgame[18]['user']['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['cache_details']['locations'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from geopy.geocoders import GoogleV3, Bing, GeoNames,Nominatim\n",
    "\n",
    "google= GoogleV3(api_key=oauth[0]['google']['serverkey'])\n",
    "bing= Bing(api_key=oauth[0]['bing']['key'])\n",
    "geonames = GeoNames(username='linwoodc3')\n",
    "osm = Nominatim()\n",
    "#geonames.geocode('kansas city metro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = google.geocode('Riverside, CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'Riverside, CA, USA', (33.9533487, -117.3961564))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0],b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name = str(nflgame[18]['user']['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test['cache_details']['cache'][name] = {\"Name\":b[0],\"Point\":b[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test['cache_details']['locations']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Extract home locations of user as string, geocode, cache name and latitude\n",
    "# and then store latitude and longitude of event for csv writing\n",
    "#**********************************************************************\n",
    "\n",
    "\n",
    "for l in nflgame[20:550]:\n",
    "    try:  \n",
    "        m = re.search('[ ]?[A-Za-z]+[ ]?([A-Za-z]+)?[ ]?([A-Za-z]+)?(,)?[ ]?([A-Za-z]+)?((\\.)[^ ])?([A-Za-z]+)?((\\.)[^ ])?',l['user']['location']).group(0).strip().lower()\n",
    "        print m \n",
    "        if m in test['cache_details']['locations']:\n",
    "            print \"It's there!!\"\n",
    "            pass\n",
    "        else:\n",
    "            test['cache_details']['locations'].append(m)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'providence, ri',\n",
       " u'adelaide, australia',\n",
       " u'california',\n",
       " u'denver, co',\n",
       " u'hq aspen beverly hills',\n",
       " u'islamic republic of',\n",
       " u'chicago, il',\n",
       " u'battle creek, mi',\n",
       " u'charlotte, nc',\n",
       " u'the internet',\n",
       " u'colorado, usa',\n",
       " u'houston, tx',\n",
       " u'the',\n",
       " u'colorado springs, co',\n",
       " u'coloroado',\n",
       " u'riverside, ca',\n",
       " u'mars',\n",
       " u'instagram',\n",
       " u'canto seven, ca',\n",
       " u'lamar, co',\n",
       " u'staywithme',\n",
       " u'aspen, co',\n",
       " u'littleton, co',\n",
       " u'denver',\n",
       " u'colorado, ny',\n",
       " u'colorado, with',\n",
       " u'denver,co',\n",
       " u'morrison co',\n",
       " u'centennial, colorado',\n",
       " u'flyin high in the',\n",
       " u'denver, colorado',\n",
       " u'mile high city',\n",
       " u'houstatlantavegas',\n",
       " u'bridgetown,',\n",
       " u'boulder, co',\n",
       " u'northern colorado',\n",
       " u'denver co',\n",
       " u'colorado',\n",
       " u'c',\n",
       " u'fort collins, co',\n",
       " u'ft',\n",
       " u'aurora, co',\n",
       " u'frisco, tx',\n",
       " u'riverside,ca',\n",
       " u'straight outta tha',\n",
       " u'queen city of the',\n",
       " u'broncoscountry denver,co',\n",
       " u'west palm beach, fl',\n",
       " u'atlanta, ga',\n",
       " u'detroit, denver',\n",
       " u'rva',\n",
       " u'es denver',\n",
       " u'csusb zta',\n",
       " u'riverside',\n",
       " u'slayin, tx',\n",
       " u'orion nebula',\n",
       " u'boston, ma',\n",
       " u'usa',\n",
       " u'denvahh via sandy eggo',\n",
       " u'parts unknown',\n",
       " u'alamosa',\n",
       " u'serendipity',\n",
       " u'worldwide',\n",
       " u'greeley, co',\n",
       " u'keller, tx',\n",
       " u'most dope family',\n",
       " u'psalms',\n",
       " u'a clawfoot bathtub in',\n",
       " u'orange county, ca',\n",
       " u'nola',\n",
       " u'tri',\n",
       " u'elizabeth',\n",
       " u'boston at heart',\n",
       " u'los angeles ca',\n",
       " u'colorado springs',\n",
       " u'tripl',\n",
       " u'denver, san',\n",
       " u'venice',\n",
       " u'kearney, ne',\n",
       " u'washington, dc',\n",
       " u'boston',\n",
       " u'hudson, co',\n",
       " u'longmont, co',\n",
       " u'miami, fl',\n",
       " u'san antonio,tx',\n",
       " u'aurora, colorado',\n",
       " u'co via nm',\n",
       " u'w',\n",
       " u'colorado livin',\n",
       " u'aco',\n",
       " u'denver, bolorado',\n",
       " u'cincinnati, oh',\n",
       " u'bennett, co',\n",
       " u'la',\n",
       " u'co',\n",
       " u'monroe',\n",
       " u'austin, tx',\n",
       " u't',\n",
       " u'lakewood, colorado',\n",
       " u'chino, ca',\n",
       " u'with mines',\n",
       " u'columbus, ohio',\n",
       " u'trying to find myself',\n",
       " u'aurora,colorado',\n",
       " u'wwf',\n",
       " u'the u.s',\n",
       " u'hays, ks',\n",
       " u'hinkley hs',\n",
       " u'brighton, co',\n",
       " u'california, usa',\n",
       " u'co,wy',\n",
       " u'united states',\n",
       " u'fort collins rock city',\n",
       " u'here, there',\n",
       " u'lincoln, ne',\n",
       " u'ovo',\n",
       " u'cleveland, ohio',\n",
       " u'nashville',\n",
       " u'the mile hi city']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test['cache_details']['locations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(test['cache_details']['locations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = [{\"first\":\"linwood\",\"last\":\"creekmore\"}]\n",
    "\n",
    "bob = [\"eit\"]\n",
    "\n",
    "for l in bob:\n",
    "    if l in d[0].keys():\n",
    "        print \"What\"\n",
    "    else:\n",
    "        print \"No\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caching(\"Colorado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cachedlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from geopy.geocoders import GoogleV3, Bing, GeoNames,Nominatim\n",
    "googlegeolocator = GoogleV3()\n",
    "binggeolocator = Bing(api_key=oauth[0]['bing']['key'])\n",
    "geonamesgeolocator = GeoNames(username='linwoodc3')\n",
    "osmgeolocator = Nominatim()\n",
    "osmgeolocator.geocode('Richmond, Virginia')\n",
    "\n",
    "\n",
    "cities=[]\n",
    "for l in viks_vs_hawks[800:820]:\n",
    "    try:\n",
    "        cities.append(re.search('^ ?[^ ]?[A-Za-z]+ ?(,)?[ ]?[A-Za-z]+',l['user']['location']).group(0).strip())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/linwood/projects/LC3-Creations/timehash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import timehash\n",
    "\n",
    "print timehash.encode(int(nflgame[400]['timestamp_ms'])/1000), timehash.encode(int(nflgame[401]['timestamp_ms'])/1000) ,timehash.encode(int(nflgame[300]['timestamp_ms'])/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geohash.encode_uint64(l['coordinates']['coordinates'][1],l['coordinates']['coordinates'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import geohash\n",
    "for l in nflgame[200:225]:\n",
    "    try:\n",
    "        \n",
    "        print l['geo']['coordinates']\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l in set(skins_vs_pack_locs):\n",
    "    try:\n",
    "        print geolocator.geocode(l)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from geopy.geocoders import GeoNames\n",
    "geolocator = GeoNames(username='linwoodc3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b= geolocator.geocode('Twin Cities')\n",
    "print latitude\n",
    "print longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skins_vs_pack_truelocs = []\n",
    "for l['user']['location'] in b:\n",
    "    try:\n",
    "        if len(geolocator.geocode(l['user']['location'])) > 0:\n",
    "            skins_vs_pack_truelocs.append(l['user']['location'])\n",
    "    except:\n",
    "        pass   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(skins_vs_pack_truelocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l in b[1:10]:\n",
    "    print l['user']['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "23.539129, -120.429975\n",
    "62.138919, -57.846612\n",
    " \n",
    "map = Basemap(projection='merc', lat_0 = 23, lon_0 = -120,\n",
    "    resolution = 'h', area_thresh = 0.1,\n",
    "    llcrnrlon=-120.429975, llcrnrlat=23.539129,\n",
    "    urcrnrlon=-57.846612, urcrnrlat=62.138919) \n",
    " \n",
    "map.drawcoastlines()\n",
    "map.drawcountries()\n",
    "map.fillcontinents(color = 'coral')\n",
    "map.drawmapboundary()\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc[0]['user']['location']={\"Name\":b[0],\"Point\":b[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc[1]['user']['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/Users/linwood/anaconda/envs/py27/lib/python27.zip',\n",
       " '/Users/linwood/anaconda/envs/py27/lib/python2.7',\n",
       " '/Users/linwood/anaconda/envs/py27/lib/python2.7/plat-darwin',\n",
       " '/Users/linwood/anaconda/envs/py27/lib/python2.7/plat-mac',\n",
       " '/Users/linwood/anaconda/envs/py27/lib/python2.7/plat-mac/lib-scriptpackages',\n",
       " '/Users/linwood/anaconda/envs/py27/lib/python2.7/lib-tk',\n",
       " '/Users/linwood/anaconda/envs/py27/lib/python2.7/lib-old',\n",
       " '/Users/linwood/anaconda/envs/py27/lib/python2.7/lib-dynload',\n",
       " '/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/setuptools-19.2-py2.7.egg',\n",
       " '/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages',\n",
       " '/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/IPython/extensions',\n",
       " '/Users/linwood/.ipython',\n",
       " '/Users/linwood/projects/LC3-Creations/timehash']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
