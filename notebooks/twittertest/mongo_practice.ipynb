{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path\n",
    "!pip install twitter\n",
    "!pip install python-geohash\n",
    "!pip install timehash\n",
    "!pip install polyglot\n",
    "!pip install scikit-learn\n",
    "!pip install stop-words\n",
    "!pip install pandas\n",
    "!pip install pandas-ply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "western_world = \"\"\"Europe (at least the European Union member states, EFTA countries, European microstates);\n",
    "in the Americas (e.g. Argentina, Brazil, Canada, Chile, Colombia, Costa Rica, Cuba, Mexico, \n",
    "United States of America, Uruguay, Venezuela), and in Oceania (Australia and New Zealand)\"\"\"\n",
    "\n",
    "western_countries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "#shameless copy paste from json/decoder.py\n",
    "FLAGS = re.VERBOSE | re.MULTILINE | re.DOTALL\n",
    "WHITESPACE = re.compile(r'[ \\t\\n\\r]*', FLAGS)\n",
    "\n",
    "class ConcatJSONDecoder(json.JSONDecoder):\n",
    "    def decode(self, s, _w=WHITESPACE.match):\n",
    "        s_len = len(s)\n",
    "\n",
    "        objs = []\n",
    "        end = 0\n",
    "        while end != s_len:\n",
    "            obj, end = self.raw_decode(s, idx=_w(s, end).end())\n",
    "            end = _w(s, end).end()\n",
    "            objs.append(obj)\n",
    "        return objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geohash, timehash, os, json, string, sklearn,unicodedata, pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = os.path.normpath(os.path.join(os.path.normpath(os.path.expanduser(\"~\")),\"projects\",\"LC3-Creations\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/linwood/projects/LC3-Creations'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oauth = json.load(open(os.path.join(path,\"apikeys.txt\")), cls=ConcatJSONDecoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Passing in oauth values; you obviscate by loading your locally stored json\n",
    "#**********************************************************************\n",
    "\n",
    "# Import the necessary package to process data in JSON format\n",
    "try:\n",
    "    import json\n",
    "except ImportError:\n",
    "    import simplejson as json\n",
    "\n",
    "# Import the necessary methods from \"twitter\" library\n",
    "from twitter import Twitter, OAuth, TwitterHTTPError, TwitterStream\n",
    "\n",
    "# Variables that contains the user credentials to access Twitter API \n",
    "# If you have not followed the obfuscate process above, this will be empty and error out\n",
    "ACCESS_TOKEN = oauth[0]['twitter']['accesstoken']\n",
    "ACCESS_SECRET = oauth[0]['twitter']['accesssecret']\n",
    "CONSUMER_KEY = oauth[0]['twitter']['consumerkey']\n",
    "CONSUMER_SECRET = oauth[0]['twitter']['consumersecret']\n",
    "\n",
    "oauth = OAuth(ACCESS_TOKEN, ACCESS_SECRET, CONSUMER_KEY, CONSUMER_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Code to pull tweets from twitter stream\n",
    "\n",
    "# I pulled this code from http://socialmedia-class.org/twittertutorial.html.  \n",
    "# I used the locations filter, but you can alter to get a sample or pull \n",
    "# specific keywords.  Use the link above.  The only line you would alter is:\n",
    "# iterator = twitter_stream.statuses.filter()\n",
    "# Want to know what arguments to pass in? visit https://dev.twitter.com/streaming/reference/post/statuses/filter\n",
    "#**********************************************************************\n",
    "\n",
    "\n",
    "# Initiate the connection to Twitter Streaming API\n",
    "twitter_stream = TwitterStream(auth=oauth)\n",
    "\n",
    "# Filter the public data following through Twitter; the format is \"long,lat , long,lat\" with southwest corner first\n",
    "iterator = twitter_stream.statuses.filter(locations = '-44.888519, -23.37085, -40.953869,-20.76347')\n",
    "\n",
    "# Print each tweet in the stream to the screen \n",
    "# Here we set it to stop after getting 1000 tweets. \n",
    "# You don't have to set it to stop, but can continue running \n",
    "# the Twitter API to collect data for days or even longer. \n",
    "\n",
    "with open('./output/<brazilSample.txt'>, 'w+') as outfile:\n",
    "    for tweet in iterator:\n",
    "        try:\n",
    "            if tweet['user']['geo_enabled']==True:\n",
    "                json.dump(tweet, outfile)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        \n",
    "        # Twitter Python Tool wraps the data returned by Twitter \n",
    "        # as a TwitterDictResponse object.\n",
    "        # We convert it back to the JSON format to print/score\n",
    "        #print json.dumps(tweet)  \n",
    "\n",
    "        # The command below will do pretty printing for JSON data, try it out\n",
    "        # print json.dumps(tweet, indent=4)\n",
    "        \n",
    "    outfile.closed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brazil = json.load(open(os.path.join(path,\"notebooks\",\"twittertest\",\"output\",\"brazilSample.txt\")), cls=ConcatJSONDecoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3454"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brazil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['coordinates', 'favorited', 'source', 'in_reply_to_status_id_str', 'created_at', 'favorite_count', 'is_quote_status', 'text', 'place', 'filter_level', 'in_reply_to_screen_name', 'contributors', 'geo', 'in_reply_to_status_id', 'id_str', 'id', 'truncated', 'retweeted', 'retweet_count', 'timestamp_ms', 'in_reply_to_user_id', 'user', 'in_reply_to_user_id_str', 'entities', 'lang'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brazil[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "braz_frame = pd.DataFrame(brazil)\n",
    "braz_frame = braz_frame[[\"id\",\"created_at\",\"place\",\"text\",\"lang\",\"geo\",\"entities\",\"timestamp_ms\",\"user\",\"coordinates\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "braz_frame['timestamp_ms'] = braz_frame['timestamp_ms'].to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75cm34xes493\n",
      "75cm36t3j13n\n",
      "75cm3e16fpcx\n",
      "75cm3ekmz41j\n",
      "75cm3spgxjcw\n"
     ]
    }
   ],
   "source": [
    "import geohash\n",
    "print (geohash.encode(-22.969051, -43.187769))\n",
    "print (geohash.encode(-22.969486, -43.179830))\n",
    "print (geohash.encode(-22.966404, -43.174465))\n",
    "print (geohash.encode(-22.964547, -43.170131))\n",
    "print (geohash.encode(-22.960793, -43.165324))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# decode('utf-8') changed from bytes to string\n",
    "tweets = [unicodedata.normalize('NFKD', i['text']).encode('ascii','ignore').decode('utf-8') for i in brazil]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tweets),type(tweets)\n",
    "tweets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_tokenize(tweets[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(word_tokenize(tweets[5])) - set(string.punctuation)-set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(stopwords.words('portuguese')) & set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB practice on Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient,GEO2D\n",
    "import pymongo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client.brazil\n",
    "collection = db['brazil-tweets']\n",
    "brazil_tweets = db['brazil-tweets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a geospatial index for querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cut = brazil_tweets.find({\"coordinates.coordinates\":{\"$ne\":None}})\n",
    "for doc in cut:\n",
    "    docID = doc['_id']\n",
    "    doc['geometry']= {\"type\":\"Point\", \"coordinates\": [float(doc['coordinates']['coordinates'][0]),\n",
    "                      float(doc['coordinates']['coordinates'][1])]}\n",
    "    brazil_tweets.update_one({'_id':docID},{\"$set\":doc}, upsert=False)\n",
    "    \n",
    "brazil_tweets.create_index([(\"geometry\",pymongo.GEOSPHERE)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boundary = {}\n",
    "boundary['geometry']={\"type\":\"Polygon\",\"coordinates\":[[[\"-43.793888092041\",\"-23.0763473510742\"],[\"-42.6359710693359\",\"-22.4493675231934\"]]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for l in brazil_tweets.find({\"geometry\":{\"$ne\":None}}):\n",
    "    print (l['text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3⃣0⃣4⃣❗😍⛴🛳🚢✨🇺🇾🇧🇷🌴☀💥💃👗👠👑🎆🎈💕💖 #Crucero2017 #Miss15 @ MSC Orchestra… https://t.co/URLiMSpgJD\n",
      "Lagoa ❤️ @ Lagoa Rodrigo De Freitas - Rio De Janeiro - Brasil https://t.co/9j1FV8f1LF\n",
      "Mais um filho nasceu, nossa verdade musical, uma proposta diferente, pra encantar! É impossível… https://t.co/aAKQzmeedw\n",
      "Ai, que saudade docês! De você também, @wiledsilveira , mas não achei fofo do quarteto ! @… https://t.co/LAjsh2aLby\n",
      "Mais um dia de recreação na @riofighters Sábado é dia das crianças #RioMiniFighters… https://t.co/JqbcQ6iKEs\n",
      "Nada de mais... Simplesmente eu! Minha nova tatuagem em fase inicial feita pelo Eduardo meu anjo… https://t.co/Os3INoPxah\n",
      "I'm at Amelie in Rio de Janeiro https://t.co/uD78CXnjiL\n",
      "Sinceridade gata, eh nois que ta linda 🌊🌺💣 @ Praia de Itacoatiara https://t.co/V4yrToNXeT\n",
      "I'm at Praia de Ipanema in Rio de Janeiro, RJ, RJ https://t.co/HFDIseYs4F\n",
      "Just posted a photo @ Estátua de Dorival Caymmi https://t.co/BxaJ9S088k\n",
      "🍻 (@ Maria da Praia in Niterói, RJ) https://t.co/Hr61Z8eW0E\n",
      "Y nos vamos a la praia! @rodrigogual elianayanuzzi 🎉🇧🇷🏃🏼🙄☝🏼️ @ Praia de Ipanema - Posto 9 https://t.co/asOtdemsEL\n",
      "Flash disponíveis pra semana que vem no valor de R$150. Os desenhos só serão tatuados apenas uma… https://t.co/iX42AL7uxH\n",
      "Porr meu celular só fica do 70 pro 86 %\n",
      "Comprar outra bateria\n",
      "I'm at Restô Ipanema in Rio de Janeiro, RJ https://t.co/tk5OFg4wto\n",
      "#ElasPorElas, evento lindo, makes lindas, tudo lindo!!!! Obrigada a todos os presentes e ao… https://t.co/Uyhhzx7uL9\n",
      "Local (@ Praia de Ponta Negra in Maricá, RJ) https://t.co/wqOM85cFYo\n",
      "@rionaturaltour\n",
      "\n",
      "Tirando onda em Rio… https://t.co/UVo9NBJUYJ\n",
      "Há boatos que se repararem bem, da pra ver o Didi Mocó escalando a cabeça do Cristo. @ Morro Do… https://t.co/U3JUjV07BD\n",
      "🌊🌊 @ Forte De Copacabana https://t.co/KyxlnMoPEW\n"
     ]
    }
   ],
   "source": [
    "for l in brazil_tweets.find({\"geometry\": {\"$geoWithin\": {\"$box\": [[-23.006508, -22.946076],\n",
    "                                                               [-43.254849, -43.171159]]}}}):\n",
    "    print (l['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bson.son import SON\n",
    "query = {\"coordinates\": SON([(\"$near\", [-43.40184053, -22.78785425]), (\"$maxDistance\", 1000)])}\n",
    "for doc in db.brazil_tweets.find(query).limit(100):\n",
    "    print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3⃣0⃣4⃣❗😍⛴🛳🚢✨🇺🇾🇧🇷🌴☀💥💃👗👠👑🎆🎈💕💖 #Crucero2017 #Miss15 @ MSC Orchestra… https://t.co/URLiMSpgJD [-43.18994352, -22.97149778]\n",
      "Mais um filho nasceu, nossa verdade musical, uma proposta diferente, pra encantar! É impossível… https://t.co/aAKQzmeedw [-43.19110816, -22.96417801]\n"
     ]
    }
   ],
   "source": [
    "for doc in brazil_tweets.find({\"geometry\": {\"$within\": {\"$center\": [[ -43.182651,-22.966658], .01]}}}):\n",
    "    print (doc['text'],doc['geometry']['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genocídio\n",
      "motim\n",
      "levante\n",
      "pronunciamento\n",
      "comício\n",
      "boicote\n",
      "retaliação\n",
      "agradecimento\n",
      "tumulto\n",
      "assédio\n"
     ]
    }
   ],
   "source": [
    "import polyglot\n",
    "from polyglot.text import Text, Word\n",
    "\n",
    "\n",
    "word = Word(\"protesto\", language=\"pt\")\n",
    "for w in word.neighbors:\n",
    "    print (w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.59737962, -0.64523113,  0.33711299, -0.3455652 ,  0.19411458,\n",
       "       -0.33565947, -2.62641382, -0.22692174,  0.56430954,  0.37213761,\n",
       "        1.62610054, -0.97020465, -2.01416898, -0.02478787,  0.75397676,\n",
       "       -1.30008435,  1.35396922,  1.00399423, -0.82601434,  2.21568418,\n",
       "       -0.91054553, -0.12676898,  1.53243148, -1.0757668 , -0.28473505,\n",
       "       -0.41047156, -1.6206857 ,  1.80200827, -0.42291695, -1.82027614,\n",
       "        0.89114261, -1.53802562, -0.85561776,  1.32964885, -0.00562795,\n",
       "        1.07157397, -0.04918216,  0.05134355,  0.71893853, -1.5087105 ,\n",
       "        0.03973555, -1.74955964,  0.56620049, -2.9240346 ,  0.2276551 ,\n",
       "        0.40591595,  0.67361599,  2.29726338,  1.00404775,  3.56751752,\n",
       "       -0.43566847,  1.67288542,  0.02764815, -2.25402856,  0.66003841,\n",
       "       -1.61814737, -1.27222264,  0.54359263,  3.7455554 ,  0.06947216,\n",
       "       -0.77350634,  1.52999365, -1.38145363, -2.08945274, -1.05367637,\n",
       "       -0.38085824,  1.29245329,  2.81955838, -3.83165956,  1.18979847,\n",
       "        0.95275378, -0.59192222, -0.80119759,  0.75842476, -1.36696148,\n",
       "       -0.84787327,  0.19997346, -0.75162631, -1.58376253,  0.90925378,\n",
       "       -1.23044431,  0.28735358,  0.88350761,  1.58150733,  0.4551717 ,\n",
       "        0.23460953, -0.87350202,  1.0386349 , -1.53554547, -0.91647321,\n",
       "        0.50300109,  1.98016906, -1.11527789, -1.38315451, -2.21848154,\n",
       "       -1.97859061, -1.09123862, -0.22129712, -0.16231433, -1.8827225 ,\n",
       "       -1.47156942, -0.76386148, -0.11800364, -1.27611709, -0.7774117 ,\n",
       "       -0.20140626, -0.43702975,  1.66505671, -3.85946321, -1.49456763,\n",
       "        0.15892522, -0.90642625,  2.46801257,  1.57532609,  0.069017  ,\n",
       "       -1.0079329 , -1.72177351, -0.57268316, -0.04037301,  0.50744575,\n",
       "       -0.23914512, -2.97935343,  2.26255202,  2.2559371 , -2.49683905,\n",
       "       -1.58295989, -0.80662763, -0.95856106, -0.14200604, -2.38509274,\n",
       "       -1.36663985,  0.53842181, -0.51944834, -1.30445135,  0.19963665,\n",
       "        0.17483287,  0.45674878, -2.23956966,  0.85650408,  2.13137174,\n",
       "        0.8472724 ,  4.1003747 , -1.7584374 , -1.37245584, -0.85932881,\n",
       "       -0.93013388,  1.64307737,  2.6994102 ,  0.04236996,  1.64171565,\n",
       "        1.63631737,  0.48165706, -1.77319431, -1.22316003, -0.39113733,\n",
       "        1.85539186, -2.48193383,  0.1837081 , -2.63874674, -1.02185535,\n",
       "        0.20020613,  1.47538006,  0.31887257, -2.39515972,  0.6868211 ,\n",
       "        2.13516808,  1.01925731, -1.1793412 , -2.27619529,  1.25125253,\n",
       "        1.05367064,  0.60153145, -0.47718808,  3.80491567, -1.38008535,\n",
       "       -1.49180865,  0.48827791, -0.74812943, -2.19267941,  1.29497302,\n",
       "        0.34041879,  0.22698779,  0.50887173,  1.80233288,  0.30043477,\n",
       "        1.43629646,  0.38822535, -1.90827096,  0.27387986, -0.85658699,\n",
       "       -0.59770292, -0.60996068,  0.43509248, -1.30836761, -1.88449132,\n",
       "       -0.13145076,  0.53008777, -0.24870574, -1.63376606, -2.2306664 ,\n",
       "       -0.55088502, -2.67537832, -0.13356088,  0.25761577, -2.87009525,\n",
       "       -0.61236227, -0.18240924,  3.0610733 ,  0.28963315, -2.3650701 ,\n",
       "        2.18477488, -1.69444776, -0.21745971,  1.4632411 ,  2.99871945,\n",
       "        2.13970423,  0.04705025, -0.0346884 , -2.12726903,  0.38207823,\n",
       "       -1.09596419, -0.62582731,  2.57660532, -0.56756222,  0.96685594,\n",
       "       -1.32595885, -0.57267362,  0.17184253, -2.07625055, -0.31766093,\n",
       "        1.7755568 ,  1.74447751,  1.01341999, -0.76080656, -0.59386647,\n",
       "        0.86396146,  4.64358234,  0.74794304,  1.60687876,  0.43952683,\n",
       "       -0.03039453,  0.17717855,  1.19925368,  3.7796104 ,  1.70908964,\n",
       "       -2.46419668, -1.03105807, -1.97542024,  1.49797332, -2.06895542,\n",
       "       -1.31174779, -2.8193233 , -0.39930025, -0.41015494, -0.92383707,\n",
       "        0.71234781], dtype=float32)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (socialMediaAnalysis)",
   "language": "python",
   "name": "socialmediaanalysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
