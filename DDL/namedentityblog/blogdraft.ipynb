{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A ~~Quick~~ Survey and Comparison of Open Source Named Entity Extractor Tools for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity extraction is a core subtask of building knowledge from semi/unstructured text sources<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup>.  Considering recent increases in computing power and decreases in the costs of data storage, data scientists and developers can build large knowledge bases that contain millions of entities and hundreds of millions of facts about them.  These knowledge bases are key contributors to intelligence computer behavior<sup><a href=\"#fn2\" id=\"ref2\">2</a></sup>.  Therefore, named entity extraction is at the core of several popular technologies such as smart assistants ([Siri](http://www.apple.com/ios/siri/), [Google Now](https://www.google.com/landing/now/)), machine reading, and deep interpretation of natural language<sup><a href=\"#fn3\" id=\"ref3\">3</a></sup>.\n",
    "\n",
    "With a realization of how essential it is to recognize information units like names, including person, organization and location names, and numeric expressions including time, date, money\n",
    "and percent expressions, several questions come to mind.  How do you perform named entity extraction, which is formally called “[Named Entity Recognition and Classification (NERC)](https://benjamins.com/catalog/bct.19)”?  What tools are out there?  How can you evaluate their performance?  And most important, what works with Python (shamelessly exposing my bias)?  \n",
    "\n",
    "This post will survey openly available NERC tools and compare the results against hand labeled data for precision, accuracy, and recall.  The tools and basic information extraction principles in this discussion begin the process of structuring unstructured data.\n",
    "\n",
    "We will specifically learn to:\n",
    "1. follow the data science pipeline (see image below)\n",
    "2. prepare semistructured natural language data for ingest using regex\n",
    "3. create a custom corpus in [Natural Language Toolkit](http://www.nltk.org/) \n",
    "4. use a suite of openly available NERC tools to extract entities and store in json format \n",
    "5. compare the performance of NERC tools on our corpus\n",
    "\n",
    "<br>\n",
    "<a href=\"#pipe\" id=\"pipeline\"><center><h3>The Data Science Pipeline:<br>Georgetown Data Science Certificate Program</h3></center></a>\n",
    "<div class=\"image\">\n",
    "\n",
    "      <img src=\"./files/data_science_pipeline.png\" alt=\"Data Science Pipeline\" height=\"300\" width=\"450\" top:\"35\" left:\"170\" />\n",
    "      \n",
    "      \n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data: Peer Reviewed Journals and Keynote Speaker Abstracts from KDD 2014 and 2015\n",
    "\n",
    "Before delving into the pipeline, we need a good dataset.  Jason Brownlee of www.machinelearningmastery.com had some good suggestions in his [August 2015 article](http://machinelearningmastery.com/practice-machine-learning-with-small-in-memory-datasets-from-the-uci-machine-learning-repository/) on picking a dataset for machine learning exercises:  \n",
    "\n",
    "* **Real-World**: The datasets should be drawn from the real world (rather than being contrived). This will keep them interesting and introduce the challenges that come with real data.\n",
    "\n",
    "* **Small**: The datasets need to be small so that you can inspect and understand them and that you can run many models quickly to accelerate your learning cycle.\n",
    "\n",
    "* **Well-Understood**: There should be a clear idea of what the data contains, why it was collected, what the problem is that needs to be solved so that you can frame your investigation.\n",
    "\n",
    "* **Baseline**: It is also important to have an idea of what algorithms are known to perform well and the scores they achieved so that you have a useful point of comparison. This is important when you are getting started and learning because you need quick feedback as to how well you are performing (close to state-of-the-art or something is broken).\n",
    "\n",
    "* **Plentiful**: You need many datasets to choose from, both to satisfy the traits you would like to investigate and (if possible) your natural curiosity and interests. \n",
    "\n",
    "Luckily, we have a dataset that meets nearly all of these requirements.  I attended the Knowledge Discovery and Data Mining (KDD) conferences in [New York City (2014)](http://www.kdd.org/kdd2014/) and [Sydney, Australia (2015)](http://www.kdd.org/kdd2015/).  Both years, attendees received a USB with the conference proceedings.  Each repository contains over 230 peer reviewed journal articles and keynote speaker abstracts on data mining, knowledge discovery, big data, data science and their applications. The full conference proceedings can be purchased for \\$60 at the [Association for Computing Machinery's Digital Library](https://dl.acm.org/purchase.cfm?id=2783258&CFID=740512201&CFTOKEN=34489585) (includes ACM membership). This post will work with a dataset that is equivalent to the conference proceedings.  It's important to note that this dataset recreates a real word data science exercise that is instructive of big data problems.  We will take semi-structured data (PDF journal articles and abstracts in publication format), strip text from the files, and add more structure to the data that would facilitate follow on analysis. \n",
    "\n",
    "<blockquote cite=\"https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/KDDwebscrape.ipynb\">\n",
    "Interested parties looking for a free option can use the <a href=\"https://pypi.python.org/pypi/beautifulsoup4/4.4.1\">beautifulsoup</a> and <a href=\"https://pypi.python.org/pypi/requests/2.9.1\">request</a> libraries to scrape the <a href=\"http://dl.acm.org/citation.cfm?id=2785464&CFID=740512201&CFTOKEN=3448958\">ACM website for KDD 2015 conference data</a> that can be used in natural language processing pipelines.  I have some <a href=\"https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/KDDwebscrape.ipynb\">skeleton web scraping code</a> to generate lists of all abstracts, author names, and journal/keynote address titles.    \n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration: Getting the number of files, and file type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored locally in the following directory:\n",
    "```python\n",
    ">>> import os\n",
    ">>> print os.getcwd()\n",
    "/Users/linwood/Desktop/KDD_15/docs\n",
    "```\n",
    "Let's explore the number of files we have and naming conventions. We begin with the administrative tasks of loading modules, establishing paths, etc.  \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Importing what we need\n",
    "#**********************************************************************\n",
    "import os\n",
    "import time\n",
    "from os import walk\n",
    "\n",
    "#**********************************************************************\n",
    "# Administrative code to set the path for file loading\n",
    "#**********************************************************************\n",
    "\n",
    "path        = os.path.abspath(os.getcwd())\n",
    "TESTDIR     = os.path.normpath(os.path.join(os.path.expanduser(\"~\"),\"Desktop\",\"KDD_15\",\"docs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Next we iterate over the files in the directory and store those names in the empty list we created called *files*.  We time the operation, print list with the file names and also print out the length of the list (gives number of target files).<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 15 µs\n",
      "\n",
      "253\n",
      "\n",
      "[p1.pdf, p1005.pdf, p1015.pdf, p1025.pdf, p1035.pdf, p1045.pdf, p1055.pdf, p1065.pdf, p1075.pdf, p1085.pdf, p109.pdf, p1095.pdf, p1105.pdf, p1115.pdf, p1125.pdf, p1135.pdf, p1145.pdf, p1155.pdf, p1165.pdf, p1175.pdf, p1185.pdf, p119.pdf, p1195.pdf, p1205.pdf, p1215.pdf, p1225.pdf, p1235.pdf, p1245.pdf, p1255.pdf, p1265.pdf, p1275.pdf, p1285.pdf, p129.pdf, p1295.pdf, p1305.pdf, p1315.pdf, p1325.pdf, p1335.pdf, p1345.pdf, p1355.pdf, p1365.pdf, p1375.pdf, p1385.pdf, p139.pdf, p1395.pdf, p1405.pdf, p1415.pdf, p1425.pdf, p1435.pdf, p1445.pdf, p1455.pdf, p1465.pdf, p1475.pdf, p1485.pdf, p149.pdf, p1495.pdf, p1503.pdf, p1513.pdf, p1523.pdf, p1533.pdf, p1543.pdf, p1553.pdf, p1563.pdf, p1573.pdf, p1583.pdf, p159.pdf, p1593.pdf, p1603.pdf, p1621.pdf, p1623.pdf, p1625.pdf, p1627.pdf, p1629.pdf, p1631.pdf, p1633.pdf, p1635.pdf, p1637.pdf, p1639.pdf, p1641.pdf, p1651.pdf, p1661.pdf, p1671.pdf, p1681.pdf, p169.pdf, p1691.pdf, p1701.pdf, p1711.pdf, p1721.pdf, p1731.pdf, p1741.pdf, p1751.pdf, p1759.pdf, p1769.pdf, p1779.pdf, p1789.pdf, p179.pdf, p1799.pdf, p1809.pdf, p1819.pdf, p1829.pdf, p1839.pdf, p1849.pdf, p1859.pdf, p1869.pdf, p1879.pdf, p1889.pdf, p189.pdf, p1899.pdf, p19.pdf, p1909.pdf, p1919.pdf, p1929.pdf, p1939.pdf, p1949.pdf, p1959.pdf, p1969.pdf, p1979.pdf, p1989.pdf, p199.pdf, p1999.pdf, p2009.pdf, p2019.pdf, p2029.pdf, p2039.pdf, p2049.pdf, p2059.pdf, p2069.pdf, p2079.pdf, p2089.pdf, p209.pdf, p2099.pdf, p2109.pdf, p2119.pdf, p2127.pdf, p2137.pdf, p2147.pdf, p2157.pdf, p2167.pdf, p2177.pdf, p2187.pdf, p219.pdf, p2197.pdf, p2207.pdf, p2217.pdf, p2227.pdf, p2237.pdf, p2247.pdf, p2257.pdf, p2267.pdf, p2277.pdf, p2287.pdf, p229.pdf, p2297.pdf, p2307.pdf, p2309.pdf, p2311.pdf, p2313.pdf, p2315.pdf, p2317.pdf, p2319.pdf, p2321.pdf, p2323.pdf, p2325.pdf, p2327.pdf, p2329.pdf, p239.pdf, p249.pdf, p259.pdf, p269.pdf, p279.pdf, p289.pdf, p29.pdf, p299.pdf, p3.pdf, p309.pdf, p319.pdf, p329.pdf, p339.pdf, p349.pdf, p359.pdf, p369.pdf, p379.pdf, p387.pdf, p39.pdf, p397.pdf, p407.pdf, p417.pdf, p427.pdf, p437.pdf, p447.pdf, p457.pdf, p467.pdf, p477.pdf, p487.pdf, p49.pdf, p497.pdf, p5.pdf, p507.pdf, p517.pdf, p527.pdf, p537.pdf, p547.pdf, p557.pdf, p567.pdf, p577.pdf, p587.pdf, p59.pdf, p597.pdf, p607.pdf, p617.pdf, p627.pdf, p635.pdf, p645.pdf, p655.pdf, p665.pdf, p675.pdf, p685.pdf, p69.pdf, p695.pdf, p7.pdf, p705.pdf, p715.pdf, p725.pdf, p735.pdf, p745.pdf, p755.pdf, p765.pdf, p775.pdf, p785.pdf, p79.pdf, p805.pdf, p815.pdf, p825.pdf, p835.pdf, p845.pdf, p855.pdf, p865.pdf, p875.pdf, p885.pdf, p89.pdf, p895.pdf, p9.pdf, p905.pdf, p915.pdf, p925.pdf, p935.pdf, p945.pdf, p955.pdf, p965.pdf, p975.pdf, p985.pdf, p99.pdf, p995.pdf]\n"
     ]
    }
   ],
   "source": [
    "# Establish an empty list to append filenames as we iterate over the directory with filenames\n",
    "files = []\n",
    "\n",
    "%time\n",
    "start_time = time.time()\n",
    "\n",
    "#**********************************************************************\n",
    "# Core \"workerbee\" code for this section to iterate over directory files\n",
    "#**********************************************************************\n",
    "\n",
    "# Iterate over the directory of filenames and add to list.  Inspection shows our target filenames begin with 'p' and end with 'pdf'\n",
    "for dirName, subdirList, fileList in os.walk(TESTDIR):\n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            files.append(fileName)\n",
    "end_time = time.time()\n",
    "\n",
    "#**********************************************************************\n",
    "# Output\n",
    "#**********************************************************************\n",
    "print\n",
    "print len(files) # Print the number of files\n",
    "print \n",
    "print '[%s]' % ', '.join(map(str, files)) # print the list of filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>There are 253 total files in the directory. We examine the pdf file in its rawest form to get an idea of the format. Here is one example:<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"./files/journalscreencap.png\" alt=\"Sample of Journal Format\" height=\"700\" width=\"700\" top:\"35\" left:\"170\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br><br>We learn a few things immediately. Our data is in PDF format and it's semistructured (follows journal article format with sections like \"abstract\", \"title\").  PDFs are a wonderful human readable presentation of data. But for data analyisis, they are extremely difficult to work with.  If you have an option to get the data BEFORE it was converted to or added to PDF, go for that option.  If it's your only option, be prepared for a lot of these moments:\n",
    "\n",
    "![Pulling hair out](http://i1012.photobucket.com/albums/af243/njmike731/man-pulling-hair-out-2-773892-1.jpg)\n",
    "\n",
    "In today's exercise, we have no alternatives outside of the web scraping code linked above.  In full disclosure, that code is imperfect because we get an incomplete dataset.  The abstracts and authors are not matched to the papers and we don't pull in the references section. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion: Stripping text from PDFs and creating a custom NLTK corpus\n",
    "\n",
    "The first step in the <href id=\"pipe\"><a href=\"#pipeline\" title=\"Jump back to data science pipeline graphic.\">data science pipeline</a> is to ingest our data.  We use several Python tools which include:\n",
    "\n",
    "* [pdfminer](https://pypi.python.org/pypi/pdfminer/) - this is the tool that makes it ALL happen.  It has a command line tool called \"pdf2text.py\" that extract text contents from a PDF. **This must be installed on your computer BEFORE executing this code**.  Visit the [pdfminer homepage](http://euske.github.io/pdfminer/index.html#pdf2txt) for instructions\n",
    "\n",
    "* [subprocess](https://docs.python.org/2/library/subprocess.html) - a standard library module that allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes.  In this excerise, we use it to invoke the pdf2texy.py command line tool within our code.  \n",
    "\n",
    "* [nltk](http://www.nltk.org/) - another work horse in this exercise.  The Natural Language ToolKit (NLTK) is one of Python's leading platforms to analyze natural language data.  The [NLTK Book](http://www.nltk.org/book/) provides practical guidance on how to handle just about any natural language preprocessing job.  \n",
    "\n",
    "* [string](https://docs.python.org/2/library/string.html) - used for variable substitutions and value formatting to strip non printable characters from the output of the text extracted from our journal article PDFs\n",
    "\n",
    "* [unicodedata](https://docs.python.org/2/library/unicodedata.html) - some unicode characters won't extract nicely. This library allows latin unicode characters to degrade gracefully into ASCII.\n",
    "\n",
    "We are now going to iterate over each file in our raw data directory, strip the text, and write the *.txt* file to newly created directory.  Then we will follow the instructions from [Section 1.9, Chapter 2 of NLTK's Book](http://www.nltk.org/book/ch02.html) to build a custom corpus from our text files.  Having our target documents loaded as an NLTK corpus brings the power of NLTK to our analysis goals.  Let's begin with administrative tasks such as loading modules and creating the necessary directories.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Importing what we need\n",
    "#**********************************************************************\n",
    "import string\n",
    "import unicodedata\n",
    "import subprocess\n",
    "import nltk\n",
    "import os, os.path\n",
    "import re\n",
    "\n",
    "#**********************************************************************\n",
    "# Create the directory we will write the .txt files to after stripping text\n",
    "#**********************************************************************\n",
    "\n",
    "corpuspath = os.path.normpath(os.path.expanduser('~/Desktop/KDD_corpus/'))\n",
    "if not os.path.exists(corpuspath):\n",
    "    os.mkdir(corpuspath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Now we are to the big task of stripping text from the PDFs.  In the code below, we walk down the directory, and strip text from the files with names that begin with 'p' and end with 'pdf'.  We use the *fileName* variable to name the files we write to disk.  This will come in handy when we load data into NLTK.  Keep in mind, this task takes the longest, so be prepared to wait a a few minutes depending on good your computer is.  If you are doing this in an environment where you can spin up compute resources, your time will be drastically reduced.  Let's begin.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Core code to iterate over files in the directory\n",
    "#**********************************************************************\n",
    "\n",
    "# We start from the code to iterate over the files\n",
    "%timeit\n",
    "for dirName, subdirList, fileList in os.walk(TESTDIR):\n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            if os.path.exists(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\"))):\n",
    "                pass\n",
    "            else:\n",
    "            \n",
    "            \n",
    "#**********************************************************************\n",
    "# This code strips the text from the PDFs\n",
    "#**********************************************************************\n",
    "                try:\n",
    "                    document = filter(lambda x: x in string.printable,unicodedata.normalize('NFKD', (unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore'))).encode('ascii','ignore').decode('unicode_escape').encode('ascii','ignore'))\n",
    "                except UnicodeDecodeError:\n",
    "                    document = unicodedata.normalize('NFKD', unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore')).encode('ascii','ignore')    \n",
    "\n",
    "                if len(document)<300:\n",
    "                    pass\n",
    "                else:\n",
    "                    # used this for assistance http://stackoverflow.com/questions/2967194/open-in-python-does-not-create-a-file-if-it-doesnt-exist\n",
    "                    if not os.path.exists(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\"))):\n",
    "                        file = open(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\")), 'w+')\n",
    "                        file.write(document)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "kddcorpus= nltk.corpus.PlaintextCorpusReader(corpuspath, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kddcorpus= nltk.corpus.PlaintextCorpusReader(corpuspath, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>This is a pretty big step.  We have a semi-structured data set in a format where we can query and analyze different pieces of data.  All of our data is loaded as an NLTK corpus, meaning we could try tons of techniques outlined in the [NLTK book](http://www.nltk.org/book/) or use the NLTK APIs to pass data into [scikit-learn machine learning pipelines for text](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) (maybe for a later blog). Let's see how many words (including stop words) we have in our entire corpus. This is a part of exploring our dataset, as laid out in a great book on Data Science<sup><a href=\"#fn4\" id=\"ref4\">4</a></sup>  <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2795267\n"
     ]
    }
   ],
   "source": [
    "wordcount = 0\n",
    "for fileid in kddcorpus.fileids():\n",
    "    wordcount += len(kddcorpus.words(fileid))\n",
    "print wordcount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step didn't come off without it's errors.  We got a little bit of gobbledygook (that is a [real word](http://www.merriam-webster.com/dictionary/gobbledygook) by the way). Here are the first 1000 characters of document 2157:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")UDX'HWHFWRU\u001d",
      "\u0003$\u0003*UDSK\u00100LQLQJ\u0010EDVHG\u0003)UDPHZRUN\u0003\u0003\n",
      "\n",
      "IRU\u0003)UDXGXOHQW\u00033KRQH\u0003&DOO\u0003'HWHFWLRQ\u0003\n",
      "\n",
      "9LQFHQW\u00036\u0011\u00037VHQJ\u0014\r",
      "\u000f\u0003-RVK\u0003-LD\u0010&KLQJ\u0003<LQJ\u0014\u000f\u0003&KH\u0010:HL\u0003+XDQJ\u0015\u000f\u0003<LPLQ\u0003.DR\u0016\u000f\u0003DQG\u0003.XDQ\u00107D\u0003&KHQ\u0017\u0003\n",
      "\n",
      "\u0014\u0003'HSDUWPHQW\u0003RI\u0003&RPSXWHU\u00036FLHQFH\u000f\u00031DWLRQDO\u0003&KLDR\u00037XQJ\u00038QLYHUVLW\\\u000f\u00037DLZDQ\u000f\u000352&\u0003\n",
      "\n",
      "\u0015\u0003'HSDUWPHQW\u0003RI\u0003&RPSXWHU\u00036FLHQFH\u0003DQG\u0003,QIRUPDWLRQ\u0003(QJLQHHULQJ\u000f\u00031DWLRQDO\u0003&KHQJ\u0003.XQJ\u00038QLYHUVLW\\\u000f\u00037DLZDQ\u000f\u000352&\u0003\n",
      "\n",
      "\u0016\u0003*RJRORRN\u0003&R\u0011\u0003/WG\u0011\u000f\u00037DLZDQ\u000f\u000352&\u0003\n",
      "\n",
      "\u0017\u0003,QVWLWXWH\u0003RI\u0003,QIRUPDWLRQ\u00036FLHQFH\u000f\u0003$FDGHPLD\u00036LQLFD\u000f\u00037DLZDQ\u000f\u000352&\u0003\n",
      "\n",
      "MDVK\\LQJ#JPDLO\u0011FRP\u000f\u0003ZHLLER\\#LGE\u0011FVLH\u0011QFNX\u0011HGX\u0011WZ\u000f\u0003\\LPLQNDR#JRJRORRN\u0011FRP\u000f\u0003VZF#LLV\u0011VLQLFD\u0011HGX\u0011WZ\u0003\n",
      "\n",
      "\r",
      "&RUUHVSRQGHQFH\u001d",
      " YWVHQJ#FV\u0011QFWX\u0011HGX\u0011WZ\u000f\u0003\n",
      "\n",
      "$%675$&7\u0003\n",
      ",Q\u0003UHFHQW\u0003\\HDUV\u000f\u0003IUDXG\u0003LV\u0003LQFUHDVLQJ\u0003UDSLGO\\\u0003ZLWK\u0003WKH\u0003GHYHORSPHQW\u0003RI\u0003\n",
      "PRGHUQ\u0003 WHFKQRORJ\\\u0003 DQG\u0003 JOREDO\u0003 FRPPXQLFDWLRQ\u0011\u0003 $OWKRXJK\u0003 PDQ\\\u0003\n",
      "OLWHUDWXUHV\u0003 KDYH\u0003 DGGUHVVHG\u0003 WKH\u0003 IUDXG\u0003 GHWHFWLRQ\u0003 SUREOHP\u000f\u0003 WKHVH\u0003\n",
      "H[LVWLQJ\u0003 ZRUNV\u0003 IRFXV\u0003 RQO\\\u0003 RQ\u0003 IRUPXODWLQJ\u0003 WKH\u0003 IUDXG\u0003 GHWHFWLRQ\u0003\n",
      "SUREOHP\u0003 DV\u0003 D\u0003 ELQDU\\\u0003 FODVVLILFDWLRQ\u0003 SUREOHP\u0011\u0003 'XH\u0003 WR\u0003 OLPLWDWLRQ\u0003 RI\u0003\n",
      "LQIRUPDWLRQ\u0003SURYLGHG\u0003E\\\u0003WHOHFRPPXQL\n"
     ]
    }
   ],
   "source": [
    "print kddcorpus.raw(\"p2157.txt\")[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>The NLTK book has an [excellent section on processing raw text and unicode issues](http://www.nltk.org/book/ch03.html#fig-unicode). I could never figure out what caused the error above but that's a dose of real world data problems.   Let's move on.  To begin our exploration of regular expressions (aka \"regex\"), it's important to point out some good resources to brush up on the topic.  The best resource I ever had was in [Videos 1-3, Week 4, Getting and Cleaning Data, Data Science Specialization Track](https://www.coursera.org/learn/data-cleaning) (At Coursera by Johns Hopkins University).  The instruction and examples in these helped me UNDERSTAND how to use regex vice googling [\"how to match text between two strings python regex\"](https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=how+to+match+text+between+two+strings+python+regex) and hacking away until getting the desired output.  When you understand regex, you will start to use metacharacter expression matches vice using literal matches, and crush any text matching requirment.  Here are some learning resources listed in my own subjective order of usefulness and relevance to python:\n",
    "* http://regexone.com/ (interactive teaching)\n",
    "* https://regex101.com/ (interactive testing; you can paste your text and test expressions)\n",
    "* http://regexr.com/ (interactive testing like above)\n",
    "* http://www.learnpython.org/en/Regular_Expressions (not very intuitive at first glimpse, but useful)\n",
    "* https://docs.python.org/2/library/re.html (default Python library documentation on regex)\n",
    "\n",
    "<br>As a quick test, we extract some \"good enough\" titles from the first 26 documents. I say \"good enough\" because some author names get caught up int he extractions below.  <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online Controlled Experiments: Lessons from Running A/B/n Tests for 12 Years\n",
      "Mining Frequent Itemsets through Progressive Sampling with Rademacher Averages\n",
      "Why It Happened: Identifying and Modeling the Reasons of the Happening of Social Events\n",
      "Matrix Completion with Queries Natali Ruchansky\n",
      "Stochastic Divergence Minimization for Online Collapsed Variational Bayes Zero Inference\n",
      "Bayesian Poisson Tensor Factorization for Inferring Multilateral Relations from Sparse Dyadic Event Counts\n",
      "TimeCrunch: Interpretable Dynamic Graph Summarization Neil Shah\n",
      "Inside Jokes: Identifying Humorous Cartoon Captions Dafna Shahaf\n",
      "Community Detection based on Distance Dynamics Junming Shao\n",
      "Discovery of Meaningful Rules in Time Series Mohammad Shokoohi-Yekta    Yanping Chen    Bilson Campana    Bing Hu\n",
      "On the Formation of Circles in Co-authorship Networks Tanmoy Chakraborty1, Sikhar Patranabis2, Pawan Goyal3, Animesh Mukherjee4\n",
      "An Evaluation of Parallel Eccentricity Estimation Algorithms on Undirected Real-World Graphs\n",
      "Efcient Latent Link Recommendation in Signed Networks\n",
      "Turn Waste into Wealth: On Simultaneous Clustering and Cleaning over Dirty Data\n",
      "Set Cover at Web Scale Stergios Stergiou\n",
      "Exploiting Relevance Feedback in Knowledge Graph Search\n",
      "LINKAGE: An Approach for Comprehensive Risk Prediction for Care Management\n",
      "Transitive Transfer Learning Ben Tan\n",
      "PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks\n",
      "An Effective Marketing Strategy for Revenue Maximization with a Quantity Constraint\n",
      "Scaling Up Stochastic Dual Coordinate Ascent Kenneth Tran\n",
      "Heterogeneous Network Embedding via Deep Architectures\n",
      "Discovering Valuable Items from Massive Data Hastagiri P Vanchinathan\n",
      "Deep Learning Architecture with Dynamically Programmed Layers for Brain Connectome Prediction\n",
      "Incorporating World Knowledge to Document Clustering via Heterogeneous Information Networks\n"
     ]
    }
   ],
   "source": [
    "# This title extraction is probably unnecessarily complex, but it gets the job done; we make use of the metacharacters vice literal matches\n",
    "\n",
    "p=re.compile('^(.*)([\\s]){2}[A-z]+[\\s]+[\\s]?.+')# matches text, starting from beginning of line, followed by at least two\n",
    "for fileid in kddcorpus.fileids()[:25]:\n",
    "    print re.search('^(.*)[\\s]+[\\s]?(.*)?',kddcorpus.raw(fileid)).group(1).strip()+\" \"+re.search('^(.*)[\\s]+[\\s]?(.*)?',kddcorpus.raw(fileid)).group(2).strip()\n",
    "      # use .strip() to remove whitespace from beginning and end of string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling and computation: Using Regular Expressions to extract specific sections of the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in <href id=\"pipe\"><a href=\"#pipeline\" title=\"Jump back to data science pipeline graphic.\">data science pipeline</a> is the most time consuming; data wrangling.  For simplicity, let's focus wrangling the data so we can use the NERC on two sections of the paper:\n",
    "* the top section which includes authors and schools; this is all text above the abstract\n",
    "* the references section of the paper (keynote speaker abstracts do not have an abstract)\n",
    "\n",
    "The tools of choice to extract sections are the [\"positive lookbehind\" and \"positive lookahead\"](https://docs.python.org/2/library/re.html) expressions. Here is an example of code to extract the abstract only:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The collapsed variational Bayes zero (CVB0) inference is a vari- ational inference improved by marginalizing out parameters, the same as with the collapsed Gibbs sampler. A drawback of the CVB0 inference is the memory requirements. A probability vec- tor must be maintained for latent topics for every token in a corpus. When the total number of tokens is N and the number of topics is K, the CVB0 inference requires O(N K) memory. A stochas- tic approximation of the CVB0 (SCVB0) inference can reduce O(N K) to O(V K), where V denotes the vocabulary size. We re- formulate the existing SCVB0 inference by using the stochastic di- vergence minimization algorithm, with which convergence can be analyzed in terms of Martingale convergence theory. We also reveal the property of the CVB0 inference in terms of the leave-one-out perplexity, which leads to the estimation algorithm of the Dirichlet distribution parameters. The predictive performance of the propose SCVB0 inference is better than that of the original SCVB0 infer- ence in four datasets.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set our regular expression\n",
    "p= re.compile('(?<=ABSTRACT)(.+)(?=Categories and Subject Descriptors)')\n",
    "try:\n",
    "    abstract= p.search(re.sub('[\\s]',\" \",kddcorpus.raw('p1035.txt'))).group(1)\n",
    "except AttributeError:\n",
    "    # include a lowercase regex match incase consistency is a problem\n",
    "    p=re.compile('(?<=abstract)(.+)(?=categories and subject descriptors)')\n",
    "    abstract=p.search(re.sub('[\\s]',\" \",holder.lower())).group(1)\n",
    "else:\n",
    "    pass\n",
    "unicodedata.normalize('NFKD', abstract).encode('ascii','ignore').strip() # convert output from unicode to string and strip leading and trailing whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!  Now, to be \"pythonic\" we build two functions that can extract the top and references section of the documents.  For fun, I also made other function to extract the keywords and abstract sections of the documents.  We could do the same for any section of paper although I must provide a warning.  **Working with natural language is a messy ordeal!**  This is a top notch organization (ACM) and a top notch conference (KDD) but human error sitll makes it way into the picture:\n",
    "\n",
    "![Human Error](http://www.process-improvement-institute.com/wp-content/uploads/2015/05/Accounting-for-Human-Error-Probability-in-SIL-Verification.jpg)\n",
    "\n",
    "Specifically in our case:\n",
    "* paper 1 header section = \"Categories and Subject Descriptors\"\n",
    "* paper 2 header section = \"Categories & Subject Descriptors\"\n",
    "\n",
    "Very small difference but these types of differences cause TONS of headaches.  The result?  You have a decision to make: **account for these differences or ignore them**.  I worked to include AS MUCH of the 253 corpus as possible in the results but it's never perfect.  There are also some documents that will be missing sections altogether (i.e. keynote speaker documents do not have a references section.  Our two functions will:\n",
    "\n",
    "1. Extract only the relevant text for the section we seek\n",
    "2. Extract a character count for the section\n",
    "3. Make additonal calculations or extractions\n",
    "  * the top section extraction also extract emails\n",
    "  * we count the number of references and store that value\n",
    "  * as added benefit, we create a simple \"word per reference\" calculation\n",
    "4. Store all the above data as a nested dictionary with the filename as a key\n",
    "\n",
    "These are loooooong blocks of code to accomplish the task above.  For now, we will only show the code to extract the references and perform the quick analysis mentioned above.  The other functions will be in the appendix.  In fairness, all functions could be reduced down to one function composed of nested function calls.  We will save that for later and get the \"functionality\" working before optimizing the code. See the comments below to follow along or just skip to the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code to pull the ferences section only, store a character count, number of references, and \"word per reference\" calculation\n",
    "\n",
    "def refpull(docnum=None,section='references',full = False):\n",
    "    \n",
    "    # Establish an empty dictionary to hold values\n",
    "    ans={}\n",
    "    \n",
    "    # Establish an empty list to hold document ids that don't make the cut (i.e. missing reference section or different format)\n",
    "    # This comes in handy when you are trying to improve your code to catch outliers\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    \n",
    "    # Admin code to set default values and raise an exception if there's human error on input\n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        # Setting the target document and the text we will extract from \n",
    "        text=kddcorpus.raw(docnum)\n",
    "        \n",
    "        \n",
    "        # This first condtional is for pulling the target section for ALL documents in the corpus\n",
    "        if full == True:\n",
    "            \n",
    "            # Iterate over the corpus to get the id; this is possible from loading our docs into a custom NLTK corpus\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid)\n",
    "                \n",
    "                # These lines of code build our regular expression.\n",
    "                # In the other functions for abstract or keywords, you see how I use this technique to create different regex arugments\n",
    "                if section == \"references\":\n",
    "                    section1=[\"REFERENCES\"] \n",
    "                    \n",
    "                    # Just in case, making sure our target string is empty before we pass data into it; just a check\n",
    "                    target = \"\"   \n",
    "\n",
    "                    #We now build our lists iteratively to build our regex\n",
    "                    for sect in section1:\n",
    "                        \n",
    "                        # We embed exceptions to remove the possibility of our code stopping; we pass failed passes into a list\n",
    "                        try:\n",
    "                            \n",
    "                            # our machine built regex\n",
    "                            part1= \"(?<=\"+sect+\")(.+)\"\n",
    "                            p=re.compile(part1)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                            \n",
    "                            # Conditoin to make sure we don't get any empty string\n",
    "                            if len(target) > 50:\n",
    "\n",
    "                                # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                                try:\n",
    "                                    refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                                except:\n",
    "                                    print \"This file does not appear to have a references section\"\n",
    "                                    pass\n",
    "                                \n",
    "                                #These are all our values; we build a nested dictonary and store the calculated values\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"references\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                ans[str(fileid)][\"refcount\"]= refnum\n",
    "                                ans[str(fileid)][\"wordperRef\"]=round(float(len(nltk.word_tokenize(text)))/float(refnum))\n",
    "                                #print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                                break\n",
    "                            else:\n",
    "\n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "\n",
    "            return ans\n",
    "            return failids\n",
    "                              \n",
    "        # This is to perform the same operations on just one document; same functionality as above.\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum)\n",
    "        \n",
    "        if section == \"references\":\n",
    "            section1=[\"REFERENCES\"] \n",
    "            target = \"\"   \n",
    "            for sect in section1:\n",
    "                try:\n",
    "                    part1= \"(?<=\"+sect+\")(.+)\"\n",
    "                    p=re.compile(part1)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    if len(target) > 50:\n",
    "                        # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                        try:\n",
    "                            refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                        except:\n",
    "                            print \"This file does not appear to have a references section\"\n",
    "                            pass\n",
    "                        ans[str(docnum)]={}\n",
    "                        ans[str(docnum)][\"references\"]=target.strip()\n",
    "                        ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                        ans[str(docnum)][\"refcount\"]= refnum\n",
    "                        ans[str(docnum)][\"wordperRef\"]=float(len(nltk.word_tokenize(text)))/float(refnum)\n",
    "\n",
    "\n",
    "                        #print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                        break\n",
    "                    else:\n",
    "\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(docnum)\n",
    "                    pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        return ans\n",
    "        return failids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a big block of code!  Don't fret, there are several similar blocks in the appendix to extract the abstract and keywords.  Data is messy; this is what cleaning looks like.  In the code above, we also make use of the *nltk.word_tokenize* tool to create the \"word per reference\" figure.  Let's test our function and some output (the word_tokenize calculation will take some time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# call our function, setting \"full=True\" extracts ALL references in corpus\n",
    "test = refpull(full=True)\n",
    "\n",
    "# To get a quick glimpse, I use the example from this page: http://stackoverflow.com/questions/7971618/python-return-first-n-keyvalue-pairs-from-dict\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "man = collections.OrderedDict(test)\n",
    "\n",
    "x = itertools.islice(man.items(), 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename      Character Count    Number of references    Words per Reference\n",
      "----------  -----------------  ----------------------  ---------------------\n",
      "p2277.txt                6295                      33                    326\n",
      "p835.txt                 5347                      38                    319\n",
      "p865.txt                 5269                      27                    399\n",
      "p2089.txt                8734                      45                    181\n",
      "p1759.txt                3677                      31                    405\n",
      "p29.txt                  5101                      40                    265\n",
      "p2227.txt               10345                      36                    332\n",
      "p2099.txt                3949                      28                    374\n",
      "p725.txt                 5771                      37                    304\n",
      "p2019.txt                9101                      60                    171\n"
     ]
    }
   ],
   "source": [
    "# Let's use a nifty table module to print this all pretty like: https://pypi.python.org/pypi/tabulate\n",
    "# The joy of Python and open source: someone has created something to do what you want; Google is your friend.  \n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "# A quick list comprehension to follow the example on the tabulate pypi page\n",
    "table = [[key,value['charcount'],value['refcount'], value['wordperRef']] for key,value in x]\n",
    "\n",
    "# print the pretty table; we invoke the \"header\" argument and assign custom header!!!!\n",
    "print tabulate(table,headers=[\"filename\",\"Character Count\", \"Number of references\",\"Words per Reference\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation,analyses, modeling, application: Using NERC tools and examining for accuracy\n",
    "\n",
    "Finally, we are in <href id=\"pipe\"><a href=\"#pipeline\" title=\"Jump back to data science pipeline graphic.\">data science pipeline</a> steps where data scientists WANT to live: computation, analyses, modeling and application!!!   Notice I've mixed the next two steps because we are using models, applying them to our data, and performing analysis below.\n",
    "\n",
    "We can now test how well some open source NERC tools extract entities from the top and reference sections of our corpus.  By top section, I am referring to all text that occurs before the keyFor comparison, I went through and hand labled entities in two documents.  Hand labeling is an expensive and tedious process.  For two documents, we have the hand labeled authors, organizations, and locations from the top section of the article.  Second, there is a list of all authors from the references section and finally, a list of all combined authors (person entities) in the top and reference section combined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 authors\n",
      "\n",
      "There are 3 author organizations\n",
      "\n",
      "There are 3 author locations\n",
      "\n",
      "There are 152 authors in the references\n"
     ]
    }
   ],
   "source": [
    "# filename p19.txt\n",
    "\n",
    "p19pdf_authors=['Tim Althoff','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang']\n",
    "p19pdf_author_organizations=['Computer Science Department','Stanford University','Google']\n",
    "p19pdf_author_locations=['Stanford, CA','1600 Amphitheatre Parkway, Mountain View, CA 94043','Mountain View']\n",
    "\n",
    "p19pdf_references_authors =['A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']\n",
    "\n",
    "p19pdf_allauthors=['Tim Althoff','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang','A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']\n",
    "\n",
    "print \"There are %r authors\" % len(p19pdf_authors)\n",
    "print  # white space\n",
    "print \"There are %r author organizations\" %len(p19pdf_author_organizations)\n",
    "print \n",
    "print \"There are %r author locations\" % len(p19pdf_author_locations)\n",
    "print  \n",
    "print \"There are %r authors in the references\" %len(p19pdf_references_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 authors\n",
      "\n",
      "There are 6 author organizations\n",
      "\n",
      "There are 8 author locations\n",
      "\n",
      "There are 106 authors in the references\n"
     ]
    }
   ],
   "source": [
    "# filename p29.txt\n",
    "\n",
    "p29pdf_authors=['Laurent Amsaleg','Stéphane Girard','Oussama Chelly','Teddy Furon','Michael E. Houle','Ken-ichi Kawarabayashi',\n",
    "               'Michael Nett']\n",
    "p29pdf_author_organizations=['Equipe LINKMEDIA','Campus Universitaire de Beaulieu','CNRS/IRISA Rennes','National Institute of Informatics',\n",
    "                             'Equipe MISTIS INRIA','Google']\n",
    "p29pdf_author_locations=['Campus Universitaire de Beaulieu','35042 Rennes Cedex, France','France','-1-2 Hitotsubashi, Chiyoda-ku Tokyo 101-8430, Japan',\n",
    "                        'Japan','6-10-1 Roppongi, Minato-ku Tokyo 106-6126','Inovallée, 655, Montbonnot 38334 Saint-Ismier Cedex','Tokyo']\n",
    "\n",
    "p29pdf_references_authors =['A. A. Balkema','L. de Haan','N. Bingham', 'C. Goldie','J. Teugels','N. Boujemaa', 'J. Fauqueur', 'M. Ferecatu', 'F. Fleuret',\n",
    "                            'V. Gouet', 'B. LeSaux','H. Sahbi','C. Bouveyron', 'G. Celeux', 'S. Girard','J. Bruske', 'G. Sommer',\n",
    "                           'F. Camastra','A. Vinciarelli','S. Coles','J. Costa' ,'A. Hero','T. de Vries', 'S. Chawla','M. E. Houle',\n",
    "                           'R. A. Fisher','L. H. C. Tippett','M. I. Fraga Alves', 'L. de Haan','T. Lin','M. I. Fraga Alves', 'M. I. Gomes','L. de Haan',\n",
    "                           'B. V. Gnedenko',' A. Gupta', 'R. Krauthgamer','J. R. Lee','A. Gupta', 'R. Krauthgamer','J. R. Lee','M. Hein','J.-Y. Audibert',\n",
    "                           'B. M. Hill','M. E. Houle','M. E. Houle','M. E. Houle','M. E. Houle', 'H. Kashima', 'M. Nett','M. E. Houle', 'X. Ma', 'M. Nett',\n",
    "                            'V. Oria','M. E. Houle', 'X. Ma', 'V. Oria','J. Sun','M. E. Houle','M. Nett','H. Jegou', 'R. Tavenard', 'M. Douze','L. Amsaleg',\n",
    "                           'I. Jollie','D. R. Karger','M. Ruhl','J. Karhunen','J. Joutsensalo','Y. LeCun', 'L. Bottou', 'Y. Bengio', 'P. Haner',\n",
    "                           'J. Pickands, III','C. R. Rao','S. T. Roweis','L. K. Saul','A. Rozza', 'G. Lombardi', 'C. Ceruti', 'E. Casiraghi', 'P. Campadelli',\n",
    "                           'B. Scholkopf', 'A. J. Smola','K.-R. Muller','U. Shaft','R. Ramakrishnan',' F. Takens','J. Tenenbaum', 'V. D. Silva','J. Langford',\n",
    "                           'J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. Venna','S. Kaski',\n",
    "                           'P. Verveer','R. Duin','J. von Brunken', 'M. E. Houle', 'A. Zimek','J. von Brunken', 'M. E. Houle','A. Zimek']\n",
    "\n",
    "p29pdf_allauthors=['Laurent Amsaleg','Stéphane Girard','Oussama Chelly','Teddy Furon','Michael E. Houle','Ken-ichi Kawarabayashi',\n",
    "               'Michael Nett','A. A. Balkema','L. de Haan','N. Bingham', 'C. Goldie','J. Teugels','N. Boujemaa', 'J. Fauqueur', 'M. Ferecatu', 'F. Fleuret',\n",
    "                            'V. Gouet', 'B. LeSaux','H. Sahbi','C. Bouveyron', 'G. Celeux', 'S. Girard','J. Bruske', 'G. Sommer',\n",
    "                           'F. Camastra','A. Vinciarelli','S. Coles','J. Costa' ,'A. Hero','T. de Vries', 'S. Chawla','M. E. Houle',\n",
    "                           'R. A. Fisher','L. H. C. Tippett','M. I. Fraga Alves', 'L. de Haan','T. Lin','M. I. Fraga Alves', 'M. I. Gomes','L. de Haan',\n",
    "                           'B. V. Gnedenko',' A. Gupta', 'R. Krauthgamer','J. R. Lee','A. Gupta', 'R. Krauthgamer','J. R. Lee','M. Hein','J.-Y. Audibert',\n",
    "                           'B. M. Hill','M. E. Houle','M. E. Houle','M. E. Houle','M. E. Houle', 'H. Kashima', 'M. Nett','M. E. Houle', 'X. Ma', 'M. Nett',\n",
    "                            'V. Oria','M. E. Houle', 'X. Ma', 'V. Oria','J. Sun','M. E. Houle','M. Nett','H. Jegou', 'R. Tavenard', 'M. Douze','L. Amsaleg',\n",
    "                           'I. Jollie','D. R. Karger','M. Ruhl','J. Karhunen','J. Joutsensalo','Y. LeCun', 'L. Bottou', 'Y. Bengio', 'P. Haner',\n",
    "                           'J. Pickands, III','C. R. Rao','S. T. Roweis','L. K. Saul','A. Rozza', 'G. Lombardi', 'C. Ceruti', 'E. Casiraghi', 'P. Campadelli',\n",
    "                           'B. Scholkopf', 'A. J. Smola','K.-R. Muller','U. Shaft','R. Ramakrishnan',' F. Takens','J. Tenenbaum', 'V. D. Silva','J. Langford',\n",
    "                           'J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. Venna','S. Kaski',\n",
    "                           'P. Verveer','R. Duin','J. von Brunken', 'M. E. Houle', 'A. Zimek','J. von Brunken', 'M. E. Houle','A. Zimek']\n",
    "\n",
    "\n",
    "print \"There are %r authors\" % len(p29pdf_authors)\n",
    "print  # white space\n",
    "print \"There are %r author organizations\" %len(p29pdf_author_organizations)\n",
    "print \n",
    "print \"There are %r author locations\" % len(p29pdf_author_locations)\n",
    "print  \n",
    "print \"There are %r authors in the references\" %len(p29pdf_references_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy test for accuracy is to compare entities extracted by the NERC tools to the hand labeled extractions.  This will help us to:\n",
    "\n",
    "* Compare machice extracted list of persons to hand labeled lists\n",
    "* Compute precision, accuracy and recall\n",
    "\n",
    "We are using three open source NERC tools.  Each NERCs could be trained for improved extractions, but we are testing \"out of the box\" performance.  The tools are:\n",
    "\n",
    "1.  [NLTK has a chunk package](http://www.nltk.org/api/nltk.chunk.html) that uses NLTK’s recommended named entity chunker to chunk the given list of tagged tokens.  Following the natural language processing pipeline where a string is tokenized, and tagged with parts of speed (POS) tags, the NLTK chunker identifies non-overlapping groups and assigns them to an entity class.  Read more about NLTK's chunking capabilities in [the NLTK book](http://www.nltk.org/book/ch07.html)\n",
    "\n",
    "2. [Standard's Named Entity Recognizer](http://nlp.stanford.edu/software/CRF-NER.shtml), often called Stanford NER, is a Java implementation of linear chain Conditional Random Field (CRF) sequence models functoning as a Named Entity Recognizer. Named Entity Recognition (NER) labels sequences of words in a text which are the names of things, such as person and company names, or gene and protein names. NLTK contains an [interface to Stanford NER](http://www.nltk.org/_modules/nltk/tag/stanford.html) written by Nitin Madnani   \n",
    "\n",
    "3. [Polyglot](http://polyglot.readthedocs.org/en/latest/index.html) is natural language pipeline that supports massive multilingual (i.e. language) applications.  It supports tokenization in 165 languages, language detection in 196 languages, named entity recognition in 40 languages, part of speech tagging in 16 languages, sentiment analysis in 136 languages, word embeddings in 137 languages, morphological analysis in 135 languages, and transliteration in 69 languages.  It is a powerhouse tool for natural language processing! We will use the named entity recognition feature for English langauge in this exercise. Read Section 3 of the [Polyglot paper by Al-Rfou et al to understand how they modeled NER as a word level classification problem using an ensemble method (neural network and one vs all classifier) where backpropagation and stochastic gradient descent were used for model optimization](http://arxiv.org/pdf/1410.3791.pdf)  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beginning, we will use our functions and NLTK corpus wrangling to position our data for the NERC tool.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need the top and references sections from p19.txt and p29.txt\n",
    "\n",
    "p19={'top': toppull(\"p19.txt\")['p19.txt']['top'], 'references':refpull(\"p19.txt\")['p19.txt']['references']}\n",
    "p29={'top': toppull(\"p29.txt\")['p29.txt']['top'], 'references':refpull(\"p29.txt\")['p29.txt']['references']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>All the munging and wrangling paid off; we can access any document and pull out a section with a few lines of code. \n",
    "\n",
    "In this next block of code, we will apply the NLTK standard chunker, Stanford Named Entity Recognizer, and Polyglot extractor to our data.  The Standard NLTK chunker comes with NLTK libraries.  NLTK provides an [interface to the Stanford NERC tool](http://www.nltk.org/_modules/nltk/tag/stanford.html).  Details for [using the tool are on the NLTK page](http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford). The jar files can be downloaded [here](http://nlp.stanford.edu/software/index.shtml).  Finally, [Polyglot](https://pypi.python.org/pypi/polyglot) is available via pypi.  \n",
    "\n",
    "For each NERC tool, I made functions in the appendix to extract entities and return classes of objects in different lists.  The functions are:\n",
    "* nltktreelist -> NLTK Standard Chunker\n",
    "* get_continuous_chunks -> Stanford Named Entity Recognizer \n",
    "* extraction -> Polyglot Extraction tool <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "#  NLTK Standard Chunker\n",
    "#**********************************************************************\n",
    "nltkstandard_p19ents = {'top': nltktreelist(p19['top']),'references': nltktreelist(p19['references'])}\n",
    "nltkstandard_p29ents = {'top': nltktreelist(p29['top']),'references': nltktreelist(p29['references'])}\n",
    "\n",
    "#**********************************************************************\n",
    "# Stanford NERC Tool\n",
    "#**********************************************************************\n",
    "\n",
    "from nltk.tag import StanfordNERTagger, StanfordPOSTagger\n",
    "stner = StanfordNERTagger('/Users/linwood/stanford-corenlp-full/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "       '/Users/linwood/stanford-corenlp-full/stanford-corenlp-3.5.2.jar',\n",
    "       encoding='utf-8')\n",
    "stpos = StanfordPOSTagger('/Users/linwood/stanford-postagger-full/models/english-bidirectional-distsim.tagger','/Users/linwood/stanford-postagger-full/stanford-postagger.jar') \n",
    "\n",
    "stan_p19ents = {'top': get_continuous_chunks(p19['top']), 'references': get_continuous_chunks(p19['references'])}\n",
    "stan_p29ents = {'top': get_continuous_chunks(p29['top']), 'references': get_continuous_chunks(p29['references'])}\n",
    "\n",
    "#**********************************************************************\n",
    "# Polyglot NERC Tool\n",
    "#**********************************************************************\n",
    "\n",
    "poly_p19ents = {'top': extraction(p19['top']), 'references': extraction(p19['references'])}\n",
    "poly_p29ents = {'top': extraction(p29['top']), 'references': extraction(p29['references'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> Using the [tabulate](https://pypi.python.org/pypi/tabulate) library again, we print out the true person entities and the extractions from all the tools for comparison.  We make use of the the [sets](https://docs.python.org/2/library/sets.html) module from the standard library, specifically the *set1.intersect(set2)* functionality, to return a single set that has elements that are common to both set1 and set2. In this case, set1 is the list of entities from the hand labeled list and set2 is the list of entities extracted by the NERC tool.  Linuxtopia has an [intuitive discussion of the *set* module and its various operations](http://www.linuxtopia.org/online_books/programming_books/python_programming/python_ch16s03.html).   First, we print out a the results of our NERC tools on top section of the journal article.<br><br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hand labeled extractions from document \"p19.txt\"\n",
      "\n",
      "persons        locations                                           organizations\n",
      "-------------  --------------------------------------------------  ---------------------------\n",
      "Tim Althoff    Stanford, CA                                        Computer Science Department\n",
      "Xin Luna Dong  1600 Amphitheatre Parkway, Mountain View, CA 94043  Stanford University\n",
      "Kevin Murphy   Mountain View                                       Google\n",
      "Safa Alai\n",
      "Van Dang\n",
      "Wei Zhang\n",
      "\n",
      "\n",
      "Common persons in each list (These are the entities that matched exactly!)\n",
      "\n",
      "['Wei Zhang', 'Tim Althoff', 'Xin Luna Dong', 'Van Dang', 'Kevin Murphy', 'Safa Alai']\n",
      "\n",
      "\n",
      "Extractions from document \"p19.txt\" using NLTK Standard Chunker\n",
      "\n",
      "persons          locations    genpurp  organizations\n",
      "-------------  -----------  ---------  -------------------\n",
      "Timeline                               Generation\n",
      "Tim Althoff                            Stanford University\n",
      "Xin Luna Dong\n",
      "Kevin Murphy\n",
      "Safa Alai\n",
      "Van Dang\n",
      "Wei Zhang\n",
      "Stanford\n",
      "Mountain View\n",
      "\n",
      "\n",
      "Extractions from document \"p19.txt\" using Stanford Chunker\n",
      "\n",
      "persons          locations  organizations\n",
      "-------------  -----------  --------------------------------------\n",
      "Tim Althoff                 Wei Zhang *Computer Science Department\n",
      "Xin Luna Dong               Stanford University\n",
      "Kevin Murphy\n",
      "\n",
      "\n",
      "Extractions from document \"p19.txt\" using Polyglot Chunker\n",
      "\n",
      "persons        locations    organizations\n",
      "-------------  -----------  ---------------------------\n",
      "Tim Althoff    CA           Computer Science Department\n",
      "Xin Luna Dong  View ,       Stanford University\n",
      "Kevin Murphy   kpmurphy     Stanford\n",
      "Safa           safa\n",
      "Van Dang       vandang\n",
      "Wei Zhang\n"
     ]
    }
   ],
   "source": [
    "truth19 = {}\n",
    "truth19['persons'] = p19pdf_authors\n",
    "truth19['locations'] = p19pdf_author_locations\n",
    "truth19['organizations'] = p19pdf_author_organizations\n",
    "print \"Hand labeled extractions from document \\\"p19.txt\\\"\"\n",
    "print\n",
    "print tabulate(truth19,headers=\"keys\")\n",
    "print\n",
    "print \n",
    "print \"Common persons in each list (These are the entities that matched exactly!)\"\n",
    "print \n",
    "print list(set(nltkstandard_p19ents['top']['persons']) & set(truth19['persons']))\n",
    "print\n",
    "print\n",
    "print \"Extractions from document \\\"p19.txt\\\" using NLTK Standard Chunker\"\n",
    "print\n",
    "print tabulate(nltkstandard_p19ents['top'], headers=\"keys\")\n",
    "print\n",
    "print\n",
    "print \"Extractions from document \\\"p19.txt\\\" using Stanford Chunker\"\n",
    "print\n",
    "print tabulate(stan_p19ents['top'], headers=\"keys\")\n",
    "print \n",
    "print \n",
    "print \"Extractions from document \\\"p19.txt\\\" using Polyglot Chunker\"\n",
    "print\n",
    "print tabulate(poly_p19ents['top'], headers=\"keys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>We will focus specifically on the \"persons\" entity extractions to estimate performance from this point forward.  Feel free to try any of these scoring methods on the extractions of organizations or locations from the top or reference sections.  Moreover, the appendix has functions that can extract the abstracts and keywords from the corpus. To begin exploration of the NERC results, we will build a dataframe that places the results side by side for visual analysis<br><br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Authors</th>\n",
       "      <th>NLTKStandard NERC</th>\n",
       "      <th>Stanford NERC</th>\n",
       "      <th>Polyglot NERC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tim Althoff</td>\n",
       "      <td>Timeline</td>\n",
       "      <td>Tim Althoff</td>\n",
       "      <td>Tim Althoff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xin Luna Dong</td>\n",
       "      <td>Tim Althoff</td>\n",
       "      <td>Xin Luna Dong</td>\n",
       "      <td>Xin Luna Dong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kevin Murphy</td>\n",
       "      <td>Xin Luna Dong</td>\n",
       "      <td>Kevin Murphy</td>\n",
       "      <td>Kevin Murphy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Safa Alai</td>\n",
       "      <td>Kevin Murphy</td>\n",
       "      <td></td>\n",
       "      <td>Safa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Van Dang</td>\n",
       "      <td>Safa Alai</td>\n",
       "      <td></td>\n",
       "      <td>Van Dang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wei Zhang</td>\n",
       "      <td>Van Dang</td>\n",
       "      <td></td>\n",
       "      <td>Wei Zhang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>Wei Zhang</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>Stanford</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>Mountain View</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    True Authors NLTKStandard NERC  Stanford NERC  Polyglot NERC\n",
       "0    Tim Althoff          Timeline    Tim Althoff    Tim Althoff\n",
       "1  Xin Luna Dong       Tim Althoff  Xin Luna Dong  Xin Luna Dong\n",
       "2   Kevin Murphy     Xin Luna Dong   Kevin Murphy   Kevin Murphy\n",
       "3      Safa Alai      Kevin Murphy                          Safa\n",
       "4       Van Dang         Safa Alai                      Van Dang\n",
       "5      Wei Zhang          Van Dang                     Wei Zhang\n",
       "6                        Wei Zhang                              \n",
       "7                         Stanford                              \n",
       "8                    Mountain View                              "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#**********************************************************************\n",
    "# Administrative code, importing necessary library or module\n",
    "#**********************************************************************\n",
    "import pandas as pd\n",
    "\n",
    "#**********************************************************************\n",
    "# Create pandas series for each NERC tool entity extraction group\n",
    "#**********************************************************************\n",
    "\n",
    "df1 = pd.Series(poly_p19ents['top']['persons'], index=None, dtype=None, name='Polyglot NERC', copy=False, fastpath=False)\n",
    "df2=pd.Series(stan_p19ents['top']['persons'], index=None, dtype=None, name='Stanford NERC', copy=False, fastpath=False)\n",
    "df3=pd.Series(nltkstandard_p19ents['top']['persons'], index=None, dtype=None, name='NLTKStandard NERC', copy=False, fastpath=False)\n",
    "df4 = pd.Series(p19pdf_authors, index=None, dtype=None, name='True Authors', copy=False, fastpath=False)\n",
    "met = pd.concat([df4,df3,df2,df1], axis=1).fillna('')\n",
    "met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataframe, we see that neither of our NERC tools preformed perfectly.  NLTK Standard NERC appears to have extracted 3 false positives while the Stanford NERC missed 3 true positives and the Polyglot NERC extracted all but one true positive.  Pulling from a body of academic information, we can improve the reliability of classifications by combining the outputs from various classifiers in an ensemble method.  In this case, we will look make our final extraction set a combination of common unique items between two elements containing NERC results.  In theory, this should improve our results. But first, let's create little dataframes of our results to calculate some key figures.\n",
    "\n",
    "1. TN / True Negative: case was negative and predicted negative\n",
    "2. TP / True Positive: case was positive and predicted positive\n",
    "3. FN / False Negative: case was positive but predicted negative\n",
    "4. FP / False Positive: case was negative but predicted positive\n",
    "\n",
    "This function will calculate metrics to give us precision and recall.  Just pass in the names of the hand labeled list and the name of the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculations and logic from http://www.kdnuggets.com/faq/precision-recall.html\n",
    "\n",
    "def metrics(truth,run):\n",
    "    truth = truth\n",
    "    run = run\n",
    "    TP = float(len(set(run) & set(truth)))\n",
    "\n",
    "    if float(len(run)) >= float(TP):\n",
    "        FP = len(run) - TP\n",
    "    else:\n",
    "        FP = TP - len(run)\n",
    "    TN = 0\n",
    "    if len(truth) >= len(run):\n",
    "        FN = len(truth) - len(run)\n",
    "    else:\n",
    "        FN = 0\n",
    "\n",
    "    accuracy = (float(TP)+float(TN))/float(len(truth))\n",
    "    recall = (float(TP))/float(len(truth))\n",
    "    precision = float(TP)/(float(FP)+float(TP))\n",
    "    print \"The accuracy is %r\" % accuracy\n",
    "    print \"The recall is %r\" % recall\n",
    "    print \"The precision is %r\" % precision\n",
    "    \n",
    "    d = {'Predicted Negative': [TN,FN], 'Predicted Positive': [FP,TP]}\n",
    "    metricsdf = pd.DataFrame(d, index=['Negative Cases','Positive Cases'])\n",
    "\n",
    "    return metricsdf   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a clean function, next is to pass each document into function to return the performance metrics for each NERC tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    NLTK Standard NERC Tool Metrics     \n",
      "\n",
      "\n",
      "The accuracy is 1.0\n",
      "The recall is 1.0\n",
      "The precision is 0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative Cases</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Cases</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Predicted Negative  Predicted Positive\n",
       "Negative Cases                   0                   3\n",
       "Positive Cases                   0                   6"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print\n",
    "print\n",
    "str = \"NLTK Standard NERC Tool Metrics\"\n",
    "\n",
    "print str.center(40, ' ')\n",
    "print\n",
    "print\n",
    "metrics(p19pdf_authors,nltkstandard_p19ents['top']['persons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "       Stanford NERC Tool Metrics       \n",
      "\n",
      "\n",
      "The accuracy is 0.5\n",
      "The recall is 0.5\n",
      "The precision is 1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative Cases</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Cases</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Predicted Negative  Predicted Positive\n",
       "Negative Cases                   0                   0\n",
       "Positive Cases                   3                   3"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print\n",
    "print\n",
    "str = \"Stanford NERC Tool Metrics\"\n",
    "\n",
    "print str.center(40, ' ')\n",
    "print\n",
    "print\n",
    "metrics(p19pdf_authors, stan_p19ents['top']['persons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "       Polyglot NERC Tool Metrics       \n",
      "\n",
      "\n",
      "The accuracy is 0.8333333333333334\n",
      "The recall is 0.8333333333333334\n",
      "The precision is 0.8333333333333334\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative Cases</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Cases</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Predicted Negative  Predicted Positive\n",
       "Negative Cases                   0                   1\n",
       "Positive Cases                   0                   5"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print\n",
    "print\n",
    "str = \"Polyglot NERC Tool Metrics\"\n",
    "\n",
    "print str.center(40, ' ')\n",
    "print\n",
    "print\n",
    "metrics(p19pdf_authors,poly_p19ents['top']['persons'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting and Visualization\n",
    "\n",
    "Looking at our crude calcuations above, we have some quick takeaways.  All the NERC tools extract true entities from the top section successfully.  An small discussion of their results suggest different applications: \n",
    "\n",
    "* The NLTK Standard Chunker has perfect accuracy and recall but lacks in the precision department.  It succesfully extracted all the authors for the document, but also extracted 3 false entities.  NLTK's chunker would serve well in an entity extraction pipeline where the data scientist is concerned with identifying all possible entities\n",
    "\n",
    "* The Stanford NER tool is very precise (specificity vs sensitivity).  The entities it extracts were 100% accurate, but it failed to identify half of the true entities.  The Stanford NER tool would be best used when a data scientist wanted to extract only those entities that have a high likelihood of being named entities, suggesting an unconscious acceptance of leaving some information on the cutting floor\n",
    "\n",
    "*  The Polyglot Named Entity Recognizer identified five named entities exactly, but only partially identified the sixth (first name returned only).  The data scientist looking for a happy medium between sensitivity and specificity would likely use Polyglot, as it will balance extracting the 100% accurate entities and those which may not necessarily be a named entity. \n",
    "\n",
    "Running these same scoring pipelines on different sections of the journal articles (references, abstract, keywords, etc.) will yield different results (i.e. Polyglot pulls last name only from reference section while NLTK Standard and Stanford NER pull first initial last name well, but with less matches). \n",
    "\n",
    "# Insert simple graphic showing comparison of FP, TP, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization using ensemble methods: Two classifiers is better than one\n",
    "\n",
    "In our discussion above, we notice the varying levels of performance.  Intuitive thought and observation of the results suggest a pathway to improve our extractor performance, by combining the results using the *set* module. Between all three NERC tools, at least one of the evaulation scores is 1.0.  Form the result sets, each NERC tool had at least 3 named persons that were true positives.  But, no two NERC tools had the same false positive named entity, or false negative for that matter.  Using the set method to [set intersection and union operations](http://www.linuxtopia.org/online_books/programming_books/python_programming/python_ch16s03.html) we can improve the performance of our named entitiy extraction by creating an ensemble classifier, which [refers to a group of individual classifiers that are cooperatively trained on data set in a supervised classification problem](http://arxiv.org/pdf/1404.4088.pdf).  Our ensemble classifier rule is very simple:\n",
    "\n",
    "1. Return all named entities that exist in at least two of the results from our NERC tool\n",
    "\n",
    "We implement this rule using the *set* module.  Because this is a supervised problem, we compare each of our NERC results for named \"person\" extractions to the hand labeled person extractions. First, we build the sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a =set(nltkstandard_p19ents['top']['persons']) & set(p19pdf_authors)\n",
    "b =set(stan_p19ents['top']['persons']) & set(p19pdf_authors)\n",
    "c = set(poly_p19ents['top']['persons']) & set(p19pdf_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We do a set of two extraction results, and then use *set union* to combine the elements\n",
    "combined = {'top':list(set(nltkstandard_p19ents['top']['persons']) & set(stan_p19ents['top']['persons']).union(set(nltkstandard_p19ents['top']['persons'])&set(poly_p19ents['top']['persons'])))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Timeline',\n",
       " u'Tim Althoff',\n",
       " u'Xin Luna Dong',\n",
       " u'Kevin Murphy',\n",
       " u'Safa Alai',\n",
       " u'Van Dang',\n",
       " u'Wei Zhang',\n",
       " u'Stanford',\n",
       " u'Mountain View']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined['top']\n",
    "nltkstandard_p19ents['top']['persons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Safa Alai'}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(p19pdf_authors).difference(set(poly_p19ents['top']['persons']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add this new combined result into our dataframe from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' type is unordered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-c7140b587c4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'top'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Combined NERCs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 raise TypeError(\"{0!r} type is unordered\"\n\u001b[0;32m--> 205\u001b[0;31m                                 \"\".format(data.__class__.__name__))\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'set' type is unordered"
     ]
    }
   ],
   "source": [
    "pd.concat(met,pd.Series(combined['top'], index=None, dtype=None, name='Combined NERCs', copy=False, fastpath=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(nltkstandard_p19ents['top']['persons']) & set(stan_p19ents['top']['persons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.union(b)\n",
    "#b.union(c)\n",
    "#a.union(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df9 = pd.Series(poly_p19ents['references']['persons'], index=None, dtype=None, name='Polyglot NERC', copy=False, fastpath=False)\n",
    "df6=pd.Series(stan_p19ents['references']['persons'], index=None, dtype=None, name='Stanford NERC', copy=False, fastpath=False)\n",
    "df7=pd.Series(nltkstandard_p19ents['references']['persons'], index=None, dtype=None, name='NLTKStandard NERC', copy=False, fastpath=False)\n",
    "df8 = pd.Series(p19pdf_references_authors, index=None, dtype=None, name='True Ref Authors', copy=False, fastpath=False)\n",
    "pd.concat([df5,df6,df7,df8], axis=1).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d =set(nltkstandard_p19ents['references']['persons']) & set(p19pdf_references_authors)\n",
    "e =set(stan_p19ents['references']['persons']) & set(p19pdf_references_authors)\n",
    "f = set(poly_p19ents['references']['persons']) & set(p19pdf_references_authors)\n",
    "\n",
    "#len(set(f.union(e)) & set(d.union(e)))\n",
    "print d.union(e).union(f)\n",
    "#d.union(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(poly_p19ents['references']['persons'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(stan_p19ents['references']['persons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(nltkstandard_p19ents['references']['persons'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sup id=\"fn1\">1. [(2014). Text Mining and its Business Applications - CodeProject. Retrieved December 26, 2015, from http://www.codeproject.com/Articles/822379/Text-Mining-and-its-Business-Applications.]<a href=\"#ref1\" title=\"Jump back to footnote 1 in the text.\">↩</a></sup>\n",
    "\n",
    "<sup id=\"fn2\">2. [Suchanek, F., & Weikum, G. (2013). Knowledge harvesting in the big-data era. Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data. ACM.]<a href=\"#ref2\" title=\"Jump back to footnote 2 in the text.\">↩</a></sup>\n",
    "\n",
    "\n",
    "<sup id =\"fn3\">3. [Nadeau, D., & Sekine, S. (2007). A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1), 3-26.]<a href=\"#ref3\" title = \"Jump back to footnote 3 in the text\">↩</a></sup>\n",
    "\n",
    "<sup id =\"fn4\">4. [Ojeda, Tony, Sean Patrick Murphy, Benjamin Bengfort, and Abhijit Dasgupta. [Practical Data Science Cookbook: 89 Hands-on Recipes to Help You Complete Real-world Data Science Projects in R and Python](https://www.packtpub.com/big-data-and-business-intelligence/practical-data-science-cookbook). N.p.: n.p., n.d. Print.]<a href=\"#ref4\" title = \"Jump back to footnote 4 in the text\">↩</a></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Parking Lot of links, leftover paragraphs, ideas, etc.</span>\n",
    "\n",
    "Describe the data -> Data available here http://dl.acm.org/citation.cfm?id=2783258# \n",
    "\n",
    "Computer Vision - ECCV 2008 pdf download online free. Retrieved December 31, 2015, from http://pdf12.mono-ebook.org/pdf/computer-vision-eccv-2008_12glgt.pdf.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<href id=\"pipe\"><a href=\"#pipeline\" title=\"Jump back to data science pipeline graphic.\">data science pipeline</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ben's Outline from email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ~~Give a brief introduction to the task, and why it's interesting, important. Then begin to discuss the data set, how you acquired, and where a reader can get access to it.~~ \n",
    "\n",
    "* ~~You then could have a data exploration section where you show the number of documents, perform a word count, show snippets of data (e.g. references) etc that are of interest.~~\n",
    "\n",
    "* ~~You can then go through one or a few of your \"code to get\" sections. These functions all follow basically the same pattern, so you could probably merge them into a single function, that appropriately selects the right regular expression.~~ \n",
    "\n",
    "* ~~The next step is to discuss, demonstrate your \"truth tests\" for text extraction accuracy.~~ \n",
    "\n",
    "* ~~Finally, you can get to an introduction of your three methods for NERC, and show how do do each of them. Then compare (visually) the results of the three according to the evaluation mechanism discussed above.~~ \n",
    "\n",
    "* You could then conclude with a discussion about NLTK chunk vs. hand labelled entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kddcorpus_bigrams=[]\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "for fileid in kddcorpus.fileids():\n",
    "    for l in (BigramCollocationFinder.from_words(kddcorpus.words(fileid)).nbest(bigram_measures.pmi, 10)):\n",
    "        kddcorpus_bigrams.append(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Prototype Holder</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extraction(corpus):\n",
    "    import itertools\n",
    "    import unicodedata\n",
    "    from polyglot.text import Text\n",
    "    \n",
    "    corpus=corpus\n",
    "    # extract entities from a single string; remove whitespace characters\n",
    "    try:\n",
    "        e = Text(corpus).entities\n",
    "    except:\n",
    "        pass #e = Text(re.sub(\"(r'(x0)',\" \",\"(re.sub('[\\s]',\" \",corpus)))).entities\n",
    "    \n",
    "    current_person =[]\n",
    "    persons =[]\n",
    "    current_org=[]\n",
    "    organizations=[]\n",
    "    current_loc=[]\n",
    "    locations=[]\n",
    "\n",
    "    for l in e:\n",
    "        if l.tag == 'I-PER':\n",
    "            for m in l:\n",
    "                current_person.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_person: # if the current chunk is not empty\n",
    "                        persons.append(\" \".join(current_person))\n",
    "                        current_person = []\n",
    "        elif l.tag == 'I-ORG':\n",
    "            for m in l:\n",
    "                current_org.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_org: # if the current chunk is not empty\n",
    "                        organizations.append(\" \".join(current_org))\n",
    "                        current_org = []\n",
    "        elif l.tag == 'I-LOC':\n",
    "            for m in l:\n",
    "                current_loc.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_loc: # if the current chunk is not empty\n",
    "                        locations.append(\" \".join(current_loc))\n",
    "                        current_loc = []\n",
    "    results = {}\n",
    "    results['persons']=persons\n",
    "    results['organizations']=organizations\n",
    "    results['locations']=locations\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_continuous_chunks(string):\n",
    "    string = string\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for token, tag in stner.tag(string.split()):\n",
    "        if tag != \"O\":\n",
    "            current_chunk.append((token, tag))\n",
    "        else:\n",
    "            if current_chunk: # if the current chunk is not empty\n",
    "                continuous_chunk.append(current_chunk)\n",
    "                current_chunk = []\n",
    "    # Flush the final current_chunk into the continuous_chunk, if any.\n",
    "    if current_chunk:\n",
    "        continuous_chunk.append(current_chunk)\n",
    "    named_entities = continuous_chunk\n",
    "    named_entities_str = [\" \".join([token for token, tag in ne]) for ne in named_entities]\n",
    "    named_entities_str_tag = [(\" \".join([token for token, tag in ne]), ne[0][1]) for ne in named_entities]\n",
    "    persons = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"PERSON\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    persons.append(n.strip(\"*\"))\n",
    "    organizations = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"ORGANIZATION\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                n.strip(\"*\")\n",
    "                if len(n)>0:\n",
    "                    organizations.append(n.strip(\"*\"))\n",
    "    locations = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"LOCATION\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    locations.append(n.strip(\"*\"))\n",
    "    dates = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"DATE\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    dates.append(n.strip(\"*\"))\n",
    "    money = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"MONEY\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    money.append(n.strip(\"*\"))\n",
    "    time = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"TIME\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    money.append(n.strip(\"*\"))\n",
    "                    \n",
    "    percent = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"PERCENT\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    money.append(n.strip(\"*\"))\n",
    "\n",
    "    entities={}\n",
    "    entities['persons']= persons\n",
    "    entities['organizations']= organizations\n",
    "    entities['locations']= locations\n",
    "    #entities['dates']= dates\n",
    "    #entities['money']= money\n",
    "    #entities['time']= time\n",
    "    #entities['percent']= percent\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p29' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-22423526d96d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_continuous_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp29\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'top'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'p29' is not defined"
     ]
    }
   ],
   "source": [
    "get_continuous_chunks(p29['top'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nltktreelist(text):\n",
    "    from operator import itemgetter\n",
    "    \n",
    "    text = text\n",
    "    \n",
    "    \n",
    "    persons = []\n",
    "    organizations = []\n",
    "    locations =[]\n",
    "    genpurp = []\n",
    "\n",
    "    for l in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))):\n",
    "        if isinstance(l,nltk.tree.Tree):\n",
    "            if l.label() == 'PERSON':\n",
    "                if len(l)== 1:\n",
    "                    if l[0][0] in persons:\n",
    "                        pass\n",
    "                    else:\n",
    "                        persons.append(l[0][0])\n",
    "                else:\n",
    "                    if \" \".join(map(itemgetter(0), l)) in persons:\n",
    "                        pass\n",
    "                    else:\n",
    "                        persons.append(\" \".join(map(itemgetter(0), l)).strip(\"*\"))\n",
    "   \n",
    "\n",
    "    for o in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))):\n",
    "        if isinstance(o,nltk.tree.Tree):\n",
    "            if o.label() == 'ORGANIZATION':\n",
    "                if len(o)== 1:\n",
    "                    if o[0][0] in organizations:\n",
    "                        pass\n",
    "                    else:\n",
    "                        organizations.append(o[0][0])\n",
    "                else:\n",
    "                    if \" \".join(map(itemgetter(0), o)) in organizations:\n",
    "                        pass\n",
    "                    else:\n",
    "                        organizations.append(\" \".join(map(itemgetter(0), o)).strip(\"*\"))\n",
    "    \n",
    "\n",
    "    for o in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))):\n",
    "        if isinstance(o,nltk.tree.Tree):\n",
    "            if o.label() == 'LOCATION':\n",
    "                if len(o)== 1:\n",
    "                    if o[0][0] in locations:\n",
    "                        pass\n",
    "                    else:\n",
    "                        locations.append(o[0][0])\n",
    "                else:\n",
    "                    if \" \".join(map(itemgetter(0), o)) in locations:\n",
    "                        pass\n",
    "                    else:\n",
    "                        locations.append(\" \".join(map(itemgetter(0), o)).strip(\"*\"))\n",
    "    \n",
    "    for e in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))):\n",
    "        if isinstance(o,nltk.tree.Tree):\n",
    "            if o.label() == 'GPE':\n",
    "                if len(o)== 1:\n",
    "                    if o[0][0] in genpurp:\n",
    "                        pass\n",
    "                    else:\n",
    "                        genpurp.append(o[0][0])\n",
    "                else:\n",
    "                    if \" \".join(map(itemgetter(0), o)) in genpurp:\n",
    "                        pass\n",
    "                    else:\n",
    "                        genpurp.append(\" \".join(map(itemgetter(0), o)).strip(\"*\"))\n",
    "                        \n",
    "       \n",
    "\n",
    "\n",
    "    results = {}\n",
    "    results['persons']=persons\n",
    "    results['organizations']=organizations\n",
    "    results['locations']=locations\n",
    "    results['genpurp'] = genpurp\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltktreelist(p19['top'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normal case test, abstract gold\n",
    "failids = []\n",
    "docnum = 'p3.txt'\n",
    "text=kddcorpus.raw(docnum)\n",
    "full = True\n",
    "section = \"abstract\"\n",
    "if full == True: \n",
    "    text = kddcorpus.raw(fileid)\n",
    "    if section == \"abstract\":\n",
    "        section1=[\"ABSTRACT\", \"Abstract \"]\n",
    "        target = \"\"   \n",
    "        section2=[\"Categories and Subject Descriptors\",\"Categories & Subject Descriptors\",\"Keywords\",\"INTRODUCTION\"]\n",
    "        for fileid in kddcorpus.fileids():\n",
    "            text = kddcorpus.raw(fileid)\n",
    "\n",
    "\n",
    "            for sect1 in section1:\n",
    "                for sect2 in section2:\n",
    "                    part1= \"(?<=\"+str(sect1)+\")(.+)\"\n",
    "                    part2 = \"(?=\"+str(sect2)+\")\"\n",
    "                    p = re.compile(part1+part2)\n",
    "                    try:\n",
    "                        target=p.search(re.sub('[\\s]',\" \",text)).group()\n",
    "                        if len(target) > 50:\n",
    "\n",
    "                            print [fileid,len(target),len(text)]\n",
    "                            break\n",
    "                        else:\n",
    "                            print fileid,\"Failed\"\n",
    "                            pass\n",
    "                    except AttributeError:\n",
    "                        pass                             \n",
    "else:\n",
    "    \n",
    "    section = \"abstract\"\n",
    "    if section == \"abstract\":\n",
    "        section1=[\"ABSTRACT\", \"Abstract \"]\n",
    "        target = \"\"   \n",
    "        section2=[\"Categories and Subject Descriptors\",\"Categories & Subject Descriptors\",\"Keywords\",\"INTRODUCTION\"]\n",
    "        for sect1 in section1:\n",
    "            for sect2 in section2:\n",
    "                part1= \"(?<=\"+str(sect1)+\")(.+?)\"\n",
    "                part2 = \"(?=\"+str(sect2)+\"[\\s]?)\"\n",
    "                p = re.compile(part1+part2)\n",
    "                try:\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group()\n",
    "                    if len(target) > 50:\n",
    "\n",
    "                        print [docnum,len(target),len(text)]\n",
    "                        break\n",
    "                    else:\n",
    "                        print fileid,\"Failed\"\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    pass      \n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# completed gold standard for keywords, got all of them..no stragglers\n",
    "docnum = 'p29.txt'\n",
    "text = kddcorpus.raw(docnum)\n",
    "failids = []\n",
    "full = True\n",
    "section = \"keywords\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid)\n",
    "        if section == \"keywords\":\n",
    "            section1=\"Keywords\"\n",
    "            target = \"\"   \n",
    "            section2=[\"Bio\",\"1.  INTRODUCTION  \",\"1.  INTROD \",\"1. MOTIVATION\",\"Permission to make \",\"1.MOTIVATION\",'1.Motivation' ]\n",
    "        \n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    if len(target) >50:\n",
    "                        if len(target) > 300:\n",
    "                            target = target[:200]\n",
    "                        else:\n",
    "                            target = target\n",
    "                        \n",
    "                        print [fileid,target.strip(),len(text)]\n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        failids.append(fileid)\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "else:\n",
    "    section = \"keywords\"\n",
    "    \n",
    "    if section == \"keywords\":\n",
    "        section1=\"Keywords\"\n",
    "        target = \"\"   \n",
    "        section2=[\"Bio\",\"1.  INTRODUCTION  \",\"1.  INTROD \",\"1. MOTIVATION\",\"Permission to make \",\"1.MOTIVATION\",'1.Motivation' ]\n",
    "        \n",
    "        part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "        for sect in section2:\n",
    "            try:\n",
    "                part2 = \"(?=\"+str(sect)+\")\"\n",
    "                p=re.compile(part1+part2)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                if target > 3:                 \n",
    "                    break                  \n",
    "            except:\n",
    "                pass\n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "failids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# completed gold standard for references, counts number of references and does \"word per reference\" score\n",
    "docnum=\"p19.txt\"\n",
    "failids = []\n",
    "text=kddcorpus.raw(docnum)\n",
    "\n",
    "full = False\n",
    "section = \"references\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid)\n",
    "        if section == \"references\":\n",
    "            section1=[\"REFERENCES\"] \n",
    "            target = \"\"   \n",
    "\n",
    "            \n",
    "\n",
    "            for sect in section1:\n",
    "                try:\n",
    "                    part1= \"(?<=\"+sect+\")(.+)\"\n",
    "                    p=re.compile(part1)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    if len(target) > 50:\n",
    "\n",
    "                        # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                        try:\n",
    "                            refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                        except:\n",
    "                            print \"This file does not appear to have a references section\"\n",
    "                            pass\n",
    "                        \n",
    "\n",
    "                        print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                        break\n",
    "                    else:\n",
    "\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "\n",
    "    \n",
    "\n",
    "# to return output from one document\n",
    "else:\n",
    "    ans = {}\n",
    "    failids=[]\n",
    "    text = kddcorpus.raw(docnum)\n",
    "    if section == \"references\":\n",
    "        section1=[\"REFERENCES\"] \n",
    "        target = \"\"   \n",
    "        for sect in section1:\n",
    "            try:\n",
    "                part1= \"(?<=\"+sect+\")(.+)\"\n",
    "                p=re.compile(part1)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                if len(target) > 50:\n",
    "                    # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                    try:\n",
    "                        refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                    except:\n",
    "                        print \"This file does not appear to have a references section\"\n",
    "                        pass\n",
    "\n",
    "\n",
    "                    print [docnum,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                    break\n",
    "                else:\n",
    "\n",
    "                    pass\n",
    "            except AttributeError:\n",
    "                failids.append(docnum)\n",
    "                pass\n",
    "\n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(failids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gold standard to get top section\n",
    "\n",
    "from emailextractor import file_to_str, get_emails # paste code to .py file from following link and save within your environment path to call it: https://gist.github.com/dideler/5219706\n",
    "\n",
    "failids = []\n",
    "docnum =\"p1.txt\"\n",
    "text=kddcorpus.raw(docnum)\n",
    "\n",
    "full = True\n",
    "section = \"top\"\n",
    "if full == True:\n",
    "    if section == 'top':\n",
    "        section = [\"ABSTRACT\",\"Abstract\",\"Bio\",\"Panel Summary\"]\n",
    "        for fileid in kddcorpus.fileids():\n",
    "            text = kddcorpus.raw(fileid)\n",
    "            for sect in section:\n",
    "                try:\n",
    "                    part1=\"(.+)(?=\"+s+\")\"\n",
    "                    #print \"re.compile\"+\"(\"+part1+\")\"\n",
    "                    p=re.compile(part1)\n",
    "                    target = p.search(re.sub('[\\s]',\" \", text)).group()\n",
    "                    #print docnum,len(target),len(text)\n",
    "\n",
    "                    emails = tuple(get_emails(target))\n",
    "                    print [fileid,len(target),len(text), emails]\n",
    "                    break\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "\n",
    "                               \n",
    "        # to return output from one document\n",
    "else:\n",
    "\n",
    "    failids=[]\n",
    "    text = kddcorpus.raw(docnum)\n",
    "\n",
    "    if section == \"top\":\n",
    "        section = [\"ABSTRACT\",\"Abstract\",\"Bio\",\"Panel Summary\"]\n",
    "        docnum=\"p19.txt\"\n",
    "        text = kddcorpus.raw(docnum)\n",
    "        for sect in section:\n",
    "            try:\n",
    "                part1=\"(.+)(?=\"+s+\")\"\n",
    "                print \"re.compile\"+\"(\"+part1+\")\"\n",
    "                p=re.compile(part1)\n",
    "                target = p.search(re.sub('[\\s]',\" \", text)).group()\n",
    "                print docnum,len(target),len(text)\n",
    "\n",
    "                emails = tuple(get_emails(target))\n",
    "\n",
    "                print [fileid,len(target),len(text),emails]\n",
    "                break\n",
    "\n",
    "            except AttributeError:\n",
    "                failids.append(fileid)\n",
    "                pass\n",
    "\n",
    "\n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "failids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:violet\">Drawing Board/Assembly Line</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attempting function with gold keywords....\n",
    "\n",
    "def keypull(docnum=None,section='keywords',full = False):\n",
    "    \n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "\n",
    "        \n",
    "\n",
    "        # to return output from entire corpus\n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid).lower()\n",
    "                if section == \"keywords\":\n",
    "                    section1=\"keywords\"\n",
    "                    target = \"\"   \n",
    "                    section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"(1. tutorial )\",\" permission to make \",\"  permission to make\",\"(  permission to make digital )\",\"    bio  \",\"abstract:  \",\"1.motivation\" ]\n",
    "\n",
    "                    part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "                    for sect in section2:\n",
    "                        try:\n",
    "                            part2 = \"(?=\"+str(sect)+\")\"\n",
    "                            p=re.compile(part1+part2)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                            if len(target) >50:\n",
    "                                if len(target) > 300:\n",
    "                                    target = target[:200]\n",
    "                                else:\n",
    "                                    target = target\n",
    "\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"keywords\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                #print [fileid,len(target),len(text)]\n",
    "                                break\n",
    "                            else:\n",
    "                                if len(target)==0:\n",
    "                                     failids.append(fileid)   \n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "            set(failids)\n",
    "            return ans\n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        if full == False:\n",
    "            if section == \"keywords\":\n",
    "                section1=\"keywords\"\n",
    "                target = \"\"   \n",
    "                section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"permission to make \",\"1.motivation\" ]\n",
    "\n",
    "                part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "                for sect in section2:\n",
    "                    try:\n",
    "                        part2 = \"(?=\"+str(sect)+\")\"\n",
    "                        p=re.compile(part1+part2)\n",
    "                        target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                        if len(target) >50:\n",
    "                            if len(target) > 300:\n",
    "                                target = target[:200]\n",
    "                            else:\n",
    "                                target = target\n",
    "                            ans[docnum]={}\n",
    "                            ans[docnum][\"keywords\"]=target.strip()\n",
    "                            ans[docnum][\"charcount\"]=len(target)\n",
    "                            break                  \n",
    "                    except:\n",
    "                        pass\n",
    "    return ans\n",
    "    return failids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attempting function with gold abstracts...Normal case done\n",
    "\n",
    "def abpull(docnum=None,section='abstract',full = False):\n",
    "    \n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        # to return output from entire corpus\n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid)\n",
    "                if section == \"abstract\":\n",
    "                    section1=[\"ABSTRACT\", \"Abstract \"]\n",
    "                    target = \"\"   \n",
    "                    section2=[\"Categories and Subject Descriptors\",\"Categories & Subject Descriptors\",\"Keywords\",\"INTRODUCTION\"]\n",
    "                    for fileid in kddcorpus.fileids():\n",
    "                        text = kddcorpus.raw(fileid)\n",
    "\n",
    "\n",
    "                        for sect1 in section1:\n",
    "                            for sect2 in section2:\n",
    "                                part1= \"(?<=\"+str(sect1)+\")(.+)\"\n",
    "                                part2 = \"(?=\"+str(sect2)+\")\"\n",
    "                                p = re.compile(part1+part2)\n",
    "                                try:\n",
    "                                    target=p.search(re.sub('[\\s]',\" \",text)).group()\n",
    "                                    if len(target) > 50:\n",
    "                                        ans[str(fileid)]={}\n",
    "                                        ans[str(fileid)][\"abstract\"]=target.strip()\n",
    "                                        ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                        #print [fileid,len(target),len(text)]\n",
    "                                        break\n",
    "                                    else:\n",
    "                                        failids.append(fileid)\n",
    "                                        pass\n",
    "                                except AttributeError:\n",
    "                                    pass \n",
    "                \n",
    "            return ans\n",
    "                              \n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum).lower()\n",
    "        if section == \"abstract\":\n",
    "            section1=[\"ABSTRACT\", \"Abstract \"]\n",
    "            target = \"\"   \n",
    "            section2=[\"Categories and Subject Descriptors\",\"Categories & Subject Descriptors\",\"Keywords\",\"INTRODUCTION\"]\n",
    "            for sect1 in section1:\n",
    "                for sect2 in section2:\n",
    "                    part1= \"(?<=\"+str(sect1)+\")(.+?)\"\n",
    "                    part2 = \"(?=\"+str(sect2)+\"[\\s]?)\"\n",
    "                    p = re.compile(part1+part2)\n",
    "                    try:\n",
    "                        target=p.search(re.sub('[\\s]',\" \",text)).group()\n",
    "                        if len(target) > 50:\n",
    "                            ans[str(docnum)]={}\n",
    "                            ans[str(docnum)][\"abstract\"]=target.strip()\n",
    "                            ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                            #print [docnum,len(target),len(text)]\n",
    "                            break\n",
    "                        else:\n",
    "                            failids.append(docnum)\n",
    "                            pass\n",
    "                    except AttributeError:\n",
    "                        pass\n",
    "        return ans\n",
    "        return failids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abpull('p19.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attempting function with gold top section...Normal case done\n",
    "\n",
    "def toppull(docnum=None,section='top',full = False):\n",
    "    from emailextractor import file_to_str, get_emails # paste code to .py file from following link and save within your environment path to call it: https://gist.github.com/dideler/5219706\n",
    "\n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        # to return output from entire corpus\n",
    "        \n",
    "        if full == True:\n",
    "            if section == 'top':\n",
    "                section = [\"ABSTRACT\",\"Abstract\",\"Bio\",\"Panel Summary\"]\n",
    "                for fileid in kddcorpus.fileids():\n",
    "                    text = kddcorpus.raw(fileid)\n",
    "                    for sect in section:\n",
    "                        try:\n",
    "                            part1=\"(.+)(?=\"+sect+\")\"\n",
    "                            #print \"re.compile\"+\"(\"+part1+\")\"\n",
    "                            p=re.compile(part1)\n",
    "                            target = p.search(re.sub('[\\s]',\" \", text)).group()\n",
    "                            #print docnum,len(target),len(text)\n",
    "\n",
    "                            emails = tuple(get_emails(target))\n",
    "                            ans[str(fileid)]={}\n",
    "                            ans[str(fileid)][\"top\"]=target.strip()\n",
    "                            ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                            ans[str(fileid)][\"emails\"]=emails\n",
    "                            #print [fileid,len(target),len(text)]\n",
    "                            break\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "        return ans\n",
    "        return failids\n",
    "                               \n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum)\n",
    "\n",
    "        if section == \"top\":\n",
    "            section = [\"ABSTRACT\",\"Abstract\",\"Bio\",\"Panel Summary\"]\n",
    "            text = kddcorpus.raw(docnum)\n",
    "            for sect in section:\n",
    "                try:\n",
    "                    part1=\"(.+)(?=\"+sect+\")\"\n",
    "                    #print \"re.compile\"+\"(\"+part1+\")\"\n",
    "                    p=re.compile(part1)\n",
    "                    target = p.search(re.sub('[\\s]',\" \", text)).group()\n",
    "                    #print docnum,len(target),len(text)\n",
    "\n",
    "                    emails = tuple(get_emails(target))\n",
    "                    ans[str(docnum)]={}\n",
    "                    ans[str(docnum)][\"top\"]=target.strip()\n",
    "                    ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                    ans[str(docnum)][\"emails\"]=emails\n",
    "                    #print [fileid,len(target),len(text)]\n",
    "                    break\n",
    "\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "\n",
    "        return ans\n",
    "        return failids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attempting function with gold references section\n",
    "\n",
    "def refpull(docnum=None,section='references',full = False):\n",
    "    \n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum)\n",
    "        # to return output from entire corpus\n",
    "        \n",
    "        \n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid)\n",
    "                if section == \"references\":\n",
    "                    section1=[\"REFERENCES\"] \n",
    "                    target = \"\"   \n",
    "\n",
    "\n",
    "\n",
    "                    for sect in section1:\n",
    "                        try:\n",
    "                            part1= \"(?<=\"+sect+\")(.+)\"\n",
    "                            p=re.compile(part1)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                            if len(target) > 50:\n",
    "\n",
    "                                # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                                try:\n",
    "                                    refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                                except:\n",
    "                                    print \"This file does not appear to have a references section\"\n",
    "                                    pass\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"references\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                ans[str(fileid)][\"refcount\"]= refnum\n",
    "                                ans[str(fileid)][\"wordperRef\"]=round(float(len(nltk.word_tokenize(text)))/float(refnum))\n",
    "                                #print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                                break\n",
    "                            else:\n",
    "\n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "\n",
    "            return ans\n",
    "            return failids\n",
    "                              \n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum)\n",
    "        \n",
    "        if section == \"references\":\n",
    "            section1=[\"REFERENCES\"] \n",
    "            target = \"\"   \n",
    "            for sect in section1:\n",
    "                try:\n",
    "                    part1= \"(?<=\"+sect+\")(.+)\"\n",
    "                    p=re.compile(part1)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    if len(target) > 50:\n",
    "                        # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                        try:\n",
    "                            refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                        except:\n",
    "                            print \"This file does not appear to have a references section\"\n",
    "                            pass\n",
    "                        ans[str(docnum)]={}\n",
    "                        ans[str(docnum)][\"references\"]=target.strip()\n",
    "                        ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                        ans[str(docnum)][\"refcount\"]= refnum\n",
    "                        ans[str(docnum)][\"wordperRef\"]=float(len(nltk.word_tokenize(text)))/float(refnum)\n",
    "\n",
    "\n",
    "                        #print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                        break\n",
    "                    else:\n",
    "\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(docnum)\n",
    "                    pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        return ans\n",
    "        return failids\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Testing Station</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltkstandard_top_entities_p19 = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(p19['top'])))\n",
    "nltkstandard_top_entities_p19 = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(p19['references'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltkstandard_top_entities_p19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "persons, organizations, locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news = \"Iran\\’s supreme leader warned Sunday that Saudi Arabia would face divine vengeance for the execution of an outspoken Shiite cleric, a day after Iranian protesters ransacked the Saudi Embassy in Tehran in outrage over the execution.\\“God\\’s hand of retaliation will grip the neck of Saudi politicians,” said the supreme leader, Ayatollah Ali Khamenei, in comments reported on his official website.  Despite the rhetoric, however, the Iranians seemed to be taking steps to prevent the dispute from escalating further. Forty Iranians were arrested on Saturday night for the violence — a sign that the authorities were trying to keep public outrage from getting out of control.Iran\\’s president, Hassan Rouhani, on Sunday condemned the execution, but also said that the attacks on the Saudi embassy in Tehran and the Saudi consulate in Mashhad had damaged Iran’s reputation.Firefighters battled a blaze at the Saudi Embassy in Tehran on Saturday after Iranian protesters entered the building.Iranian Protesters Ransack Saudi Embassy After Execution of Shiite ClericJAN. 2, 2016\\“We do not allow rogue groups to commit illegal actions and damage the holy reputation of Islamic Republic of Iran,\\” he said in a statement. \\“What happened last night in Mashhad and Tehran and collateral damages in Saudi consulate and Embassy is not acceptable and justifiable.\\”\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unicodedata.normalize('NFKD', news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter(lambda x: x in string.printable,unicodedata.normalize('NFKD', (unicode(news,errors='ignore'))).encode('ascii','ignore').decode('unicode_escape').encode('ascii','ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsent = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize((news.decode('unicode_escape').encode('ascii','ignore')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.chunk.api.ChunkParserI(nltk.pos_tag(nltk.word_tokenize((news.decode('unicode_escape').encode('ascii','ignore')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.chunk.api.ChunkParserI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chiefs = nltk.corpus.PlaintextCorpusReader(chiefpath, 'KCChiefs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chiefs.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw = chiefs.raw('KCChiefs').decode('unicode_escape').encode('ascii','ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw ='This post was originally titled, \"Should we root for the Chiefs to lose?\", and was written when our Kansas City Chiefs were 3-5.\\n\\nThat may seem like an eternity ago, but there was a general sense during the week before the Chiefs played the Broncos in Denver that the season was lost. The focus here at Arrowhead Pride went quickly from the Super Bowl to mock drafts. Arguments that the Chiefs just tank the season for a better draft pick were seriously made. It seemed like a waste to even win two straight if it meant improving to a still-worthless 3-5.\\n\\nAfter all, the Andy Reid era Chiefs had never beaten Denver. And, even if they did, what were they supposed to do, go on a nine-game win streak and make the playoffs?\\n\\nStill, even within that atmosphere of capitulation as the Chiefs headed into their Week 9 bye to prepare for a trip to Denver, no one wanted them to lose that game. With the hated enemies one Sunday away, publishing a post asking for patience and a little faith in our 2015 Kansas City Chiefs seemed silly. AP was unanimous about going 1- 0, even if that meant only for one week. We could get back to rooting for the Chiefs to go 4-12 next week.\\n\\nI wrote an article prior to the Denver game saying the Chiefs are a better team than Denver and could beat them on the road. It ended with talk of the season\\'s rough start and provided optimism moving forward:\\n\\nBut that should make winning all the more awesome when it happens, right? And the Chiefs are very, very close to winning. This is a good team finally on the right side of an unfortunate first half of football.\\n\\nSo I tucked away this post you\\'re about to read. And I waited. And waited. And kept on waiting. The Chiefs kept on winning. And the Chiefs kept on winning some more. They kept on winning until this post, written near the nadir of 2015, was suddenly arguing for a perspective that everyone had since acquired.\\n\\nBut now Week 17 is upon us, and Kansas City is guaranteed at least two more games. The game will occur in Denver, Colorado at 5:30 pm.  This post could wait no longer. Take it as a reminder of how far our Chiefs have come since Week 9.\\n\\nI present, unedited from its original text, an argument for why we should stop calling for Andy Reid\\'s head.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stner.tag(raw.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_continuous_chunks(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from polyglot.text import Text\n",
    "Text(raw).entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string = \"I am the man, you will,do what, I say\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re.split(';|,',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string.split(\",\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name = \"Wei Zhang *Computer Science Department\"\n",
    "name.strip(\" *C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
