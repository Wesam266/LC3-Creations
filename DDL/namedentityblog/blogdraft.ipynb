{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick Survey and Comparison of Open Source Named Entity Extractor Tools for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity extraction is a core subtask of building knowledge from semi/unstructured text sources<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup>.  Considering recent increases in computing power and decreases in the costs of data storage, data scientists and developers can build large knowledge bases that contain millions of entities and hundreds of millions of facts about them.  These knowledge bases are key contributors to intelligence computer behavior<sup><a href=\"#fn2\" id=\"ref2\">2</a></sup>.  Therefore, named entity extraction is at the core of several popular technologies such as smart assistants ([Siri](http://www.apple.com/ios/siri/), [Google Now](https://www.google.com/landing/now/)), machine reading, and deep interpretation of natural language<sup><a href=\"#fn3\" id=\"ref3\">3</a></sup>.\n",
    "\n",
    "With a realization of how essential it is to recognize information units like names, including person, organization and location names, and numeric expressions including time, date, money\n",
    "and percent expressions, several questions come to mind.  How do you perform named entity extraction, which is formally called “[Named Entity Recognition and Classification (NERC)](https://benjamins.com/catalog/bct.19)”?  What tools are out there?  How can you evaluate their performance?  And most important, what works with Python (shamelessly exposing my bias)?  \n",
    "\n",
    "This post will survey openly available NERC tools and compare the results against hand labeled data for precision, accuracy, and recall.  The tools and basic information extraction principles in this discussion begin the process of structuring unstructured data.\n",
    "\n",
    "We will specifically learn to:\n",
    "1. follow the data science pipeline (see image below)\n",
    "2. prepare semistructured natural language data for ingest using regex\n",
    "3. create a custom corpus in [Natural Language Toolkit](http://www.nltk.org/) \n",
    "4. use a suite of openly available NERC tools to extract entities and store in json format \n",
    "5. compare the performance of NERC tools on our corpus\n",
    "\n",
    "<br>\n",
    "<a href=\"#pipe\" id=\"pipeline\"><center><h3>The Data Science Pipeline:<br>Georgetown Data Science Certificate Program</h3></center></a>\n",
    "<div class=\"image\">\n",
    "\n",
    "      <img src=\"./files/data_science_pipeline.png\" alt=\"Data Science Pipeline\" height=\"300\" width=\"450\" top:\"35\" left:\"170\" />\n",
    "      \n",
    "      \n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data: Peer Reviewed Journals and Keynote Speaker Abstracts from KDD 2014 and 2015\n",
    "\n",
    "Before delving into the pipeline, we need a good dataset.  Jason Brownlee of www.machinelearningmastery.com had some good suggestions in his [August 2015 article](http://machinelearningmastery.com/practice-machine-learning-with-small-in-memory-datasets-from-the-uci-machine-learning-repository/) on picking a dataset for machine learning exercises:  \n",
    "\n",
    "* **Real-World**: The datasets should be drawn from the real world (rather than being contrived). This will keep them interesting and introduce the challenges that come with real data.\n",
    "\n",
    "* **Small**: The datasets need to be small so that you can inspect and understand them and that you can run many models quickly to accelerate your learning cycle.\n",
    "\n",
    "* **Well-Understood**: There should be a clear idea of what the data contains, why it was collected, what the problem is that needs to be solved so that you can frame your investigation.\n",
    "\n",
    "* **Baseline**: It is also important to have an idea of what algorithms are known to perform well and the scores they achieved so that you have a useful point of comparison. This is important when you are getting started and learning because you need quick feedback as to how well you are performing (close to state-of-the-art or something is broken).\n",
    "\n",
    "* **Plentiful**: You need many datasets to choose from, both to satisfy the traits you would like to investigate and (if possible) your natural curiosity and interests. \n",
    "\n",
    "Luckily, we have a dataset that meets nearly all of these requirements.  I attended the Knowledge Discovery and Data Mining (KDD) conferences in [New York City (2014)](http://www.kdd.org/kdd2014/) and [Sydney, Australia (2015)](http://www.kdd.org/kdd2015/).  Both years, attendees received a USB with the conference proceedings.  Each repository contains over 230 peer reviewed journal articles and keynote speaker abstracts on data mining, knowledge discovery, big data, data science and their applications. The full conference proceedings can be purchased for \\$60 at the [Association for Computing Machinery's Digital Library](https://dl.acm.org/purchase.cfm?id=2783258&CFID=740512201&CFTOKEN=34489585) (includes ACM membership). This post will work with a dataset that is equivalent to the conference proceedings.  It's important to note that this dataset recreates a real word data science exercise that is instructive of big data problems.  We will take semi-structured data (PDF journal articles and abstracts in publication format), strip text from the files, and add more structure to the data that would facilitate follow on analysis. \n",
    "\n",
    "<blockquote cite=\"https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/KDDwebscrape.ipynb\">\n",
    "Interested parties looking for a free option can use the <a href=\"https://pypi.python.org/pypi/beautifulsoup4/4.4.1\">beautifulsoup</a> and <a href=\"https://pypi.python.org/pypi/requests/2.9.1\">request</a> libraries to scrape the <a href=\"http://dl.acm.org/citation.cfm?id=2785464&CFID=740512201&CFTOKEN=3448958\">ACM website for KDD 2015 conference data</a> that can be used in natural language processing pipelines.  I have some <a href=\"https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/KDDwebscrape.ipynb\">skeleton web scraping code</a> to generate lists of all abstracts, author names, and journal/keynote address titles.    \n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration: Getting the number of files, and file type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored locally in the following directory:\n",
    "```python\n",
    ">>> import os\n",
    ">>> print os.getcwd()\n",
    "/Users/linwood/Desktop/KDD_15/docs\n",
    "```\n",
    "Let's explore the number of files we have and naming conventions. We begin with the administrative tasks of loading modules, establishing paths, etc.  \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Importing what we need\n",
    "#**********************************************************************\n",
    "import os\n",
    "import time\n",
    "from os import walk\n",
    "\n",
    "#**********************************************************************\n",
    "# Administrative code to set the path for file loading\n",
    "#**********************************************************************\n",
    "\n",
    "path        = os.path.abspath(os.getcwd())\n",
    "TESTDIR     = os.path.normpath(os.path.join(os.path.expanduser(\"~\"),\"Desktop\",\"KDD_15\",\"docs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Next we iterate over the files in the directory and store those names in the empty list we created called *files*.  We time the operation, print list with the file names and also print out the length of the list (gives number of target files).<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n",
      "\n",
      "253\n",
      "\n",
      "[p1.pdf, p1005.pdf, p1015.pdf, p1025.pdf, p1035.pdf, p1045.pdf, p1055.pdf, p1065.pdf, p1075.pdf, p1085.pdf, p109.pdf, p1095.pdf, p1105.pdf, p1115.pdf, p1125.pdf, p1135.pdf, p1145.pdf, p1155.pdf, p1165.pdf, p1175.pdf, p1185.pdf, p119.pdf, p1195.pdf, p1205.pdf, p1215.pdf, p1225.pdf, p1235.pdf, p1245.pdf, p1255.pdf, p1265.pdf, p1275.pdf, p1285.pdf, p129.pdf, p1295.pdf, p1305.pdf, p1315.pdf, p1325.pdf, p1335.pdf, p1345.pdf, p1355.pdf, p1365.pdf, p1375.pdf, p1385.pdf, p139.pdf, p1395.pdf, p1405.pdf, p1415.pdf, p1425.pdf, p1435.pdf, p1445.pdf, p1455.pdf, p1465.pdf, p1475.pdf, p1485.pdf, p149.pdf, p1495.pdf, p1503.pdf, p1513.pdf, p1523.pdf, p1533.pdf, p1543.pdf, p1553.pdf, p1563.pdf, p1573.pdf, p1583.pdf, p159.pdf, p1593.pdf, p1603.pdf, p1621.pdf, p1623.pdf, p1625.pdf, p1627.pdf, p1629.pdf, p1631.pdf, p1633.pdf, p1635.pdf, p1637.pdf, p1639.pdf, p1641.pdf, p1651.pdf, p1661.pdf, p1671.pdf, p1681.pdf, p169.pdf, p1691.pdf, p1701.pdf, p1711.pdf, p1721.pdf, p1731.pdf, p1741.pdf, p1751.pdf, p1759.pdf, p1769.pdf, p1779.pdf, p1789.pdf, p179.pdf, p1799.pdf, p1809.pdf, p1819.pdf, p1829.pdf, p1839.pdf, p1849.pdf, p1859.pdf, p1869.pdf, p1879.pdf, p1889.pdf, p189.pdf, p1899.pdf, p19.pdf, p1909.pdf, p1919.pdf, p1929.pdf, p1939.pdf, p1949.pdf, p1959.pdf, p1969.pdf, p1979.pdf, p1989.pdf, p199.pdf, p1999.pdf, p2009.pdf, p2019.pdf, p2029.pdf, p2039.pdf, p2049.pdf, p2059.pdf, p2069.pdf, p2079.pdf, p2089.pdf, p209.pdf, p2099.pdf, p2109.pdf, p2119.pdf, p2127.pdf, p2137.pdf, p2147.pdf, p2157.pdf, p2167.pdf, p2177.pdf, p2187.pdf, p219.pdf, p2197.pdf, p2207.pdf, p2217.pdf, p2227.pdf, p2237.pdf, p2247.pdf, p2257.pdf, p2267.pdf, p2277.pdf, p2287.pdf, p229.pdf, p2297.pdf, p2307.pdf, p2309.pdf, p2311.pdf, p2313.pdf, p2315.pdf, p2317.pdf, p2319.pdf, p2321.pdf, p2323.pdf, p2325.pdf, p2327.pdf, p2329.pdf, p239.pdf, p249.pdf, p259.pdf, p269.pdf, p279.pdf, p289.pdf, p29.pdf, p299.pdf, p3.pdf, p309.pdf, p319.pdf, p329.pdf, p339.pdf, p349.pdf, p359.pdf, p369.pdf, p379.pdf, p387.pdf, p39.pdf, p397.pdf, p407.pdf, p417.pdf, p427.pdf, p437.pdf, p447.pdf, p457.pdf, p467.pdf, p477.pdf, p487.pdf, p49.pdf, p497.pdf, p5.pdf, p507.pdf, p517.pdf, p527.pdf, p537.pdf, p547.pdf, p557.pdf, p567.pdf, p577.pdf, p587.pdf, p59.pdf, p597.pdf, p607.pdf, p617.pdf, p627.pdf, p635.pdf, p645.pdf, p655.pdf, p665.pdf, p675.pdf, p685.pdf, p69.pdf, p695.pdf, p7.pdf, p705.pdf, p715.pdf, p725.pdf, p735.pdf, p745.pdf, p755.pdf, p765.pdf, p775.pdf, p785.pdf, p79.pdf, p805.pdf, p815.pdf, p825.pdf, p835.pdf, p845.pdf, p855.pdf, p865.pdf, p875.pdf, p885.pdf, p89.pdf, p895.pdf, p9.pdf, p905.pdf, p915.pdf, p925.pdf, p935.pdf, p945.pdf, p955.pdf, p965.pdf, p975.pdf, p985.pdf, p99.pdf, p995.pdf]\n"
     ]
    }
   ],
   "source": [
    "# Establish an empty list to append filenames as we iterate over the directory with filenames\n",
    "files = []\n",
    "\n",
    "%time\n",
    "start_time = time.time()\n",
    "\n",
    "#**********************************************************************\n",
    "# Core \"workerbee\" code for this section to iterate over directory files\n",
    "#**********************************************************************\n",
    "\n",
    "# Iterate over the directory of filenames and add to list.  Inspection shows our target filenames begin with 'p' and end with 'pdf'\n",
    "for dirName, subdirList, fileList in os.walk(TESTDIR):\n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            files.append(fileName)\n",
    "end_time = time.time()\n",
    "\n",
    "#**********************************************************************\n",
    "# Output\n",
    "#**********************************************************************\n",
    "print\n",
    "print len(files) # Print the number of files\n",
    "print \n",
    "print '[%s]' % ', '.join(map(str, files)) # print the list of filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>There are 253 total files in the directory. We examine the pdf file in its rawest form to get an idea of the format. Here is one example:<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"./files/journalscreencap.png\" alt=\"Sample of Journal Format\" height=\"700\" width=\"700\" top:\"35\" left:\"170\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br><br>We learn a few things immediately. Our data is in PDF format and it's semistructured (follows journal article format with sections like \"abstract\", \"title\").  PDFs are a wonderful human readable presentation of data. But for data analyisis, they are extremely difficult to work with.  If you have an option to get the data BEFORE it was converted to or added to PDF, go for that option.  Save yourself the headache.  In this case however, we have no alternatives outside of the web scraping code linked above.  The web scraping code is imperfect because it is incomplete (only get abstracts and not full-text of journal ariticle) and unordered (multiple authors need to be aligned to specific articles).<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion: Stripping text from PDFs and creating a custom NLTK corpus\n",
    "\n",
    "The first step in the <href id=\"pipe\"><a href=\"#pipeline\" title=\"Jump back to data science pipeline graphic.\">data science pipeline</a> is to ingest our data.  We use several Python tools which include:\n",
    "\n",
    "* [pdfminer](https://pypi.python.org/pypi/pdfminer/) - this is the tool that makes it ALL happen.  It has a command line tool called \"pdf2text.py\" that extract text contents from a PDF. **This must be installed on your computer BEFORE executing this code**.  Visit the [pdfminer homepage](http://euske.github.io/pdfminer/index.html#pdf2txt) for instructions\n",
    "\n",
    "* [subprocess](https://docs.python.org/2/library/subprocess.html) - a standard library module that allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes.  In this excerise, we use it to invoke the pdf2texy.py command line tool within our code.  \n",
    "\n",
    "* [nltk](http://www.nltk.org/) - another work horse in this exercise.  The Natural Language ToolKit (NLTK) is one of Python's leading platforms to analyze natural language data.  The [NLTK Book](http://www.nltk.org/book/) provides practical guidance on how to handle just about any natural language preprocessing job.  \n",
    "\n",
    "* [string](https://docs.python.org/2/library/string.html) - used for variable substitutions and value formatting to strip non printable characters from the output of the text extracted from our journal article PDFs\n",
    "\n",
    "* [unicodedata](https://docs.python.org/2/library/unicodedata.html) - some unicode characters won't extract nicely. This library allows latin unicode characters to degrade gracefully into ASCII.\n",
    "\n",
    "We are now going to iterate over each file in our raw data directory, strip the text, and write the *.txt* file to newly created directory.  Then we will follow the instructions from [Section 1.9, Chapter 2 of NLTK's Book](http://www.nltk.org/book/ch02.html) to build a custom corpus from our text files.  Having our target documents loaded as an NLTK corpus brings the power of NLTK to our analysis goals.  Let's begin with administrative tasks such as loading modules and creating the necessary directories.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Importing what we need\n",
    "#**********************************************************************\n",
    "import string\n",
    "import unicodedata\n",
    "import subprocess\n",
    "import nltk\n",
    "import os, os.path\n",
    "import re\n",
    "\n",
    "#**********************************************************************\n",
    "# Create the directory we will write the .txt files to after stripping text\n",
    "#**********************************************************************\n",
    "\n",
    "corpuspath = os.path.normpath(os.path.expanduser('~/Desktop/KDD_corpus/'))\n",
    "if not os.path.exists(corpuspath):\n",
    "    os.mkdir(corpuspath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Now we are to the big task of stripping text from the PDFs.  In the code below, we walk down the directory, and strip text from the files with names that begin with 'p' and end with 'pdf'.  We use the *fileName* variable to name the files we write to disk.  This will come in handy when we load data into NLTK.  Keep in mind, this task takes the longest, so be prepared to wait a a few minutes depending on good your computer is.  If you are doing this in an environment where you can spin up compute resources, your time will be drastically reduced.  Let's begin.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Core code to iterate over files in the directory\n",
    "#**********************************************************************\n",
    "\n",
    "# We start from the code to iterate over the files\n",
    "%timeit\n",
    "for dirName, subdirList, fileList in os.walk(TESTDIR):\n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            if os.path.exists(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\"))):\n",
    "                pass\n",
    "            else:\n",
    "            \n",
    "            \n",
    "#**********************************************************************\n",
    "# This code strips the text from the PDFs\n",
    "#**********************************************************************\n",
    "                try:\n",
    "                    document = filter(lambda x: x in string.printable,unicodedata.normalize('NFKD', (unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore'))).encode('ascii','ignore').decode('unicode_escape').encode('ascii','ignore'))\n",
    "                except UnicodeDecodeError:\n",
    "                    document = unicodedata.normalize('NFKD', unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore')).encode('ascii','ignore')    \n",
    "\n",
    "                if len(document)<300:\n",
    "                    pass\n",
    "                else:\n",
    "                    # used this for assistance http://stackoverflow.com/questions/2967194/open-in-python-does-not-create-a-file-if-it-doesnt-exist\n",
    "                    if not os.path.exists(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\"))):\n",
    "                        file = open(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\")), 'w+')\n",
    "                        file.write(document)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "kddcorpus= nltk.corpus.PlaintextCorpusReader(corpuspath, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kddcorpus= nltk.corpus.PlaintextCorpusReader(corpuspath, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>This is a pretty big step.  We have a semi-structured data set in a format where we can query and analyze different pieces of data.  All of our data is loaded as an NLTK corpus, meaning we could try tons of techniques outlined in the [NLTK book](http://www.nltk.org/book/) or use the NLTK APIs to pass data into [scikit-learn machine learning pipelines for text](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) (maybe for a later blog). Let's see how many words (including stop words) we have in our entire corpus.  <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2795267\n"
     ]
    }
   ],
   "source": [
    "wordcount = 0\n",
    "for fileid in kddcorpus.fileids():\n",
    "    wordcount += len(kddcorpus.words(fileid))\n",
    "print wordcount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>To begin exploration of regular expressions, let's extract 'good enough' titles from a few of the documents.  For help on regex, visit https://regex101.com/. Here are the titles for the first 26 papers. <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online Controlled Experiments:\n",
      "Mining Frequent Itemsets through Progressive Sampling\n",
      "Why It Happened: Identifying and Modeling the Reasons of\n",
      "Matrix Completion with Queries\n",
      "Stochastic Divergence Minimization\n",
      "Bayesian Poisson Tensor Factorization for Inferring\n",
      "TimeCrunch: Interpretable Dynamic Graph Summarization\n",
      "Inside Jokes: Identifying Humorous Cartoon Captions\n",
      "Community Detection based on Distance Dynamics\n",
      "Discovery of Meaningful Rules in Time Series\n",
      "On the Formation of Circles in Co-authorship Networks\n",
      "An Evaluation of Parallel Eccentricity Estimation\n",
      "Efcient Latent Link Recommendation in\n",
      "Turn Waste into Wealth: On Simultaneous Clustering and\n",
      "Set Cover at Web Scale\n",
      "Exploiting Relevance Feedback in Knowledge Graph\n",
      "LINKAGE: An Approach for Comprehensive Risk\n",
      "Transitive Transfer Learning\n",
      "PTE: Predictive Text Embedding through Large-scale\n",
      "An Effective Marketing Strategy for Revenue Maximization\n",
      "Scaling Up Stochastic Dual Coordinate Ascent\n",
      "Heterogeneous Network Embedding via Deep\n",
      "Discovering Valuable Items from Massive Data\n",
      "Deep Learning Architecture with Dynamically Programmed\n",
      "Incorporating World Knowledge to Document Clustering\n"
     ]
    }
   ],
   "source": [
    "# code uses regular expression to extract text up to the first new line character\n",
    "\n",
    "p=re.compile('(.+)(\\\\n)')\n",
    "for fileid in kddcorpus.fileids()[:25]:\n",
    "    print p.search(kddcorpus.raw(fileid)).group(1).strip()  # use .strip() to remove whitespace from beginning and end of string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling and computation: Using Regular Expressions to extract specific sections of the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are close to the NERC portion.  But, there's a bit more wrangling to do (remember, PDFs are tough work).  For simplicity, let's focus the NERC on two sections of the paper:\n",
    "* the top section which includes authors and schools\n",
    "* the references section of the paper (keynote speaker abstracts do not have an abstract)\n",
    "\n",
    "The tools of choice to extract sections are the [\"positive lookbehind\" and \"positive lookahead\"](https://docs.python.org/2/library/re.html) expressions. Here is an example of code to extract the abstract only:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The collapsed variational Bayes zero (CVB0) inference is a vari- ational inference improved by marginalizing out parameters, the same as with the collapsed Gibbs sampler. A drawback of the CVB0 inference is the memory requirements. A probability vec- tor must be maintained for latent topics for every token in a corpus. When the total number of tokens is N and the number of topics is K, the CVB0 inference requires O(N K) memory. A stochas- tic approximation of the CVB0 (SCVB0) inference can reduce O(N K) to O(V K), where V denotes the vocabulary size. We re- formulate the existing SCVB0 inference by using the stochastic di- vergence minimization algorithm, with which convergence can be analyzed in terms of Martingale convergence theory. We also reveal the property of the CVB0 inference in terms of the leave-one-out perplexity, which leads to the estimation algorithm of the Dirichlet distribution parameters. The predictive performance of the propose SCVB0 inference is better than that of the original SCVB0 infer- ence in four datasets.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set our regular expression\n",
    "p= re.compile('(?<=ABSTRACT)(.+)(?=Categories and Subject Descriptors)')\n",
    "try:\n",
    "    abstract= p.search(re.sub('[\\s]',\" \",kddcorpus.raw('p1035.txt'))).group(1)\n",
    "except AttributeError:\n",
    "    # include a lowercase regex match incase consistency is a problem\n",
    "    p=re.compile('(?<=abstract)(.+)(?=categories and subject descriptors)')\n",
    "    abstract=p.search(re.sub('[\\s]',\" \",holder.lower())).group(1)\n",
    "else:\n",
    "    pass\n",
    "unicodedata.normalize('NFKD', abstract).encode('ascii','ignore').strip() # convert output from unicode to string and strip leading and trailing whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sup id=\"fn1\">1. [(2014). Text Mining and its Business Applications - CodeProject. Retrieved December 26, 2015, from http://www.codeproject.com/Articles/822379/Text-Mining-and-its-Business-Applications.]<a href=\"#ref1\" title=\"Jump back to footnote 1 in the text.\">↩</a></sup>\n",
    "\n",
    "<sup id=\"fn2\">2. [Suchanek, F., & Weikum, G. (2013). Knowledge harvesting in the big-data era. Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data. ACM.]<a href=\"#ref2\" title=\"Jump back to footnote 2 in the text.\">↩</a></sup>\n",
    "\n",
    "\n",
    "<sup id =\"fn3\">3. [Nadeau, D., & Sekine, S. (2007). A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1), 3-26.]<a href=\"#ref3\" title = \"Jump back to footnote 3 in the text\">↩</a></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Parking Lot of links, leftover paragraphs, ideas, etc.</span>\n",
    "\n",
    "Describe the data -> Data available here http://dl.acm.org/citation.cfm?id=2783258# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<href id=\"pipe\"><a href=\"#pipeline\" title=\"Jump back to data science pipeline graphic.\">data science pipeline</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ben's Outline from email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ~~Give a brief introduction to the task, and why it's interesting, important. Then begin to discuss the data set, how you acquired, and where a reader can get access to it.~~ \n",
    "\n",
    "* ~~You then could have a data exploration section where you show the number of documents, perform a word count, show snippets of data (e.g. references) etc that are of interest.~~\n",
    "\n",
    "* You can then go through one or a few of your \"code to get\" sections. These functions all follow basically the same pattern, so you could probably merge them into a single function, that appropriately selects the right regular expression. \n",
    "\n",
    "* The next step is to discuss, demonstrate your \"truth tests\" for text extraction accuracy. \n",
    "\n",
    "* Finally, you can get to an introduction of your three methods for NERC, and show how do do each of them. Then compare (visually) the results of the three according to the evaluation mechanism discussed above. \n",
    "\n",
    "* You could then conclude with a discussion about NLTK chunk vs. hand labelled entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kddcorpus_bigrams=[]\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "for fileid in kddcorpus.fileids():\n",
    "    for l in (BigramCollocationFinder.from_words(kddcorpus.words(fileid)).nbest(bigram_measures.pmi, 10)):\n",
    "        kddcorpus_bigrams.append(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Prototype Holder</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# completed gold standard for keywords, got all of them..no stragglers\n",
    "text = kddcorpus.raw('p39.txt').lower()\n",
    "failids = []\n",
    "full = True\n",
    "section = \"keywords\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid).lower()\n",
    "        if section == \"keywords\":\n",
    "            section1=\"keywords\"\n",
    "            target = \"\"   \n",
    "            section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"permission to make \",\"1.motivation\" ]\n",
    "        \n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    if len(target) >50:\n",
    "                        if len(target) > 300:\n",
    "                            target = target[:200]\n",
    "                        else:\n",
    "                            target = target\n",
    "                        \n",
    "                        print [fileid,target,len(text)]\n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        failids.append(fileid)\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "else:\n",
    "    section = \"keywords\"\n",
    "    \n",
    "    if section == \"keywords\":\n",
    "        section1=\"keywords\"\n",
    "        target = \"\"   \n",
    "        section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"permission to make \",\"1.motivation\" ]\n",
    "\n",
    "        part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "        for sect in section2:\n",
    "            try:\n",
    "                part2 = \"(?=\"+str(sect)+\")\"\n",
    "                p=re.compile(part1+part2)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                if target > 3:                 \n",
    "                    break                  \n",
    "            except:\n",
    "                pass\n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# completed gold standard for abstract, near zero stragglers\n",
    "failids = []\n",
    "text=kddcorpus.raw('p1055.txt')\n",
    "\n",
    "full = True\n",
    "section = \"abstract\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid).lower()\n",
    "        if section == \"abstract\":\n",
    "            section1=\"abstract\"\n",
    "            target = \"\"   \n",
    "            section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if len(target) > 50:\n",
    "                        \n",
    "                        \n",
    "                        print [fileid,len(target),len(text)]\n",
    "                        break\n",
    "                    else:\n",
    "                        failids.append(fileid)\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    \n",
    "                    pass\n",
    "                              \n",
    "else:\n",
    "    \n",
    "    section = \"abstract\"\n",
    "    text = kddcorpus.raw('p1627.txt').lower()\n",
    "    if section == \"abstract\":\n",
    "        section1=\"abstract\"\n",
    "        target = \"\"   \n",
    "        section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "\n",
    "        part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "        for sect in section2:\n",
    "            try:\n",
    "                part2 = \"(?=\"+str(sect)+\")\"\n",
    "                p=re.compile(part1+part2)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                if target > 50:\n",
    "\n",
    "                    \n",
    "                    break\n",
    "            except:\n",
    "                \n",
    "                pass\n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# completed gold standard for references, counts number of references and does \"word per reference\" score\n",
    "\n",
    "failids = []\n",
    "text=kddcorpus.raw('p149.txt').lower()\n",
    "\n",
    "full = True\n",
    "section = \"references\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid).lower()\n",
    "        if section == \"references\":\n",
    "            section1=\"references \\[\" \n",
    "            target = \"\"   \n",
    "            \n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "            \n",
    "            for sect in section1:\n",
    "                try:\n",
    "                    p=re.compile(part1)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "\n",
    "                    if len(target) > 50:\n",
    "                        \n",
    "                        # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                        try:\n",
    "                            if 'references' in locals():\n",
    "\n",
    "                                refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                        except:\n",
    "                            print \"This file does not appear to have a references section\"\n",
    "                            pass\n",
    "                        print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                        break\n",
    "                    else:\n",
    "                        \n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "                              \n",
    "else:\n",
    "    \n",
    "    section = \"references\"\n",
    "    \n",
    "    if section == \"references\":\n",
    "        section1=\"references \\[\"\n",
    "        target = \"\"   \n",
    "        \n",
    "        part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "        for sect in section1:\n",
    "            try:\n",
    "                \n",
    "                p=re.compile(part1)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                if target > 50:\n",
    "                    print len(target)\n",
    "\n",
    "                    \n",
    "                break\n",
    "            except:\n",
    "                \n",
    "                pass\n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(failids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gold standard to get top section\n",
    "\n",
    "failids = []\n",
    "text=kddcorpus.raw('p1623.txt').lower()\n",
    "\n",
    "full = True\n",
    "section = \"top\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid).lower()\n",
    "        if section == \"top\":\n",
    "            section1=\"\"\n",
    "            section2=[\"abstract\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if len(target)> 1000:\n",
    "                        if len(target) > 3000 and float(len(target))/float(len(text)) > .22:\n",
    "                            target = target[:2500]\n",
    "                        else:\n",
    "                            target=target\n",
    "                   \n",
    "                        print [fileid,len(target),len(text)]\n",
    "                        break\n",
    "                    else:\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "    print \"Done\"\n",
    "                              \n",
    "else:\n",
    "    \n",
    "    if section == \"top\":\n",
    "            section1=\"\"\n",
    "            section2=[\"abstract\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if len(target)> 1000:\n",
    "                        if len(target) > 3000 and float(len(target))/float(len(text)) > .22:\n",
    "                            target = target[:2500]\n",
    "                        else:\n",
    "                            target=target\n",
    "                   \n",
    "                        print [fileid,len(target),len(text)]\n",
    "                        break\n",
    "                     \n",
    "                    else:\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "failids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:violet\">Drawing Board/Assembly Line</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attempting function with gold keywords....WORKS\n",
    "\n",
    "def keypull(docnum=None,section='keywords',full = False):\n",
    "    \n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "\n",
    "        \n",
    "\n",
    "        # to return output from entire corpus\n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid).lower()\n",
    "                if section == \"keywords\":\n",
    "                    section1=\"keywords\"\n",
    "                    target = \"\"   \n",
    "                    section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"(1. tutorial )\",\" permission to make \",\"  permission to make\",\"(  permission to make digital )\",\"    bio  \",\"abstract:  \",\"1.motivation\" ]\n",
    "\n",
    "                    part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "                    for sect in section2:\n",
    "                        try:\n",
    "                            part2 = \"(?=\"+str(sect)+\")\"\n",
    "                            p=re.compile(part1+part2)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                            if len(target) >50:\n",
    "                                if len(target) > 300:\n",
    "                                    target = target[:200]\n",
    "                                else:\n",
    "                                    target = target\n",
    "\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"keywords\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                #print [fileid,len(target),len(text)]\n",
    "                                break\n",
    "                            else:\n",
    "                                if len(target)==0:\n",
    "                                     failids.append(fileid)   \n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "            set(failids)\n",
    "            return ans\n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        if full == False:\n",
    "            if section == \"keywords\":\n",
    "                section1=\"keywords\"\n",
    "                target = \"\"   \n",
    "                section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"permission to make \",\"1.motivation\" ]\n",
    "\n",
    "                part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "                for sect in section2:\n",
    "                    try:\n",
    "                        part2 = \"(?=\"+str(sect)+\")\"\n",
    "                        p=re.compile(part1+part2)\n",
    "                        target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                        if len(target) >50:\n",
    "                            if len(target) > 300:\n",
    "                                target = target[:200]\n",
    "                            else:\n",
    "                                target = target\n",
    "                            ans[docnum]={}\n",
    "                            ans[docnum][\"keywords\"]=target.strip()\n",
    "                            ans[docnum][\"charcount\"]=len(target)\n",
    "                            break                  \n",
    "                    except:\n",
    "                        pass\n",
    "    return ans\n",
    "    return failids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attempting function with gold abstracts...WORKS\n",
    "\n",
    "def abpull(docnum=None,section='abstract',full = False):\n",
    "    \n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        # to return output from entire corpus\n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid).lower()\n",
    "                if section == \"abstract\":\n",
    "                    section1=\"abstract\"\n",
    "                    target = \"\"   \n",
    "                    section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "                    part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "                    for sect in section2:\n",
    "                        try:\n",
    "                            part2 = \"(?=\"+str(sect)+\")\"\n",
    "                            p=re.compile(part1+part2)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "\n",
    "                            if len(target) > 50:\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"abstract\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                \n",
    "                                #print [fileid,len(target),len(text)]\n",
    "                                break\n",
    "                            else:\n",
    "                                failids.append(fileid)\n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            pass\n",
    "            return ans\n",
    "                              \n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum).lower()\n",
    "        if section == \"abstract\":\n",
    "            section1=\"abstract\"\n",
    "            target = \"\"   \n",
    "            section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if target > 50:\n",
    "                        ans[str(docnum)]={}\n",
    "                        ans[str(docnum)][\"abstract\"]=target.strip()\n",
    "                        ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "        return ans\n",
    "        return failids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = abpull('p19.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print abstracts;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, value in abstracts.iteritems():\n",
    "    print value['charcount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "print len(keywords.keys())\n",
    "print len(set(failids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print set(failids)\n",
    "print\n",
    "print\n",
    "print set(failids) & set(keywords.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Testing Station</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?<=keywords)(.+)(?=introduction  )\n"
     ]
    }
   ],
   "source": [
    "print part1+part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning a Hierarchical Monitoring System for Detecting  and Diagnosing Service Issues  Vinod Nair, Ameya Raul  Microsoft Research India  {vnair,  t-amraul}@microsoft.com  Shwetabh Khanduja Microsoft Research India  t-shwetk@microsoft.com  Vikas Bahirwani  Microsoft, Redmond, WA  vikasba@microsoft.com  S. Sundararajan  Microsoft Research India ssrajan@microsoft.com  Sathiya Keerthi  Microsoft, Mountain View, CA keerthi@microsoft.com  Steve Herbert,  Sudheer Dhulipalla Microsoft, Redmond, WA {steve.herbert, sud-  heerd}@microsoft.com  ABSTRACT We propose a machine learning based framework for build- ing a hierarchical monitoring system to detect and diagnose service issues. We demonstrate its use for building a moni- toring system for a distributed data storage and computing service consisting of tens of thousands of machines. Our solution has been deployed in production as an end-to-end system, starting from telemetry data collection from indi- vidual machines, to a visualization tool for service operators to examine the detection outputs. Evaluation results are presented on detecting 19 customer impacting issues in the past three months.  1.  INTRODUCTION  Consumer and enterprise services increasingly run on large scale distributed storage and computing platforms built on hundreds of thousands of commodity machines. Service quality can be aected by hardware and software failures, unexpected user load changes, and so on, resulting in slow re- sponse, unavailability, violation of service level agreements, and ultimately customer dissatisfaction and negative repu- tation. Therefore monitoring the service to detect and diag- nose issues quickly is an important problem. Recently there has been a steady increase in the literature in this area, from designing incident ticket management systems [13, 15], to building large scale textual log and time series analyzers [8, 20], developing specialized machine learning and data min- ing algorithms for anomaly detection [4, 9, 19], and many others [1, 2, 6, 18, 22].  There are several challenges in building a service mon- itoring system. Telemetry data manageability: A typical service is heavily instrumented by sensors of various kinds (e.g., performance counters, textual logs, event streams), re-  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = kddcorpus.raw('p2029.txt')\n",
    "p=re.compile('(.*)(?=abstract)')\n",
    "top = p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "if len(top)> 3000 and float(len(top))/float(len(text)) > .22:\n",
    "    top = top[:2500]\n",
    "print top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
