{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A ~~Quick~~ Survey and Comparison of Open Source Named Entity Extractor Tools for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity extraction is a core subtask of building knowledge from semi/unstructured text sources<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup>.  Considering recent increases in computing power and decreases in the costs of data storage, data scientists and developers can build large knowledge bases that contain millions of entities and hundreds of millions of facts about them.  These knowledge bases are key contributors to intelligence computer behavior<sup><a href=\"#fn2\" id=\"ref2\">2</a></sup>.  Therefore, named entity extraction is at the core of several popular technologies such as smart assistants ([Siri](http://www.apple.com/ios/siri/), [Google Now](https://www.google.com/landing/now/)), machine reading, and deep interpretation of natural language<sup><a href=\"#fn3\" id=\"ref3\">3</a></sup>.\n",
    "\n",
    "With a realization of how essential it is to recognize information units like names, including person, organization and location names, and numeric expressions including time, date, money\n",
    "and percent expressions, several questions come to mind.  How do you perform named entity extraction, which is formally called “[Named Entity Recognition and Classification (NERC)](https://benjamins.com/catalog/bct.19)”?  What tools are out there?  How can you evaluate their performance?  And most important, what works with Python (shamelessly exposing my bias)?  \n",
    "\n",
    "This post will survey openly available NERC tools and compare the results against hand labeled data for precision, accuracy, and recall.  The tools and basic information extraction principles in this discussion begin the process of structuring unstructured data.\n",
    "\n",
    "We will specifically learn to:\n",
    "1. follow the data science pipeline (see image below)\n",
    "2. prepare semistructured natural language data for ingest using regex\n",
    "3. create a custom corpus in [Natural Language Toolkit](http://www.nltk.org/) \n",
    "4. use a suite of openly available NERC tools to extract entities and store in json format \n",
    "5. compare the performance of NERC tools on our corpus\n",
    "\n",
    "<br>\n",
    "<a href=\"#pipe\" id=\"pipeline\"><center><h3>The Data Science Pipeline:<br>Georgetown Data Science Certificate Program</h3></center></a>\n",
    "<div class=\"image\">\n",
    "\n",
    "      <img src=\"./files/data_science_pipeline.png\" alt=\"Data Science Pipeline\" height=\"300\" width=\"450\" top:\"35\" left:\"170\" />\n",
    "      \n",
    "      \n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data: Peer Reviewed Journals and Keynote Speaker Abstracts from KDD 2014 and 2015\n",
    "\n",
    "Before delving into the pipeline, we need a good dataset.  Jason Brownlee of www.machinelearningmastery.com had some good suggestions in his [August 2015 article](http://machinelearningmastery.com/practice-machine-learning-with-small-in-memory-datasets-from-the-uci-machine-learning-repository/) on picking a dataset for machine learning exercises:  \n",
    "\n",
    "* **Real-World**: The datasets should be drawn from the real world (rather than being contrived). This will keep them interesting and introduce the challenges that come with real data.\n",
    "\n",
    "* **Small**: The datasets need to be small so that you can inspect and understand them and that you can run many models quickly to accelerate your learning cycle.\n",
    "\n",
    "* **Well-Understood**: There should be a clear idea of what the data contains, why it was collected, what the problem is that needs to be solved so that you can frame your investigation.\n",
    "\n",
    "* **Baseline**: It is also important to have an idea of what algorithms are known to perform well and the scores they achieved so that you have a useful point of comparison. This is important when you are getting started and learning because you need quick feedback as to how well you are performing (close to state-of-the-art or something is broken).\n",
    "\n",
    "* **Plentiful**: You need many datasets to choose from, both to satisfy the traits you would like to investigate and (if possible) your natural curiosity and interests. \n",
    "\n",
    "Luckily, we have a dataset that meets nearly all of these requirements.  I attended the Knowledge Discovery and Data Mining (KDD) conferences in [New York City (2014)](http://www.kdd.org/kdd2014/) and [Sydney, Australia (2015)](http://www.kdd.org/kdd2015/).  Both years, attendees received a USB with the conference proceedings.  Each repository contains over 230 peer reviewed journal articles and keynote speaker abstracts on data mining, knowledge discovery, big data, data science and their applications. The full conference proceedings can be purchased for \\$60 at the [Association for Computing Machinery's Digital Library](https://dl.acm.org/purchase.cfm?id=2783258&CFID=740512201&CFTOKEN=34489585) (includes ACM membership). This post will work with a dataset that is equivalent to the conference proceedings.  It's important to note that this dataset recreates a real word data science exercise that is instructive of big data problems.  We will take semi-structured data (PDF journal articles and abstracts in publication format), strip text from the files, and add more structure to the data that would facilitate follow on analysis. \n",
    "\n",
    "<blockquote cite=\"https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/KDDwebscrape.ipynb\">\n",
    "Interested parties looking for a free option can use the <a href=\"https://pypi.python.org/pypi/beautifulsoup4/4.4.1\">beautifulsoup</a> and <a href=\"https://pypi.python.org/pypi/requests/2.9.1\">request</a> libraries to scrape the <a href=\"http://dl.acm.org/citation.cfm?id=2785464&CFID=740512201&CFTOKEN=3448958\">ACM website for KDD 2015 conference data</a> that can be used in natural language processing pipelines.  I have some <a href=\"https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/KDDwebscrape.ipynb\">skeleton web scraping code</a> to generate lists of all abstracts, author names, and journal/keynote address titles.    \n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration: Getting the number of files, and file type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored locally in the following directory:\n",
    "```python\n",
    ">>> import os\n",
    ">>> print os.getcwd()\n",
    "/Users/linwood/Desktop/KDD_15/docs\n",
    "```\n",
    "Let's explore the number of files we have and naming conventions. We begin with the administrative tasks of loading modules, establishing paths, etc.  \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Importing what we need\n",
    "#**********************************************************************\n",
    "import os\n",
    "import time\n",
    "from os import walk\n",
    "\n",
    "#**********************************************************************\n",
    "# Administrative code to set the path for file loading\n",
    "#**********************************************************************\n",
    "\n",
    "path        = os.path.abspath(os.getcwd())\n",
    "TESTDIR     = os.path.normpath(os.path.join(os.path.expanduser(\"~\"),\"Desktop\",\"KDD_15\",\"docs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Next we iterate over the files in the directory and store those names in the empty list we created called *files*.  We time the operation, print list with the file names and also print out the length of the list (gives number of target files).<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n",
      "\n",
      "253\n",
      "\n",
      "[p1.pdf, p1005.pdf, p1015.pdf, p1025.pdf, p1035.pdf, p1045.pdf, p1055.pdf, p1065.pdf, p1075.pdf, p1085.pdf, p109.pdf, p1095.pdf, p1105.pdf, p1115.pdf, p1125.pdf, p1135.pdf, p1145.pdf, p1155.pdf, p1165.pdf, p1175.pdf, p1185.pdf, p119.pdf, p1195.pdf, p1205.pdf, p1215.pdf, p1225.pdf, p1235.pdf, p1245.pdf, p1255.pdf, p1265.pdf, p1275.pdf, p1285.pdf, p129.pdf, p1295.pdf, p1305.pdf, p1315.pdf, p1325.pdf, p1335.pdf, p1345.pdf, p1355.pdf, p1365.pdf, p1375.pdf, p1385.pdf, p139.pdf, p1395.pdf, p1405.pdf, p1415.pdf, p1425.pdf, p1435.pdf, p1445.pdf, p1455.pdf, p1465.pdf, p1475.pdf, p1485.pdf, p149.pdf, p1495.pdf, p1503.pdf, p1513.pdf, p1523.pdf, p1533.pdf, p1543.pdf, p1553.pdf, p1563.pdf, p1573.pdf, p1583.pdf, p159.pdf, p1593.pdf, p1603.pdf, p1621.pdf, p1623.pdf, p1625.pdf, p1627.pdf, p1629.pdf, p1631.pdf, p1633.pdf, p1635.pdf, p1637.pdf, p1639.pdf, p1641.pdf, p1651.pdf, p1661.pdf, p1671.pdf, p1681.pdf, p169.pdf, p1691.pdf, p1701.pdf, p1711.pdf, p1721.pdf, p1731.pdf, p1741.pdf, p1751.pdf, p1759.pdf, p1769.pdf, p1779.pdf, p1789.pdf, p179.pdf, p1799.pdf, p1809.pdf, p1819.pdf, p1829.pdf, p1839.pdf, p1849.pdf, p1859.pdf, p1869.pdf, p1879.pdf, p1889.pdf, p189.pdf, p1899.pdf, p19.pdf, p1909.pdf, p1919.pdf, p1929.pdf, p1939.pdf, p1949.pdf, p1959.pdf, p1969.pdf, p1979.pdf, p1989.pdf, p199.pdf, p1999.pdf, p2009.pdf, p2019.pdf, p2029.pdf, p2039.pdf, p2049.pdf, p2059.pdf, p2069.pdf, p2079.pdf, p2089.pdf, p209.pdf, p2099.pdf, p2109.pdf, p2119.pdf, p2127.pdf, p2137.pdf, p2147.pdf, p2157.pdf, p2167.pdf, p2177.pdf, p2187.pdf, p219.pdf, p2197.pdf, p2207.pdf, p2217.pdf, p2227.pdf, p2237.pdf, p2247.pdf, p2257.pdf, p2267.pdf, p2277.pdf, p2287.pdf, p229.pdf, p2297.pdf, p2307.pdf, p2309.pdf, p2311.pdf, p2313.pdf, p2315.pdf, p2317.pdf, p2319.pdf, p2321.pdf, p2323.pdf, p2325.pdf, p2327.pdf, p2329.pdf, p239.pdf, p249.pdf, p259.pdf, p269.pdf, p279.pdf, p289.pdf, p29.pdf, p299.pdf, p3.pdf, p309.pdf, p319.pdf, p329.pdf, p339.pdf, p349.pdf, p359.pdf, p369.pdf, p379.pdf, p387.pdf, p39.pdf, p397.pdf, p407.pdf, p417.pdf, p427.pdf, p437.pdf, p447.pdf, p457.pdf, p467.pdf, p477.pdf, p487.pdf, p49.pdf, p497.pdf, p5.pdf, p507.pdf, p517.pdf, p527.pdf, p537.pdf, p547.pdf, p557.pdf, p567.pdf, p577.pdf, p587.pdf, p59.pdf, p597.pdf, p607.pdf, p617.pdf, p627.pdf, p635.pdf, p645.pdf, p655.pdf, p665.pdf, p675.pdf, p685.pdf, p69.pdf, p695.pdf, p7.pdf, p705.pdf, p715.pdf, p725.pdf, p735.pdf, p745.pdf, p755.pdf, p765.pdf, p775.pdf, p785.pdf, p79.pdf, p805.pdf, p815.pdf, p825.pdf, p835.pdf, p845.pdf, p855.pdf, p865.pdf, p875.pdf, p885.pdf, p89.pdf, p895.pdf, p9.pdf, p905.pdf, p915.pdf, p925.pdf, p935.pdf, p945.pdf, p955.pdf, p965.pdf, p975.pdf, p985.pdf, p99.pdf, p995.pdf]\n"
     ]
    }
   ],
   "source": [
    "# Establish an empty list to append filenames as we iterate over the directory with filenames\n",
    "files = []\n",
    "\n",
    "%time\n",
    "start_time = time.time()\n",
    "\n",
    "#**********************************************************************\n",
    "# Core \"workerbee\" code for this section to iterate over directory files\n",
    "#**********************************************************************\n",
    "\n",
    "# Iterate over the directory of filenames and add to list.  Inspection shows our target filenames begin with 'p' and end with 'pdf'\n",
    "for dirName, subdirList, fileList in os.walk(TESTDIR):\n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            files.append(fileName)\n",
    "end_time = time.time()\n",
    "\n",
    "#**********************************************************************\n",
    "# Output\n",
    "#**********************************************************************\n",
    "print\n",
    "print len(files) # Print the number of files\n",
    "print \n",
    "print '[%s]' % ', '.join(map(str, files)) # print the list of filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>There are 253 total files in the directory. We examine the pdf file in its rawest form to get an idea of the format. Here is one example:<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"./files/journalscreencap.png\" alt=\"Sample of Journal Format\" height=\"700\" width=\"700\" top:\"35\" left:\"170\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br><br>We learn a few things immediately. Our data is in PDF format and it's semistructured (follows journal article format with sections like \"abstract\", \"title\").  PDFs are a wonderful human readable presentation of data. But for data analyisis, they are extremely difficult to work with.  If you have an option to get the data BEFORE it was converted to or added to PDF, go for that option.  If it's your only option, be prepared for a lot of these moments:\n",
    "\n",
    "![Pulling hair out](http://i1012.photobucket.com/albums/af243/njmike731/man-pulling-hair-out-2-773892-1.jpg)\n",
    "\n",
    "In today's exercise, we have no alternatives outside of the web scraping code linked above.  In full disclosure, that code is imperfect because we get an incomplete dataset.  The abstracts and authors are not matched to the papers and we don't pull in the references section. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion: Stripping text from PDFs and creating a custom NLTK corpus\n",
    "\n",
    "The first step in the <href id=\"pipe\"><a href=\"#pipeline\" title=\"Jump back to data science pipeline graphic.\">data science pipeline</a> is to ingest our data.  We use several Python tools which include:\n",
    "\n",
    "* [pdfminer](https://pypi.python.org/pypi/pdfminer/) - this is the tool that makes it ALL happen.  It has a command line tool called \"pdf2text.py\" that extract text contents from a PDF. **This must be installed on your computer BEFORE executing this code**.  Visit the [pdfminer homepage](http://euske.github.io/pdfminer/index.html#pdf2txt) for instructions\n",
    "\n",
    "* [subprocess](https://docs.python.org/2/library/subprocess.html) - a standard library module that allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes.  In this excerise, we use it to invoke the pdf2texy.py command line tool within our code.  \n",
    "\n",
    "* [nltk](http://www.nltk.org/) - another work horse in this exercise.  The Natural Language ToolKit (NLTK) is one of Python's leading platforms to analyze natural language data.  The [NLTK Book](http://www.nltk.org/book/) provides practical guidance on how to handle just about any natural language preprocessing job.  \n",
    "\n",
    "* [string](https://docs.python.org/2/library/string.html) - used for variable substitutions and value formatting to strip non printable characters from the output of the text extracted from our journal article PDFs\n",
    "\n",
    "* [unicodedata](https://docs.python.org/2/library/unicodedata.html) - some unicode characters won't extract nicely. This library allows latin unicode characters to degrade gracefully into ASCII.\n",
    "\n",
    "We are now going to iterate over each file in our raw data directory, strip the text, and write the *.txt* file to newly created directory.  Then we will follow the instructions from [Section 1.9, Chapter 2 of NLTK's Book](http://www.nltk.org/book/ch02.html) to build a custom corpus from our text files.  Having our target documents loaded as an NLTK corpus brings the power of NLTK to our analysis goals.  Let's begin with administrative tasks such as loading modules and creating the necessary directories.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Importing what we need\n",
    "#**********************************************************************\n",
    "import string\n",
    "import unicodedata\n",
    "import subprocess\n",
    "import nltk\n",
    "import os, os.path\n",
    "import re\n",
    "\n",
    "#**********************************************************************\n",
    "# Create the directory we will write the .txt files to after stripping text\n",
    "#**********************************************************************\n",
    "\n",
    "corpuspath = os.path.normpath(os.path.expanduser('~/Desktop/KDD_corpus/'))\n",
    "if not os.path.exists(corpuspath):\n",
    "    os.mkdir(corpuspath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Now we are to the big task of stripping text from the PDFs.  In the code below, we walk down the directory, and strip text from the files with names that begin with 'p' and end with 'pdf'.  We use the *fileName* variable to name the files we write to disk.  This will come in handy when we load data into NLTK.  Keep in mind, this task takes the longest, so be prepared to wait a a few minutes depending on good your computer is.  If you are doing this in an environment where you can spin up compute resources, your time will be drastically reduced.  Let's begin.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Core code to iterate over files in the directory\n",
    "#**********************************************************************\n",
    "\n",
    "# We start from the code to iterate over the files\n",
    "%timeit\n",
    "for dirName, subdirList, fileList in os.walk(TESTDIR):\n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            if os.path.exists(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\"))):\n",
    "                pass\n",
    "            else:\n",
    "            \n",
    "            \n",
    "#**********************************************************************\n",
    "# This code strips the text from the PDFs\n",
    "#**********************************************************************\n",
    "                try:\n",
    "                    document = filter(lambda x: x in string.printable,unicodedata.normalize('NFKD', (unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore'))).encode('ascii','ignore').decode('unicode_escape').encode('ascii','ignore'))\n",
    "                except UnicodeDecodeError:\n",
    "                    document = unicodedata.normalize('NFKD', unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore')).encode('ascii','ignore')    \n",
    "\n",
    "                if len(document)<300:\n",
    "                    pass\n",
    "                else:\n",
    "                    # used this for assistance http://stackoverflow.com/questions/2967194/open-in-python-does-not-create-a-file-if-it-doesnt-exist\n",
    "                    if not os.path.exists(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\"))):\n",
    "                        file = open(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\")), 'w+')\n",
    "                        file.write(document)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "kddcorpus= nltk.corpus.PlaintextCorpusReader(corpuspath, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kddcorpus= nltk.corpus.PlaintextCorpusReader(corpuspath, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>This is a pretty big step.  We have a semi-structured data set in a format where we can query and analyze different pieces of data.  All of our data is loaded as an NLTK corpus, meaning we could try tons of techniques outlined in the [NLTK book](http://www.nltk.org/book/) or use the NLTK APIs to pass data into [scikit-learn machine learning pipelines for text](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) (maybe for a later blog). Let's see how many words (including stop words) we have in our entire corpus.  <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2795267\n"
     ]
    }
   ],
   "source": [
    "wordcount = 0\n",
    "for fileid in kddcorpus.fileids():\n",
    "    wordcount += len(kddcorpus.words(fileid))\n",
    "print wordcount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>To begin exploration of regular expressions, let's extract 'good enough' titles from a few of the documents.  For help on regex, visit https://regex101.com/. Here are the titles for the first 26 papers. <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online Controlled Experiments:\n",
      "Mining Frequent Itemsets through Progressive Sampling\n",
      "Why It Happened: Identifying and Modeling the Reasons of\n",
      "Matrix Completion with Queries\n",
      "Stochastic Divergence Minimization\n",
      "Bayesian Poisson Tensor Factorization for Inferring\n",
      "TimeCrunch: Interpretable Dynamic Graph Summarization\n",
      "Inside Jokes: Identifying Humorous Cartoon Captions\n",
      "Community Detection based on Distance Dynamics\n",
      "Discovery of Meaningful Rules in Time Series\n",
      "On the Formation of Circles in Co-authorship Networks\n",
      "An Evaluation of Parallel Eccentricity Estimation\n",
      "Efcient Latent Link Recommendation in\n",
      "Turn Waste into Wealth: On Simultaneous Clustering and\n",
      "Set Cover at Web Scale\n",
      "Exploiting Relevance Feedback in Knowledge Graph\n",
      "LINKAGE: An Approach for Comprehensive Risk\n",
      "Transitive Transfer Learning\n",
      "PTE: Predictive Text Embedding through Large-scale\n",
      "An Effective Marketing Strategy for Revenue Maximization\n",
      "Scaling Up Stochastic Dual Coordinate Ascent\n",
      "Heterogeneous Network Embedding via Deep\n",
      "Discovering Valuable Items from Massive Data\n",
      "Deep Learning Architecture with Dynamically Programmed\n",
      "Incorporating World Knowledge to Document Clustering\n"
     ]
    }
   ],
   "source": [
    "# code uses regular expression to extract text up to the first new line character\n",
    "\n",
    "p=re.compile('(.+)(\\\\n)')\n",
    "for fileid in kddcorpus.fileids()[:25]:\n",
    "    print p.search(kddcorpus.raw(fileid)).group(1).strip()  # use .strip() to remove whitespace from beginning and end of string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling and computation: Using Regular Expressions to extract specific sections of the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are close to the NERC portion.  But, there's a bit more wrangling to do (remember, PDFs are tough work).  For simplicity, let's focus the NERC on two sections of the paper:\n",
    "* the top section which includes authors and schools\n",
    "* the references section of the paper (keynote speaker abstracts do not have an abstract)\n",
    "\n",
    "The tools of choice to extract sections are the [\"positive lookbehind\" and \"positive lookahead\"](https://docs.python.org/2/library/re.html) expressions. Here is an example of code to extract the abstract only:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The collapsed variational Bayes zero (CVB0) inference is a vari- ational inference improved by marginalizing out parameters, the same as with the collapsed Gibbs sampler. A drawback of the CVB0 inference is the memory requirements. A probability vec- tor must be maintained for latent topics for every token in a corpus. When the total number of tokens is N and the number of topics is K, the CVB0 inference requires O(N K) memory. A stochas- tic approximation of the CVB0 (SCVB0) inference can reduce O(N K) to O(V K), where V denotes the vocabulary size. We re- formulate the existing SCVB0 inference by using the stochastic di- vergence minimization algorithm, with which convergence can be analyzed in terms of Martingale convergence theory. We also reveal the property of the CVB0 inference in terms of the leave-one-out perplexity, which leads to the estimation algorithm of the Dirichlet distribution parameters. The predictive performance of the propose SCVB0 inference is better than that of the original SCVB0 infer- ence in four datasets.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set our regular expression\n",
    "p= re.compile('(?<=ABSTRACT)(.+)(?=Categories and Subject Descriptors)')\n",
    "try:\n",
    "    abstract= p.search(re.sub('[\\s]',\" \",kddcorpus.raw('p1035.txt'))).group(1)\n",
    "except AttributeError:\n",
    "    # include a lowercase regex match incase consistency is a problem\n",
    "    p=re.compile('(?<=abstract)(.+)(?=categories and subject descriptors)')\n",
    "    abstract=p.search(re.sub('[\\s]',\" \",holder.lower())).group(1)\n",
    "else:\n",
    "    pass\n",
    "unicodedata.normalize('NFKD', abstract).encode('ascii','ignore').strip() # convert output from unicode to string and strip leading and trailing whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!  Now, to be \"pythonic\" we build two functions that can extract the top and references section of the documents.  For fun, I also made other function to extract the keywords and abstract sections of the documents.  We could do the same for any section of paper although I must provide a warning.  **Working with natural language is a messy ordeal!**  This is a top notch organization (ACM) and a top notch conference (KDD) but human error sitll makes it way into the picture:\n",
    "\n",
    "![Human Error](http://www.process-improvement-institute.com/wp-content/uploads/2015/05/Accounting-for-Human-Error-Probability-in-SIL-Verification.jpg)\n",
    "\n",
    "Specifically in our case:\n",
    "* paper 1 header section = \"Categories and Subject Descriptors\"\n",
    "* paper 2 header section = \"Categories & Subject Descriptors\"\n",
    "\n",
    "Very small difference but these types of differences cause TONS of headaches.  The result?  You have a decision to make: **account for these differences or ignore them**.  I worked to include AS MUCH of the 253 corpus as possible in the results but it's never perfect.  There are also some documents that will be missing sections altogether (i.e. keynote speaker documents do not have a references section.  Our two functions will:\n",
    "\n",
    "1. Extract only the relevant text for the section we seek\n",
    "2. Extract a character count for the section\n",
    "3. Make additonal calculations or extractions\n",
    "  * the top section extraction also extract emails\n",
    "  * we count the number of references and store that value\n",
    "  * as added benefit, we create a simple \"word per reference\" calculation\n",
    "4. Store all the above data as a nested dictionary with the filename as a key\n",
    "\n",
    "These are loooooong blocks of code to accomplish the task above.  For now, we will only show the code to extract the references and perform the quick analysis mentioned above.  The other functions will be in the appendix.  In fairness, all functions could be reduced down to one function composed of nested function calls.  We will save that for later and get the \"functionality\" working before optimizing the code. See the comments below to follow along or just skip to the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code to pull the ferences section only, store a character count, number of references, and \"word per reference\" calculation\n",
    "\n",
    "def refpull(docnum=None,section='references',full = False):\n",
    "    \n",
    "    # Establish an empty dictionary to hold values\n",
    "    ans={}\n",
    "    \n",
    "    # Establish an empty list to hold document ids that don't make the cut (i.e. missing reference section or different format)\n",
    "    # This comes in handy when you are trying to improve your code to catch outliers\n",
    "    failids = []\n",
    "    \n",
    "    # Eliminate any variations you can; covert everything to lowercase.\n",
    "    # This can cause problems because a titlecase \"Abstract\" could be a section header but \"abstract\" can be an adjective in a sentence\n",
    "    section = section.lower()\n",
    "    \n",
    "    # Admin code to set default values and raise an exception if there's human error on input\n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        # We set our text file here; this is what our regular expression will work on\n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        \n",
    "        # This first condtional is for pulling the target section for ALL documents in the corpus\n",
    "        if full == True:\n",
    "            \n",
    "            # Iterate over the corpus to get the id\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                \n",
    "                # This for loop passes the fileid from above into the nltk format to retrieve the raw text\n",
    "                # again, we eliminate any variations by making this lowercase, and append .lower() to the end\n",
    "                text = kddcorpus.raw(fileid).lower()\n",
    "                \n",
    "                # The default section for this function is references, but we set it again to be sure\n",
    "                if section == \"references\":\n",
    "                    \n",
    "                    # These lines of code build our regular expression.\n",
    "                    # In the other functions for abstract or keywords, you see how I use this technique to create different regex arugments\n",
    "                    section1=\"references \\[\" \n",
    "                    target = \"\"   \n",
    "                    part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "                    # We use our regex on the target file; remember, we're still in the loop so it applies to every file\n",
    "                    for sect in section1:\n",
    "                        \n",
    "                        # Using try/except to avoid pesky errors that stop our process; failed processing will append ids \\nto the \"failedids\" list\n",
    "                        try:\n",
    "                            # our regex\n",
    "                            p=re.compile(part1)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "\n",
    "                            # conditional to make sure we pull in references that have more than 50 characters; we don't want empty references\n",
    "                            if len(target) > 50:\n",
    "\n",
    "                                # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                                try:\n",
    "                                    refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                                except:\n",
    "                                    print \"This file does not appear to have a references section\"\n",
    "                                    pass\n",
    "                                \n",
    "                                # we pass the values into a nested dictionary for retrieval\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"references\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                ans[str(fileid)][\"refcount\"]= refnum\n",
    "                                ans[str(fileid)][\"wordperRef\"]=float(len(nltk.word_tokenize(text)))/float(refnum)\n",
    "                                #print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                                break\n",
    "                            else:\n",
    "\n",
    "                                pass\n",
    "                        \n",
    "                        # Anything that failed, gets appended to a failed list for later testing\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "        \n",
    "            return ans\n",
    "            return failids\n",
    "                              \n",
    "        # The code below is to extract the target section from one document; same functionality as above\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum).lower()\n",
    "        \n",
    "        if section == \"top\":\n",
    "            section1=\"\"\n",
    "            section2=[\"abstract\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if len(target)> 1000:\n",
    "                        if len(target) > 3000 and float(len(target))/float(len(text)) > .22:\n",
    "                            target = target[:2500]\n",
    "                        else:\n",
    "                            target=target\n",
    "                        ans[str(docnum)]={}\n",
    "                        ans[str(docnum)][\"references\"]=target.strip()\n",
    "                        ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                        ans[str(docnum)][\"refcount\"]= refnum\n",
    "                        ans[str(docnum)][\"wordperRef\"]=float(len(nltk.word_tokenize(text)))/float(refnum)\n",
    "\n",
    "                        #print [fileid,len(target),len(text)]\n",
    "                        break\n",
    "                     \n",
    "                    else:\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "                \n",
    "        return ans\n",
    "        return failids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a big block of code!  Don't fret, there are several similar blocks in the appendix to extract the abstract and keywords.  Data is messy; this is what cleaning looks like.  In the code above, we also make use of the *nltk.word_tokenize* tool to create the \"word per reference\" figure.  Let's test our function and some output (the word_tokenize calculation will take some time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# call our function, setting \"full=True\" extracts ALL references in corpus\n",
    "test = refpull(full=True)\n",
    "\n",
    "# To get a quick glimpse, I use the example from this page: http://stackoverflow.com/questions/7971618/python-return-first-n-keyvalue-pairs-from-dict\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "man = collections.OrderedDict(test)\n",
    "\n",
    "x = itertools.islice(man.items(), 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename      Character Count    Number of references    Words per Reference\n",
      "----------  -----------------  ----------------------  ---------------------\n",
      "p835.txt                 5345                      37                326.189\n",
      "p865.txt                 5267                      26                412.962\n",
      "p2089.txt                8732                      44                184.455\n",
      "p815.txt                 7775                      60                195.967\n",
      "p2099.txt                3947                      27                386.889\n",
      "p785.txt                 4279                      36                323.667\n",
      "p725.txt                 5769                      36                311.111\n",
      "p597.txt                 6417                      35                306.143\n",
      "p1789.txt                6742                      35                297.486\n",
      "p577.txt                 6694                      42                203.905\n"
     ]
    }
   ],
   "source": [
    "# Let's use a nifty table module to print this all pretty like: https://pypi.python.org/pypi/tabulate\n",
    "# The joy of Python and open source: someone has created something to do what you want; Google is your friend.  \n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "# A quick list comprehension to follow the example on the tabulate pypi page\n",
    "table = [[key,value['charcount'],value['refcount'], value['wordperRef']] for key,value in x]\n",
    "\n",
    "# print the pretty table; we invoke the \"header\" argument and assign custom header!!!!\n",
    "print tabulate(table,headers=[\"filename\",\"Character Count\", \"Number of references\",\"Words per Reference\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data computation and analyses: Using NERC tools and examining for accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are in the spot where data scientists WANT to live: computation and analyses!!!  In truth, most spend their time ingesting, wrangling, and munging data, as you see above. \n",
    "\n",
    "We are ready to test how well some open source NERC tools extract names, places, and organizations from the top and reference sections of our corpus.  As an added benefit (using the web scraping code from above), we can do a comparison to see how well our pdf-ingest-scrape-regex-NERC pipeline works compared to old-fashioned web scraping.  \n",
    "\n",
    "We start with a few hand labeled documents.  Hand labeling is an expensive and tedious process; the entities for two documents I labeled (yea..it's only 2 but that was 295 cut-and-pastes not counting writing the list names):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 authors\n",
      "\n",
      "There are 3 author organizations\n",
      "\n",
      "There are 7 author locations\n",
      "\n",
      "There are 152 authors in the references\n"
     ]
    }
   ],
   "source": [
    "# filename p19.txt\n",
    "\n",
    "p19pdf_authors=['Tim Althoff*','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang']\n",
    "p19pdf_author_organizations=['Computer Science Department','Stanford University','Google']\n",
    "p19pdf_author_locations=['Stanford, CA','Stanford','CA','Google','1600 Amphitheatre Parkway, Mountain View, CA 94043','1600 Amphitheatre Parkway','Mountain View']\n",
    "\n",
    "p19pdf_references_authors =['A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']\n",
    "\n",
    "p19pdf_allauthors=['Tim Althoff*','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang','A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']\n",
    "\n",
    "print \"There are %r authors\" % len(p19pdf_authors)\n",
    "print  # white space\n",
    "print \"There are %r author organizations\" %len(p19pdf_author_organizations)\n",
    "print \n",
    "print \"There are %r author locations\" % len(p19pdf_author_locations)\n",
    "print  \n",
    "print \"There are %r authors in the references\" %len(p19pdf_references_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 authors\n",
      "\n",
      "There are 6 author organizations\n",
      "\n",
      "There are 8 author locations\n",
      "\n",
      "There are 106 authors in the references\n"
     ]
    }
   ],
   "source": [
    "# filename p29.txt\n",
    "\n",
    "p29pdf_authors=['Laurent Amsaleg','Stéphane Girard','Oussama Chelly','Teddy Furon','Michael E. Houle','Ken-ichi Kawarabayashi',\n",
    "               'Michael Nett']\n",
    "p29pdf_author_organizations=['Equipe LINKMEDIA','Campus Universitaire de Beaulieu','CNRS/IRISA Rennes','National Institute of Informatics',\n",
    "                             'Equipe MISTIS INRIA','Google']\n",
    "p29pdf_author_locations=['Campus Universitaire de Beaulieu','35042 Rennes Cedex, France','France','-1-2 Hitotsubashi, Chiyoda-ku Tokyo 101-8430, Japan',\n",
    "                        'Japan','6-10-1 Roppongi, Minato-ku Tokyo 106-6126','Inovallée, 655, Montbonnot 38334 Saint-Ismier Cedex','Tokyo']\n",
    "\n",
    "p29pdf_references_authors =['A. A. Balkema','L. de Haan','N. Bingham', 'C. Goldie','J. Teugels','N. Boujemaa', 'J. Fauqueur', 'M. Ferecatu', 'F. Fleuret',\n",
    "                            'V. Gouet', 'B. LeSaux','H. Sahbi','C. Bouveyron', 'G. Celeux', 'S. Girard','J. Bruske', 'G. Sommer',\n",
    "                           'F. Camastra','A. Vinciarelli','S. Coles','J. Costa' ,'A. Hero','T. de Vries', 'S. Chawla','M. E. Houle',\n",
    "                           'R. A. Fisher','L. H. C. Tippett','M. I. Fraga Alves', 'L. de Haan','T. Lin','M. I. Fraga Alves', 'M. I. Gomes','L. de Haan',\n",
    "                           'B. V. Gnedenko',' A. Gupta', 'R. Krauthgamer','J. R. Lee','A. Gupta', 'R. Krauthgamer','J. R. Lee','M. Hein','J.-Y. Audibert',\n",
    "                           'B. M. Hill','M. E. Houle','M. E. Houle','M. E. Houle','M. E. Houle', 'H. Kashima', 'M. Nett','M. E. Houle', 'X. Ma', 'M. Nett',\n",
    "                            'V. Oria','M. E. Houle', 'X. Ma', 'V. Oria','J. Sun','M. E. Houle','M. Nett','H. Jegou', 'R. Tavenard', 'M. Douze','L. Amsaleg',\n",
    "                           'I. Jollie','D. R. Karger','M. Ruhl','J. Karhunen','J. Joutsensalo','Y. LeCun', 'L. Bottou', 'Y. Bengio', 'P. Haner',\n",
    "                           'J. Pickands, III','C. R. Rao','S. T. Roweis','L. K. Saul','A. Rozza', 'G. Lombardi', 'C. Ceruti', 'E. Casiraghi', 'P. Campadelli',\n",
    "                           'B. Scholkopf', 'A. J. Smola','K.-R. Muller','U. Shaft','R. Ramakrishnan',' F. Takens','J. Tenenbaum', 'V. D. Silva','J. Langford',\n",
    "                           'J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. Venna','S. Kaski',\n",
    "                           'P. Verveer','R. Duin','J. von Brunken', 'M. E. Houle', 'A. Zimek','J. von Brunken', 'M. E. Houle','A. Zimek']\n",
    "\n",
    "p29pdf_allauthors=['Laurent Amsaleg','Stéphane Girard','Oussama Chelly','Teddy Furon','Michael E. Houle','Ken-ichi Kawarabayashi',\n",
    "               'Michael Nett','A. A. Balkema','L. de Haan','N. Bingham', 'C. Goldie','J. Teugels','N. Boujemaa', 'J. Fauqueur', 'M. Ferecatu', 'F. Fleuret',\n",
    "                            'V. Gouet', 'B. LeSaux','H. Sahbi','C. Bouveyron', 'G. Celeux', 'S. Girard','J. Bruske', 'G. Sommer',\n",
    "                           'F. Camastra','A. Vinciarelli','S. Coles','J. Costa' ,'A. Hero','T. de Vries', 'S. Chawla','M. E. Houle',\n",
    "                           'R. A. Fisher','L. H. C. Tippett','M. I. Fraga Alves', 'L. de Haan','T. Lin','M. I. Fraga Alves', 'M. I. Gomes','L. de Haan',\n",
    "                           'B. V. Gnedenko',' A. Gupta', 'R. Krauthgamer','J. R. Lee','A. Gupta', 'R. Krauthgamer','J. R. Lee','M. Hein','J.-Y. Audibert',\n",
    "                           'B. M. Hill','M. E. Houle','M. E. Houle','M. E. Houle','M. E. Houle', 'H. Kashima', 'M. Nett','M. E. Houle', 'X. Ma', 'M. Nett',\n",
    "                            'V. Oria','M. E. Houle', 'X. Ma', 'V. Oria','J. Sun','M. E. Houle','M. Nett','H. Jegou', 'R. Tavenard', 'M. Douze','L. Amsaleg',\n",
    "                           'I. Jollie','D. R. Karger','M. Ruhl','J. Karhunen','J. Joutsensalo','Y. LeCun', 'L. Bottou', 'Y. Bengio', 'P. Haner',\n",
    "                           'J. Pickands, III','C. R. Rao','S. T. Roweis','L. K. Saul','A. Rozza', 'G. Lombardi', 'C. Ceruti', 'E. Casiraghi', 'P. Campadelli',\n",
    "                           'B. Scholkopf', 'A. J. Smola','K.-R. Muller','U. Shaft','R. Ramakrishnan',' F. Takens','J. Tenenbaum', 'V. D. Silva','J. Langford',\n",
    "                           'J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. Venna','S. Kaski',\n",
    "                           'P. Verveer','R. Duin','J. von Brunken', 'M. E. Houle', 'A. Zimek','J. von Brunken', 'M. E. Houle','A. Zimek']\n",
    "\n",
    "\n",
    "print \"There are %r authors\" % len(p29pdf_authors)\n",
    "print  # white space\n",
    "print \"There are %r author organizations\" %len(p29pdf_author_organizations)\n",
    "print \n",
    "print \"There are %r author locations\" % len(p29pdf_author_locations)\n",
    "print  \n",
    "print \"There are %r authors in the references\" %len(p29pdf_references_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we can programmatically access just about all of the corpus, we are free to hand label as much as we want to do the test.  Our measureable test:\n",
    "\n",
    "* Compare machice extracted list of persons, places, and organizations to hand labeled lists\n",
    "* Compute precision, accuracy and recall\n",
    "* Compare different NERC tool scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the performance of three open source NERC tools.  All of them can be trained to improve performance, but for now we will test \"out of the box\" performance:\n",
    "\n",
    "1.  [NLTK's standard chunker](http://www.nltk.org/api/nltk.chunk.html); read more in [the NLTK book](http://www.nltk.org/book/ch07.html)\n",
    "2. [Standard's Named Entity Recognizer](http://nlp.stanford.edu/software/CRF-NER.shtml), which can be accessed as an API via the NLTK tool\n",
    "3. [Polyglot NER](http://polyglot.readthedocs.org/en/latest/index.html) which is natural language pipeline that supports massive multilingual applications.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin to chunk our data using the benefits of having our texts loaded into NLTK.  We first get the data for our test documents.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need the top and references sections from p19.txt and p29.txt\n",
    "\n",
    "p19top = toppull(\"p19.txt\")\n",
    "p19ref = refpull(\"p19.txt\")\n",
    "\n",
    "p29top=toppull(\"p29.txt\")\n",
    "p29ref=refpull(\"p29.txt\")\n",
    "\n",
    "p19={}\n",
    "p19['top']=p19top['p19.txt']['top']\n",
    "p19['references']=p19ref['p19.txt']['references']\n",
    "\n",
    "\n",
    "p29={}\n",
    "p29['top']=p29top['p29.txt']['top']\n",
    "p29['references']=p29ref['p29.txt']['references']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was easy!  All the munging and wrangling pays off down the road.  Now, we test our first NERC tool, the standard chunker in NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sup id=\"fn1\">1. [(2014). Text Mining and its Business Applications - CodeProject. Retrieved December 26, 2015, from http://www.codeproject.com/Articles/822379/Text-Mining-and-its-Business-Applications.]<a href=\"#ref1\" title=\"Jump back to footnote 1 in the text.\">↩</a></sup>\n",
    "\n",
    "<sup id=\"fn2\">2. [Suchanek, F., & Weikum, G. (2013). Knowledge harvesting in the big-data era. Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data. ACM.]<a href=\"#ref2\" title=\"Jump back to footnote 2 in the text.\">↩</a></sup>\n",
    "\n",
    "\n",
    "<sup id =\"fn3\">3. [Nadeau, D., & Sekine, S. (2007). A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1), 3-26.]<a href=\"#ref3\" title = \"Jump back to footnote 3 in the text\">↩</a></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Parking Lot of links, leftover paragraphs, ideas, etc.</span>\n",
    "\n",
    "Describe the data -> Data available here http://dl.acm.org/citation.cfm?id=2783258# \n",
    "\n",
    "Computer Vision - ECCV 2008 pdf download online free. Retrieved December 31, 2015, from http://pdf12.mono-ebook.org/pdf/computer-vision-eccv-2008_12glgt.pdf.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<href id=\"pipe\"><a href=\"#pipeline\" title=\"Jump back to data science pipeline graphic.\">data science pipeline</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ben's Outline from email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ~~Give a brief introduction to the task, and why it's interesting, important. Then begin to discuss the data set, how you acquired, and where a reader can get access to it.~~ \n",
    "\n",
    "* ~~You then could have a data exploration section where you show the number of documents, perform a word count, show snippets of data (e.g. references) etc that are of interest.~~\n",
    "\n",
    "* ~~You can then go through one or a few of your \"code to get\" sections. These functions all follow basically the same pattern, so you could probably merge them into a single function, that appropriately selects the right regular expression.~~ \n",
    "\n",
    "* ~~The next step is to discuss, demonstrate your \"truth tests\" for text extraction accuracy.~~ \n",
    "\n",
    "* Finally, you can get to an introduction of your three methods for NERC, and show how do do each of them. Then compare (visually) the results of the three according to the evaluation mechanism discussed above. \n",
    "\n",
    "* You could then conclude with a discussion about NLTK chunk vs. hand labelled entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kddcorpus_bigrams=[]\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "for fileid in kddcorpus.fileids():\n",
    "    for l in (BigramCollocationFinder.from_words(kddcorpus.words(fileid)).nbest(bigram_measures.pmi, 10)):\n",
    "        kddcorpus_bigrams.append(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Prototype Holder</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test on normal case extraction\n",
    "failids = []\n",
    "text=kddcorpus.raw('p1055.txt')\n",
    "\n",
    "full = True\n",
    "section = \"abstract\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid)\n",
    "        if section == \"abstract\":\n",
    "            section1=\"abstract\"\n",
    "            target = \"\"   \n",
    "            section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if len(target) > 50:\n",
    "                        \n",
    "                        \n",
    "                        print [fileid,len(target),len(text)]\n",
    "                        break\n",
    "                    else:\n",
    "                        failids.append(fileid)\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    \n",
    "                    pass\n",
    "                              \n",
    "else:\n",
    "    \n",
    "    section = \"abstract\"\n",
    "    text = kddcorpus.raw('p1627.txt').lower()\n",
    "    if section == \"abstract\":\n",
    "        section1=\"abstract\"\n",
    "        target = \"\"   \n",
    "        section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "\n",
    "        part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "        for sect in section2:\n",
    "            try:\n",
    "                part2 = \"(?=\"+str(sect)+\")\"\n",
    "                p=re.compile(part1+part2)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                if target > 50:\n",
    "\n",
    "                    \n",
    "                    break\n",
    "            except:\n",
    "                \n",
    "                pass\n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# completed gold standard for keywords, got all of them..no stragglers\n",
    "text = kddcorpus.raw('p39.txt').lower()\n",
    "failids = []\n",
    "full = True\n",
    "section = \"keywords\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid).lower()\n",
    "        if section == \"keywords\":\n",
    "            section1=\"keywords\"\n",
    "            target = \"\"   \n",
    "            section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"permission to make \",\"1.motivation\" ]\n",
    "        \n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    if len(target) >50:\n",
    "                        if len(target) > 300:\n",
    "                            target = target[:200]\n",
    "                        else:\n",
    "                            target = target\n",
    "                        \n",
    "                        print [fileid,target,len(text)]\n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        failids.append(fileid)\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "else:\n",
    "    section = \"keywords\"\n",
    "    \n",
    "    if section == \"keywords\":\n",
    "        section1=\"keywords\"\n",
    "        target = \"\"   \n",
    "        section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"permission to make \",\"1.motivation\" ]\n",
    "\n",
    "        part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "        for sect in section2:\n",
    "            try:\n",
    "                part2 = \"(?=\"+str(sect)+\")\"\n",
    "                p=re.compile(part1+part2)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                if target > 3:                 \n",
    "                    break                  \n",
    "            except:\n",
    "                pass\n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# completed gold standard for abstract, near zero stragglers\n",
    "failids = []\n",
    "text=kddcorpus.raw('p1055.txt')\n",
    "\n",
    "full = True\n",
    "section = \"abstract\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid).lower()\n",
    "        if section == \"abstract\":\n",
    "            section1=\"abstract\"\n",
    "            target = \"\"   \n",
    "            section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if len(target) > 50:\n",
    "                        \n",
    "                        \n",
    "                        print [fileid,len(target),len(text)]\n",
    "                        break\n",
    "                    else:\n",
    "                        failids.append(fileid)\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    \n",
    "                    pass\n",
    "                              \n",
    "else:\n",
    "    \n",
    "    section = \"abstract\"\n",
    "    text = kddcorpus.raw('p1627.txt').lower()\n",
    "    if section == \"abstract\":\n",
    "        section1=\"abstract\"\n",
    "        target = \"\"   \n",
    "        section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "\n",
    "        part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "        for sect in section2:\n",
    "            try:\n",
    "                part2 = \"(?=\"+str(sect)+\")\"\n",
    "                p=re.compile(part1+part2)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                if target > 50:\n",
    "\n",
    "                    \n",
    "                    break\n",
    "            except:\n",
    "                \n",
    "                pass\n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# completed gold standard for references, counts number of references and does \"word per reference\" score\n",
    "\n",
    "failids = []\n",
    "text=kddcorpus.raw('p29.txt').lower()\n",
    "\n",
    "full = False\n",
    "section = \"references\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid).lower()\n",
    "        if section == \"references\":\n",
    "            section1=\"references \\[\" \n",
    "            target = \"\"   \n",
    "            \n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "            \n",
    "            for sect in section1:\n",
    "                try:\n",
    "                    p=re.compile(part1)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "\n",
    "                    if len(target) > 50:\n",
    "                        \n",
    "                        # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                        try:\n",
    "                            if 'references' in locals():\n",
    "\n",
    "                                refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                        except:\n",
    "                            print \"This file does not appear to have a references section\"\n",
    "                            pass\n",
    "                        print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                        break\n",
    "                    else:\n",
    "                        \n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "                              \n",
    "else:\n",
    "    \n",
    "    section = \"references\"\n",
    "    \n",
    "    if section == \"references\":\n",
    "        section1=\"references \\[\"\n",
    "        target = \"\"   \n",
    "        \n",
    "        part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "        for sect in section1:\n",
    "            try:\n",
    "                \n",
    "                p=re.compile(part1)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                if target > 50:\n",
    "                    print len(target)\n",
    "\n",
    "                    \n",
    "                break\n",
    "            except:\n",
    "                \n",
    "                pass\n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(failids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gold standard to get top section\n",
    "\n",
    "from emailextractor import file_to_str, get_emails # paste code to .py file from following link and save within your environment path to call it: https://gist.github.com/dideler/5219706\n",
    "\n",
    "failids = []\n",
    "text=kddcorpus.raw('p1623.txt').lower()\n",
    "\n",
    "full = True\n",
    "section = \"top\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid).lower()\n",
    "        if section == \"top\":\n",
    "            section1=\"\"\n",
    "            section2=[\"abstract\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if len(target)> 1000:\n",
    "                        if len(target) > 3000 and float(len(target))/float(len(text)) > .22:\n",
    "                            target = target[:2500]\n",
    "                        else:\n",
    "                            target=target\n",
    "                        emails = tuple(get_emails(target))\n",
    "                        print [fileid,len(target),len(text),emails]\n",
    "                        break\n",
    "                    else:\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "    print \"Done\"\n",
    "                              \n",
    "else:\n",
    "    \n",
    "    if section == \"top\":\n",
    "            section1=\"\"\n",
    "            section2=[\"abstract\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if len(target)> 1000:\n",
    "                        if len(target) > 3000 and float(len(target))/float(len(text)) > .22:\n",
    "                            target = target[:2500]\n",
    "                        else:\n",
    "                            target=target\n",
    "                        emails = tuple(get_emails(target))\n",
    "                        print [fileid,len(target),len(text),emails]\n",
    "                        break\n",
    "                     \n",
    "                    else:\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "failids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:violet\">Drawing Board/Assembly Line</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attempting function with gold keywords....WORKS\n",
    "\n",
    "def keypull(docnum=None,section='keywords',full = False):\n",
    "    \n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "\n",
    "        \n",
    "\n",
    "        # to return output from entire corpus\n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid).lower()\n",
    "                if section == \"keywords\":\n",
    "                    section1=\"keywords\"\n",
    "                    target = \"\"   \n",
    "                    section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"(1. tutorial )\",\" permission to make \",\"  permission to make\",\"(  permission to make digital )\",\"    bio  \",\"abstract:  \",\"1.motivation\" ]\n",
    "\n",
    "                    part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "                    for sect in section2:\n",
    "                        try:\n",
    "                            part2 = \"(?=\"+str(sect)+\")\"\n",
    "                            p=re.compile(part1+part2)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                            if len(target) >50:\n",
    "                                if len(target) > 300:\n",
    "                                    target = target[:200]\n",
    "                                else:\n",
    "                                    target = target\n",
    "\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"keywords\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                #print [fileid,len(target),len(text)]\n",
    "                                break\n",
    "                            else:\n",
    "                                if len(target)==0:\n",
    "                                     failids.append(fileid)   \n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "            set(failids)\n",
    "            return ans\n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        if full == False:\n",
    "            if section == \"keywords\":\n",
    "                section1=\"keywords\"\n",
    "                target = \"\"   \n",
    "                section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"permission to make \",\"1.motivation\" ]\n",
    "\n",
    "                part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "                for sect in section2:\n",
    "                    try:\n",
    "                        part2 = \"(?=\"+str(sect)+\")\"\n",
    "                        p=re.compile(part1+part2)\n",
    "                        target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                        if len(target) >50:\n",
    "                            if len(target) > 300:\n",
    "                                target = target[:200]\n",
    "                            else:\n",
    "                                target = target\n",
    "                            ans[docnum]={}\n",
    "                            ans[docnum][\"keywords\"]=target.strip()\n",
    "                            ans[docnum][\"charcount\"]=len(target)\n",
    "                            break                  \n",
    "                    except:\n",
    "                        pass\n",
    "    return ans\n",
    "    return failids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attempting function with gold abstracts...WORKS\n",
    "\n",
    "def abpull(docnum=None,section='abstract',full = False):\n",
    "    \n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        # to return output from entire corpus\n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid).lower()\n",
    "                if section == \"abstract\":\n",
    "                    section1=\"abstract\"\n",
    "                    target = \"\"   \n",
    "                    section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "                    part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "                    for sect in section2:\n",
    "                        try:\n",
    "                            part2 = \"(?=\"+str(sect)+\")\"\n",
    "                            p=re.compile(part1+part2)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "\n",
    "                            if len(target) > 50:\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"abstract\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                \n",
    "                                #print [fileid,len(target),len(text)]\n",
    "                                break\n",
    "                            else:\n",
    "                                failids.append(fileid)\n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            pass\n",
    "            return ans\n",
    "                              \n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum).lower()\n",
    "        if section == \"abstract\":\n",
    "            section1=\"abstract\"\n",
    "            target = \"\"   \n",
    "            section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if target > 50:\n",
    "                        ans[str(docnum)]={}\n",
    "                        ans[str(docnum)][\"abstract\"]=target.strip()\n",
    "                        ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "        return ans\n",
    "        return failids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attempting function with gold top section...WORKS\n",
    "\n",
    "from emailextractor import file_to_str, get_emails # paste code to .py file from following link and save within your environment path to call it: https://gist.github.com/dideler/5219706\n",
    "def toppull(docnum=None,section='top',full = False):\n",
    "    \n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        # to return output from entire corpus\n",
    "        \n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid).lower()\n",
    "                if section == \"top\":\n",
    "                    section1=\"\"\n",
    "                    section2=[\"abstract\"]\n",
    "                    part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "                    for sect in section2:\n",
    "                        try:\n",
    "                            part2 = \"(?=\"+str(sect)+\")\"\n",
    "                            p=re.compile(part1+part2)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "\n",
    "                            if len(target)> 1000:\n",
    "                                if len(target) > 3000 and float(len(target))/float(len(text)) > .22:\n",
    "                                    target = target[:2500]\n",
    "                                else:\n",
    "                                    target=target\n",
    "                                emails = tuple(get_emails(target))\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"top\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                ans[str(fileid)][\"emails\"]=emails\n",
    "                                \n",
    "                                #print [fileid,len(target),len(text)]\n",
    "                                break\n",
    "                            else:\n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "            return ans\n",
    "            return failids\n",
    "        \n",
    "        \n",
    "        \n",
    "                              \n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum).lower()\n",
    "        \n",
    "        if section == \"top\":\n",
    "            section1=\"\"\n",
    "            section2=[\"abstract\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if len(target)> 1000:\n",
    "                        if len(target) > 3000 and float(len(target))/float(len(text)) > .22:\n",
    "                            target = target[:2500]\n",
    "                        else:\n",
    "                            target=target\n",
    "                        emails = tuple(get_emails(target))\n",
    "                        ans[str(docnum)]={}\n",
    "                        ans[str(docnum)][\"top\"]=target.strip()\n",
    "                        ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                        ans[str(docnum)][\"emails\"]=emails\n",
    "                        #print [fileid,len(target),len(text)]\n",
    "                        break\n",
    "                     \n",
    "                    else:\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "                \n",
    "        return ans\n",
    "        return failids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attempting function with gold references section\n",
    "\n",
    "def refpull(docnum=None,section='references',full = False):\n",
    "    \n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        # to return output from entire corpus\n",
    "        \n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid).lower()\n",
    "                if section == \"references\":\n",
    "                    section1=[\"references \\[\",\"references\"] \n",
    "                    target = \"\"   \n",
    "\n",
    "                    part1= \"(?<=\"+str(section1)+\")\"\n",
    "\n",
    "                    for sect in section1:\n",
    "                        try:\n",
    "                            part1= \"(?<=\"+str(sect)+\")\"\n",
    "                            p=re.compile(part1)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "\n",
    "                            if len(target) > 50:\n",
    "\n",
    "                                # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                                try:\n",
    "                                    refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                                except:\n",
    "                                    print \"This file does not appear to have a references section\"\n",
    "                                    pass\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"references\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                ans[str(fileid)][\"refcount\"]= refnum\n",
    "                                ans[str(fileid)][\"wordperRef\"]=round(float(len(nltk.word_tokenize(text)))/float(refnum))\n",
    "                                \n",
    "                                #print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                                break\n",
    "                            else:\n",
    "\n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "        \n",
    "            return ans\n",
    "            return failids\n",
    "                              \n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum).lower()\n",
    "        if section == \"references\":\n",
    "                    section1=\"references \\[\" \n",
    "                    target = \"\"   \n",
    "\n",
    "                    \n",
    "\n",
    "                    for sect in section1:\n",
    "                        try:\n",
    "                            part1= \"(?<=\"+str(sect)+\")(.+)\"\n",
    "                            p=re.compile(part1)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "\n",
    "                            if len(target) > 50:\n",
    "\n",
    "                                # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                                try:\n",
    "                                    refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                                except:\n",
    "                                    print \"This file does not appear to have a references section\"\n",
    "                                    pass\n",
    "                                ans[str(docnum)]={}\n",
    "                                ans[str(docnum)][\"references\"]=target.strip()\n",
    "                                ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                                ans[str(docnum)][\"refcount\"]= refnum\n",
    "                                ans[str(docnum)][\"wordperRef\"]=float(len(nltk.word_tokenize(text)))/float(refnum)\n",
    "                                \n",
    "                                #print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                                break\n",
    "                            else:\n",
    "\n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(docnum)\n",
    "                            pass\n",
    "        \n",
    "        return ans\n",
    "        return failids\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Testing Station</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = toppull(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test= refpull('p29.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(test.keys())\n",
    "print len(set(failids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print set(failids)\n",
    "print\n",
    "print\n",
    "print set(failids) & set(keywords.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?<=keywords)(.+)(?=introduction  )\n"
     ]
    }
   ],
   "source": [
    "print part1+part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key,value in test.iteritems():\n",
    "    print key,value['emails']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "No such file or directory: '/Users/linwood/Desktop/KDD_corpus/f1035.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-262-513a58dcb453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkddcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f1035.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'([\\d]\\.)+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[\\s]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/corpus/reader/plaintext.pyc\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/corpus/reader/api.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \"\"\"\n\u001b[1;32m    209\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, fileid)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/compat.pyc\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, _path)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No such file or directory: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: No such file or directory: '/Users/linwood/Desktop/KDD_corpus/f1035.txt'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = kddcorpus.raw('f1035.txt')\n",
    "p=re.compile('([\\d]\\.)+')\n",
    "refs = p.search(re.sub('[\\s]',\" \",text)).group()\n",
    "print refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "No such file or directory: '/Users/linwood/Desktop/KDD_corpus/1036.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-261-1ae632301b64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[^ ](AB.T{1,2}.*)[^ ]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkddcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1036.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/corpus/reader/plaintext.pyc\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/corpus/reader/api.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \"\"\"\n\u001b[1;32m    209\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, fileid)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/compat.pyc\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, _path)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No such file or directory: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: No such file or directory: '/Users/linwood/Desktop/KDD_corpus/1036.txt'"
     ]
    }
   ],
   "source": [
    "print re.findall('[^ ](AB.T{1,2}.*)[^ ]',kddcorpus.raw(\"1036.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Estimating Local Intrinsic Dimensionality\\n', u'\\n', u'Laurent Amsaleg\\n', u'Equipe LINKMEDIA,\\n', u'\\n', u'CNRS/IRISA Rennes, France\\n', u'\\n', u'Campus Universitaire de\\n', u'\\n', u'Beaulieu\\n', u'\\n', u'35042 Rennes Cedex, France\\n', u'laurent.amsaleg@irisa.fr\\n', u'\\n', u'Stphane Girard\\n', u'Equipe MISTIS, INRIA\\n', u'\\n', u'Grenoble, France\\n', u'\\n', u'Inovalle, 655, Montbonnot\\n', u'38334 Saint-Ismier Cedex,\\n', u'stephane.girard@inria.fr\\n', u'\\n', u'France\\n', u'\\n', u'Teddy Furon\\n', u'\\n', u'Equipe LINKMEDIA,\\n', u'\\n', u'INRIA/IRISA Rennes, France\\n', u'\\n', u'Campus Universitaire de\\n', u'\\n', u'Beaulieu\\n', u'\\n', u'35042 Rennes Cedex, France\\n', u'\\n', u'teddy.furon@inria.fr\\n', u'\\n', u'Ken-ichi Kawarabayashi\\n', u'\\n', u'National Institute of\\n', u'Informatics, Japan\\n', u'2-1-2 Hitotsubashi,\\n', u'\\n', u'Chiyoda-ku\\n', u'\\n', u'Tokyo 101-8430, Japan\\n', u'k_keniti@nii.ac.jp\\n', u'\\n', u'Oussama Chelly\\n', u'National Institute of\\n', u'Informatics, Japan\\n', u'2-1-2 Hitotsubashi,\\n', u'\\n', u'Chiyoda-ku\\n', u'\\n', u'Tokyo 101-8430, Japan\\n', u'\\n', u'chelly@nii.ac.jp\\n', u'Michael E. Houle\\n', u'National Institute of\\n', u'Informatics, Japan\\n', u'2-1-2 Hitotsubashi,\\n', u'\\n', u'Chiyoda-ku\\n', u'\\n', u'Tokyo 101-8430, Japan\\n', u'\\n', u'meh@nii.ac.jp\\n', u'Michael Nett\\n', u'Google, Japan\\n', u'\\n', u'6-10-1 Roppongi, Minato-ku\\n', u'\\n', u'Tokyo 106-6126, Japan\\n', u'mnett@google.com\\n', u'\\n', u'ABSTRACT\\n', u'This paper is concerned with the estimation of a local mea-\\n', u'sure of intrinsic dimensionality (ID) recently proposed by\\n', u'Houle. The local model can be regarded as an extension of\\n', u'Karger and Ruhls expansion dimension to a statistical set-\\n', u'ting in which the distribution of distances to a query point\\n', u'is modeled in terms of a continuous random variable. This\\n', u'form of intrinsic dimensionality can be particularly useful in\\n', u'search, classication, outlier detection, and other contexts in\\n', u'machine learning, databases, and data mining, as it has been\\n', u'shown to be equivalent to a measure of the discriminative\\n', u'power of similarity functions. Several estimators of local ID\\n', u'are proposed and analyzed based on extreme value theory,\\n', u'using maximum likelihood estimation (MLE), the method\\n', u'of moments (MoM), probability weighted moments (PWM),\\n', u'and regularly varying functions (RV). An experimental eval-\\n', u'uation is also provided, using both real and articial data.\\n', u'\\n', u'Categories and Subject Descriptors\\n', u'G.3 [Mathematics of Computing]: Probability and Statis-\\n', u'ticsDistribution Functions; I.2.6 [Computing Method-\\n', u'ologies]: Articial IntelligenceLearning, Parameter Learn-\\n', u'ing\\n', u'\\n', u'Permission to make digital or hard copies of all or part of this work for personal or\\n', u'classroom use is granted without fee provided that copies are not made or distributed\\n', u'for prot or commercial advantage and that copies bear this notice and the full cita-\\n', u'tion on the rst page. Copyrights for components of this work owned by others than\\n', u'ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-\\n', u'publish, to post on servers or to redistribute to lists, requires prior specic permission\\n', u'and/or a fee. Request permissions from Permissions@acm.org.\\n', u'KDD 15\\n', u'c(cid:13) 2015 ACM. ISBN 978-1-4503-3664-2/15/08 ...$15.00.\\n', u'DOI: http://dx.doi.org/10.1145/2783258.2783405.\\n', u'\\n', u'August 10-13, 2015,\\n', u'\\n', u'Sydney, Australia\\n', u'\\n', u',\\n', u'\\n', u'Keywords\\n', u'intrinsic dimension, indiscriminability, manifold learning\\n', u'\\n', u'1.\\n', u'\\n', u'INTRODUCTION\\n', u'\\n', u'In an attempt to improve the discriminability of similar-\\n', u'ity measures, and the scalability of methods that depend on\\n', u'them, much attention has been given in the areas of machine\\n', u'learning, databases, and data mining to the development of\\n', u'dimensional reduction techniques. Linear techniques for di-\\n', u'mensionality reduction include Principal Component Anal-\\n', u'ysis (PCA) and its variants [4, 24]. Non-linear dimension-\\n', u'ality reduction methods  also known as manifold learn-\\n', u'ing techniques  include Isometric Mapping [36], Multi-\\n', u'Dimensional Scaling [35,37], Locally Linear Embedding and\\n', u'its variants [30], and Non-Linear Component Analysis [32].\\n', u'Most reduction techniques require that a target dimension\\n', u'be provided by the user, although some attempt to deter-\\n', u'mine the dimension automatically. Ideally, the supplied di-\\n', u'mension should depend on the intrinsic dimensionality (ID)\\n', u'of the data. This has served to motivate the development of\\n', u'models of ID, as well as accurate estimators.\\n', u'\\n', u'Over the past few decades, many practical models of the\\n', u'intrinsic dimensionality of data sets have been proposed.\\n', u'Examples include the previously mentioned Principal Com-\\n', u'ponent Analysis and its variants [4, 24], as well as several\\n', u'manifold learning techniques [26, 30, 32, 37]. Topological ap-\\n', u'proaches to ID estimate the basis dimension of the tangent\\n', u'space of the data manifold from local samples [5,38]. Fractal\\n', u'methods such as the Correlation Dimension (CD) estimate\\n', u'an intrinsic dimension from the space-lling capacity of the\\n', u'data [6, 14]. Graph-based methods use the k-nearest neigh-\\n', u'bors graph along with density in order to estimate ID [8].\\n', u'\\n', u'The aforementioned intrinsic dimensionality measures can\\n', u'be described as global, in that they consider the dimension-\\n', u'\\n', u'29\\x0cality of a given set as a whole, without any individual object\\n', u'being given a special role. In contrast, local ID measures\\n', u'are dened in this paper as those that involve only the k-\\n', u'nearest neighbor distances of a specic location in the space.\\n', u'Several local intrinsic dimensionality models have been pro-\\n', u'posed recently, such as the expansion dimension (ED) [25],\\n', u'the generalized expansion dimension (GED) [19], the min-\\n', u'imum neighbor distance (MiND) [31], and local continuous\\n', u'intrinsic dimension (which we will refer to here as LID) [17].\\n', u'These models quantify ID in terms of the rate at which the\\n', u'number of encountered objects grows as the considered range\\n', u'of distances expands from a reference location.\\n', u'\\n', u'Local approaches can be very useful when data is com-\\n', u'posed of heterogeneous manifolds. In addition to applica-\\n', u'tions in manifold learning, measures of local ID have been\\n', u'used in the context of similarity search, where they are used\\n', u'to assess the complexity of a search query [22, 25], or to\\n', u'control the early termination of search [20, 21]. They have\\n', u'also found applications in outlier detection, in the analysis\\n', u'of a projection-based heuristic [9], and in the estimation of\\n', u'local density [39]. The eciency and eectiveness of the\\n', u'algorithmic applications of intrinsic dimensional estimation\\n', u'(such as [20, 21]) depends greatly on the quality of of the\\n', u'estimators employed.\\n', u'\\n', u'Distances from a query point can be seen as realizations\\n', u'of a continuous positive random variable. In this case, the\\n', u'smallest distances encountered would be extreme events\\n', u'associated with the lower tail of the underlying distance dis-\\n', u'tribution.\\n', u'In Extreme Value Theory (EVT), a discipline\\n', u'of statistics concerned with the study of tails of continu-\\n', u'ous probability distributions, the random variable associated\\n', u'with nearest neighbor distances can be assumed to follow a\\n', u'power-law distribution [7]. Continuous lower-bounded ran-\\n', u'dom variables are known to asymptotically converge to the\\n', u'Weibull distribution as the sample size grows, regardless of\\n', u'the original distance measure and its distribution.\\n', u'In an\\n', u'equivalent formulation of EVT due to Karamata, the cu-\\n', u'mulative distribution function of a tail distribution can be\\n', u'represented as a regularly-varying (RV) function whose dom-\\n', u'inant factor is a polynomial in the distance [7,18]; the degree\\n', u'(or index) of this polynomial factor determines the shape\\n', u'parameter of the associated Weibull distribution, or equiva-\\n', u'lently the exponent of the associated power law. The index\\n', u'has been interpreted as a form of intrinsic dimension [7].\\n', u'Maximum likelihood estimation of the index leads to the\\n', u'well-known Hill estimator for power-law distributions [16].\\n', u'While EVT provides an asymptotic description of tail dis-\\n', u'tributions, in the case of continuous distance distributions,\\n', u'the distribution can be exactly characterized in terms of\\n', u'LID [18]. The LID model introduces a function that assesses\\n', u'the discriminative power of the distribution at any given dis-\\n', u'tance value [17, 18]. A distance measure is described as dis-\\n', u'criminative when an expansion in the distance results in a\\n', u'relatively small increase in the number of observations. This\\n', u'function is shown to fully characterize the cumulative dis-\\n', u'tribution function without the explicit involvement of the\\n', u'probability density [18]. The limit of this function yields\\n', u'the skewness of the Weibull distribution (or equivalently,\\n', u'the Karamata representation index, or power law exponent)\\n', u'associated with the lower tail. It is the estimation of this\\n', u'limit that is the main focus of this paper.\\n', u'\\n', u'In addition to the more traditional applications stated ear-\\n', u'lier, LID has the potential for wide application in many ma-\\n', u'\\n', u'chine learning and data mining contexts, as it makes no as-\\n', u'sumptions on the nature of the data distribution other than\\n', u'continuity.\\n', u'\\n', u'The main original contributions of this paper are:\\n', u' a framework for the estimation of local continuous in-\\n', u'trinsic dimension (LID) using well-established tech-\\n', u'niques: the maximum likelihood estimation (MLE),\\n', u'the method of moments (MoM), and the method of\\n', u'probability-weighted moments (PWM). In particular,\\n', u'we verify that applying MLE to LID leads to the well-\\n', u'known Hill estimator [16].\\n', u'\\n', u' a new family of estimators based on the extreme-value-\\n', u'theoretic notion of regularly varying functions. Several\\n', u'existing dimensionality models (ED, GED, and MiND)\\n', u'are shown to be special cases of this family.\\n', u'\\n', u' condence intervals for the variance and convergence\\n', u'\\n', u'of the estimators we propose.\\n', u'\\n', u' an experimental study using articial data and syn-\\n', u'thetic distance distributions, in which we compare our\\n', u'estimators with state-of-the-art global and local esti-\\n', u'mators. We also show that the empirical variance and\\n', u'convergence rates of the MLE (Hill) and MoM estima-\\n', u'tors are superior to those of the other local estimators\\n', u'studied.\\n', u'\\n', u' experiments showing that local estimators are more ro-\\n', u'bust than global ones in the presence of noise in non-\\n', u'linear manifolds. Our experiments show that our ap-\\n', u'proaches are very competitive in this regard with other\\n', u'methods, both local and global.\\n', u'\\n', u' proles of several real-world data sets in terms of LID,\\n', u'illustrating the degree of variability of complexity from\\n', u'region to region within a dataset. The proles demon-\\n', u'strate that a single global ID value is in general not\\n', u'sucient to fully characterize the complexity of real-\\n', u'world data.\\n', u'\\n', u'2. CONTINUOUS INTRINSIC DIMENSION\\n', u'LID [17] aims to quantify the local ID of a feature space\\n', u'exclusively in terms of the distribution of inter-point dis-\\n', u'tances. Formally, let (Rm, d) be a domain equipped with\\n', u'a non-negative distance function d. Let us consider the\\n', u'distribution of distances within the domain with respect to\\n', u'some xed point of reference. We model this distribution in\\n', u'terms of a random variable X with support [0,). X is said\\n', u'to have probability density fX, where fX is a non-negative\\n', u'Lebesgue-integrable function, if and only if\\n', u'\\n', u'Pr[a  X  b] =\\n', u'\\n', u'fX(x) dx,\\n', u'\\n', u'(cid:90) b\\n', u'\\n', u'x=a\\n', u'\\n', u'(cid:90) x\\n', u'\\n', u'for any a, b  [0,) such that a  b. The corresponding\\n', u'cumulative density function FX is canonically dened as\\n', u'\\n', u'FX(x) = Pr[X  x] =\\n', u'\\n', u'fX(u) du.\\n', u'\\n', u'u=0\\n', u'\\n', u'Accordingly, whenever X is absolutely continuous at x, FX\\n', u'is dierentiable at x and its rst-order derivative is fX(x).\\n', u'\\n', u'Definition 1\\n', u'\\n', u'(Houle [17]). Given an absolutely con-\\n', u'tinuous random distance variable X, for any distance thresh-\\n', u'old x such that FX(x) > 0, the local continuous intrinsic\\n', u'\\n', u'30\\x0cdimension of X at distance x is given by\\n', u'\\n', u'IDX(x) (cid:44) lim0+\\n', u'\\n', u'ln FX ((1 + )x)  ln FX(x)\\n', u'\\n', u'ln(1 + )\\n', u'\\n', u'wherever the limit exists.\\n', u'\\n', u'With respect to the generalized expansion dimension [19],\\n', u'a precursor of LID, the above denition of IDX(x) is the out-\\n', u'come of a dimensional test of neighborhoods of radii x and\\n', u'(1+)x in which the neighborhood cardinalities are replaced\\n', u'by the expected number of neighbors. LID also turns out to\\n', u'be equivalent to a formulation of the (lack of) discriminative\\n', u'power of a distance measure, as both formulations have the\\n', u'same closed form:\\n', u'\\n', u'Theorem 1\\n', u'\\n', u'(Houle [17]). Let X be an absolutely con-\\n', u'tinuous random distance variable. If FX is both positive and\\n', u'dierentiable at x, then\\n', u'\\n', u'IDX(x) =\\n', u'\\n', u'xfX(x)\\n', u'FX(x)\\n', u'\\n', u'.\\n', u'\\n', u'3. EXTREME VALUE THEORY\\n', u'\\n', u'Extreme value theory is concerned with the modeling of\\n', u'what can be regarded as the extreme behavior of stochastic\\n', u'processes. Its best known theorem, attributed in parts to\\n', u'Fisher and Tippett [10], and Gnedenko [13], states that the\\n', u'maximum of N independent identically-distributed random\\n', u'variables (after proper renormalization) converges in distri-\\n', u'bution to a generalized extreme value distribution as N goes\\n', u'to innity.\\n', u'3.1 Threshold excesses\\n', u'\\n', u'Consider the following two denitions.\\n', u'Definition 2. Let   R and  > 0. The family of gen-\\n', u'eralized Pareto distributions is dened by its cumulative dis-\\n', u'tribution function:\\n', u'\\n', u'(cid:18)\\n', u'\\n', u'(cid:19) 1\\n', u'\\n', u'\\n', u'\\n', u'.\\n', u'\\n', u'FX (x) = 1 \\n', u'\\n', u'1 +\\n', u'\\n', u'x\\n', u'\\n', u'\\n', u'Definition 3. Let X be a random variable whose distri-\\n', u'bution FX has the upper endpoint x+  R  {}. Given\\n', u'w < x+, the conditional excess distribution FX,w of X is the\\n', u'distribution of X  w conditioned on the event X > w:\\n', u'\\n', u'FX,w (x) =\\n', u'\\n', u'FX(w + x)  FX(w)\\n', u'\\n', u'1  FX(w)\\n', u'\\n', u'.\\n', u'\\n', u'We are now in a position to introduce a powerful theorem\\n', u'due to Balkema and de Haan [1], and Pickands [28], which\\n', u'can be regarded as the counterpart to the central limit the-\\n', u'orem for extremal statistics.\\n', u'\\n', u'Theorem 2\\n', u'\\n', u'(Balkema-de Haan [1], Pickands [28]).\\n', u'Let (Xi)iN be a sequence of independent random variables\\n', u'with identical distribution function FX satisfying the condi-\\n', u'tions of the Fisher-Tippett-Gnedenko Theorem. As w  x+,\\n', u'FX,w (x) converges to a distribution in FGPD.\\n', u'\\n', u'In the following we demonstrate a direct relation between\\n', u'local ID and extreme value theory, which arises as an im-\\n', u'plication of Theorem 2. Note that any choice of distance\\n', u'threshold w corresponds to a neighborhood of radius w based\\n', u'\\n', u'at the reference point, or equivalently, to the tail of the dis-\\n', u'tribution of distances on [0, w). As discussed in [7], Theo-\\n', u'rem 2 also applies to lower tails: one can reason about min-\\n', u'ima using the transformation Y = X. The distribution of\\n', u'the excess Y  (w) (conditioned on Y > w) then tends\\n', u'to a distribution in FGPD, as w tends to the lower endpoint\\n', u'of FX located at zero. Accordingly, as w tends to zero, the\\n', u'distribution in the tail [0, w) can be restated as follows [7].\\n', u'Lemma 1. Let X be an absolutely continuous random dis-\\n', u'tance variable with support [0,) and cumulative distribu-\\n', u'tion function FX such that FX(x) > 0 if x > 0. Let c  (0, 1)\\n', u'be an arbitrary constant. Let w > 0 be a distance threshold,\\n', u'and consider x restricted to the range [cw, w). As w tends\\n', u'to zero, the distribution of X restricted to the tail [cw, w)\\n', u'satises, for some xed  < 0:\\n', u'\\n', u' 1\\n', u'\\n', u'\\n', u'\\n', u'(x/w)\\n', u'FX,w (x)\\n', u'\\n', u' 1\\n', u'\\n', u'Note that the distribution of excess distance w  X is\\n', u'bounded from above by w which, according to [7], enforces\\n', u'that  < 0.\\n', u'\\n', u'To summarize, whenever Theorem 2 applies to a distance\\n', u'variable X, the cumulative distribution of distances within\\n', u'a radius-w neighborhood is asymptotically determined by a\\n', u'single parameter  < 0. We can prove the following state-\\n', u'ment concerning LID.\\n', u'\\n', u'Theorem 3. Let X be an absolutely continuous random\\n', u'distance variable with support [0,), satisfying the condi-\\n', u'tions of Theorem 2, and w > 0 be a distance threshold.\\n', u'Then, as w tends to zero,\\n', u'\\n', u'IDX(w)   1\\n', u'\\n', u'\\n', u'=: IDX.\\n', u'\\n', u'Proof. Omitted due to space limitations.\\n', u'\\n', u'Note that together Lemma 1 and Theorem 3 allow us to\\n', u'restate the asymptotic cumulative distribution of distances\\n', u'in the tail [cw, w) as\\n', u'\\n', u'(x/w)IDX\\n', u'FX,w (x)\\n', u'\\n', u' 1.\\n', u'\\n', u'(1)\\n', u'\\n', u'3.2 Regularly-varying functions\\n', u'\\n', u'The Fisher-Tippett-Gnedenko Theorem and the Pickands-\\n', u'Balkema-de Haan Theorem have been shown to be equiva-\\n', u'lent to a third characterization of the tail behavior, in terms\\n', u'of regularly-varying (RV) functions. The asymptotic cumu-\\n', u'lative distribution of X in the tail [0, w) can be expressed as\\n', u'FX(x) = x(cid:96)X(1/x), where (cid:96)X is dierentiable and slowly\\n', u'varying; that is, for all c > 0, (cid:96)X satises\\n', u'\\n', u'lim\\n', u't\\n', u'\\n', u'(cid:96)X(ct)\\n', u'(cid:96)X(t)\\n', u'\\n', u'= 1.\\n', u'\\n', u'FX restricted to [0, w) is itself said to be regularly varying\\n', u'with index . In particular, a cumulative distribution F \\n', u'FGEV has  < 0 if and only if F is RV and has a nite\\n', u'endpoint. Note that the slowly-varying component (cid:96)X(1/x)\\n', u'of FX is not necessarily constant as x tends to zero. For a\\n', u'detailed account of RV functions, we refer the reader to [2].\\n', u'The following corollary is a straightforward extension of\\n', u'\\n', u'the examples given in Section 2.\\n', u'\\n', u'Corollary 1. Let X be a random distance variable re-\\n', u'stricted to [0, w) with distribution FX(x) = x(cid:96)X(1/x). As\\n', u'w tends to zero, the index  converges to IDX.\\n', u'\\n', u'31\\x0c4. ESTIMATION\\n', u'\\n', u'This section is concerned with practical methods for the\\n', u'estimation of the local intrinsic dimension of a random dis-\\n', u'tance variable X. In particular, we adapt known GPD pa-\\n', u'rameter estimators such as the maximum-likelihood estima-\\n', u'tor (in Section 4.1) and moment based estimators (in Sec-\\n', u'tions 4.2 and 4.3), and propose a new estimator based on\\n', u'regularly varying functions (in Section 4.4).\\n', u'\\n', u'For the remainder of this discussion we assume that we\\n', u'are given a sequence x1, . . . , xn of observations of a random\\n', u'distance variable X with support [0, w), in ascending order\\n', u' that is, x1  x2    xn.\\n', u'4.1 Maximum Likelihood Estimation\\n', u'\\n', u'Using the asymptotic expression of the distance distribu-\\n', u'tion given in Equation 1, we see that the log-likelihood of\\n', u'IDX for the sample is\\n', u'L(IDX) = n ln\\n', u'\\n', u'n(cid:88)\\n', u'Accordingly, the maximum-likelihood estimate (cid:99)IDX is\\n', u'\\n', u'+ n ln IDX + (IDX  1)\\n', u'\\n', u'FX,w (w)\\n', u'\\n', u'xi\\n', u'w\\n', u'\\n', u'i=1\\n', u'\\n', u'ln\\n', u'\\n', u'w\\n', u'\\n', u'.\\n', u'\\n', u'(cid:99)IDX = \\n', u'\\n', u'(cid:18) 1\\n', u'\\n', u'(cid:88)n\\n', u'\\n', u'n\\n', u'\\n', u'i=1\\n', u'\\n', u'ln\\n', u'\\n', u'xi\\n', u'w\\n', u'\\n', u'(cid:19)1\\n', u'\\n', u',\\n', u'\\n', u'which follows the form of the well-known Hill estimator for\\n', u'the scaling exponent of a power-law tail distribution [16].\\n', u'\\n', u'The MLE model ensures the usual regularity conditions\\n', u'that guarantee the consistency, the asymptotic normality\\n', u'and the eciency of this estimator. The variance is asymp-\\n', u'totically given by the inverse of the Fisher information de-\\n', u'ned as:\\n', u'\\n', u'(cid:20)\\n', u'\\n', u'I = E\\n', u'\\n', u' 2L(IDX)\\n', u'\\n', u' ID2\\n', u'X\\n', u'\\n', u'=\\n', u'\\n', u'n\\n', u'ID2\\n', u'X\\n', u'\\n', u',\\n', u'\\n', u'where E[] denotes the expectation. Therefore, if the number\\n', u'/ n). Accordingly, with probability 1  , a sample of n dis-\\n', u'\\n', u'of samples n is suciently large, we have(cid:99)IDX  N (IDX, ID2\\n', u'tances in [0, w) provides an estimate (cid:99)IDX lying within\\n', u'\\n', u'X\\n', u'\\n', u'(cid:21)\\n', u'\\n', u'(cid:18)\\n', u'\\n', u'IDX  IDX\\n', u'n\\n', u'\\n', u'1\\n', u'\\n', u'\\n', u'\\n', u'1  \\n', u'2\\n', u'\\n', u'In other words, the 1   condence interval is\\n', u'\\n', u'(cid:34)\\n', u'\\n', u'(cid:99)IDX\\n', u'\\n', u'.\\n', u'\\n', u'(cid:19)\\n', u'(cid:99)IDX\\n', u'\\n', u'1 + n1/21(1  /2)\\n', u'\\n', u',\\n', u'\\n', u'1  n1/21(1  /2)\\n', u'\\n', u'(cid:35)\\n', u'\\n', u'.\\n', u'\\n', u'4.2 Method of Moments\\n', u'\\n', u'For any choice of k  N, the k-th order non-central mo-\\n', u'\\n', u'ment k of the random distance X is\\n', u'\\n', u'k = E\\n', u'\\n', u'=\\n', u'\\n', u'xkfX(x) dx = wk\\n', u'\\n', u'(cid:104)\\n', u'\\n', u'Xk(cid:105)\\n', u'\\n', u'(cid:90) w\\n', u'\\n', u'x=0\\n', u'\\n', u'IDX\\n', u'\\n', u'IDX + k\\n', u'\\n', u'.\\n', u'\\n', u',\\n', u'\\n', u'(cid:17)\\n', u'\\n', u'(cid:16) k\\n', u'(cid:80)n\\n', u'\\n', u'wk\\n', u'\\n', u'Solving for the intrinsic dimension gives\\n', u'\\n', u'IDX = k\\n', u'\\n', u'k\\n', u'\\n', u'k  wk = g\\n', u'\\n', u'with g(x) = k x\\n', u'by its empirical counterpart k = 1\\n', u'n\\n', u'E[k] = k and E[2\\n', u'\\n', u'1x . When estimating the order-k moment\\n', u'i , we see that\\n', u'\\n', u'r] = (n2k + n(n  1)2\\n', u'\\n', u'k)n2, so that\\n', u'\\n', u'i=1 xk\\n', u'\\n', u'Var[2\\n', u'\\n', u'k] =\\n', u'\\n', u'2k  2\\n', u'\\n', u'k\\n', u'\\n', u'n\\n', u'\\n', u'=\\n', u'\\n', u'w2kIDXk2\\n', u'\\n', u'n(IDX + 2k)(IDX + k)2 .\\n', u'\\n', u'mk,l,m = E\\n', u'\\n', u'(cid:18) IDX\\n', u'\\n', u'Therefore, the distribution of k\\n', u'with\\n', u'\\n', u'wk is asymptotically normal\\n', u'\\n', u'k\\n', u'\\n', u'wk  N\\n', u'\\n', u';\\n', u'\\n', u'IDXk2\\n', u'\\n', u'n(IDX + 2k)(IDX + k)2\\n', u'\\n', u'IDX + k\\n', u'\\n', u'According to [29, Th. 6a2.9], if x  N (; 2n1) asymp-\\n', u'totically, then g(x)  N (g(); 2n1g(cid:48)()2), where g(cid:48) is the\\n', u'rst-order derivative of g. Therefore, asymptotically\\n', u'\\n', u'(cid:18)\\n', u'\\n', u'(cid:99)IDX  N\\n', u'\\n', u'(cid:18)\\n', u'\\n', u'(cid:19)\\n', u'\\n', u'.\\n', u'\\n', u'(cid:19)(cid:19)\\n', u'\\n', u'.\\n', u'\\n', u'IDX;\\n', u'\\n', u'ID2\\n', u'X\\n', u'n\\n', u'\\n', u'1 +\\n', u'\\n', u'(k/IDX)2\\n', u'\\n', u'ID2\\n', u'\\n', u'X(1 + 2k/IDX)\\n', u'\\n', u'This variance is monotonically increasing in k/IDX, which\\n', u'indicates that we should use moments of small order k.\\n', u'When k/IDX tends to zero, the variance converges to ID2\\n', u'X/n,\\n', u'the variance of the maximum-likelihood estimator (see Sec-\\n', u'tion 4.1). Note that an upper bound on IDX implies that the\\n', u'variance is bounded. In this case we can derive condence\\n', u'intervals similar to Section 4.1.\\n', u'4.3 Probability-Weighted Moments\\n', u'\\n', u'General probability-weighted moments are dened as\\n', u'\\n', u'(cid:104)\\n', u'\\n', u'FX(x)k(1  FX(x))lXm(cid:105)\\n', u'(cid:105)\\n', u'\\n', u'(cid:90) w\\n', u'\\n', u'.\\n', u'\\n', u'We restrict here our attention to a subfamily: for any choice\\n', u'of k  N, k is dened as\\n', u'FX(x)kX\\n', u'\\n', u'FX(x)kxfX(x) dx\\n', u'\\n', u'k (cid:44) E\\n', u'\\n', u'=\\n', u'\\n', u'(cid:104)\\n', u'\\n', u'=\\n', u'\\n', u'IDX w\\n', u'\\n', u'IDX k + IDX + 1\\n', u'\\n', u'x=0\\n', u'\\n', u';\\n', u'\\n', u'solving for the intrinsic dimension yields\\n', u'\\n', u'IDX =\\n', u'\\n', u'k\\n', u'\\n', u'w  k(k + 1)\\n', u'\\n', u'= h\\n', u'\\n', u'(cid:17)\\n', u'\\n', u',\\n', u'\\n', u'(cid:16) k\\n', u'\\n', u'w\\n', u'\\n', u'x\\n', u'\\n', u'1(k+1)x .\\n', u'\\n', u'where h(x) =\\n', u'4.4 Estimation Using Regularly Varying Func-\\n', u'\\n', u'tions\\n', u'\\n', u'In this section we introduce an ad hoc estimator for the\\n', u'intrinsic dimensionality based on the characterization of dis-\\n', u'tribution tails as regularly varying functions (as discussed in\\n', u'Section 3). Consider the empirical distribution function FX,\\n', u'dened as\\n', u'\\n', u'FX(x) =\\n', u'\\n', u'where (cid:74)(cid:75) refers to the Iverson bracket which evaluates to\\n', u'\\n', u'1 if  is true, and 0 otherwise. We propose the following\\n', u'estimator for the index  of FX.\\n', u'\\n', u'(cid:88)n\\n', u'j=1(cid:74)xj < x(cid:75) ,\\n', u'\\n', u'1\\n', u'n\\n', u'\\n', u'Definition 4. Let X be an absolutely continuous random\\n', u'distance variable restricted to [0, w). The local intrinsic di-\\n', u'mension IDX can be estimated as\\n', u'\\n', u'(cid:104) FX((1 + jn)xn)/ FX(xn)\\n', u'(cid:80)J\\n', u'\\n', u'j=1 j ln(1 + jn)\\n', u'\\n', u'(cid:105)\\n', u'\\n', u',\\n', u'\\n', u'(cid:99)IDX =  =\\n', u'\\n', u'(cid:80)J\\n', u'\\n', u'j=1 j ln\\n', u'\\n', u'under the assumption that xn, n  0 as n  , where\\n', u'(j)1jJ and (j)1jJ are sequences.\\n', u'\\n', u'We will refer to this family of estimators as RV, for reg-\\n', u'ularly varying. Note that since RV estimators involve only\\n', u'the products jn for 1  j  J, we may assume without\\n', u'loss of generality that 1 +  + J = 1. The estimators are\\n', u'based on the observation that, for all 1  j  J,\\n', u'\\n', u'32\\x0cln [FX((1 + jn)xn)/FX(xn)]\\n', u'=  ln(1 + jn) + ln [(cid:96)X((1 + jn)xn)/(cid:96)X(xn)]\\n', u'(cid:39)  ln(1 + jn).\\n', u'\\n', u'The RV family covers several of the known local estima-\\n', u'tors of intrinsic dimensionality. For the parameter choices\\n', u'J = 1 and  =  n, the RV estimator reduces to the GED\\n', u'formulation proposed in [19]:\\n', u'\\n', u'(cid:104) FX((1 + )xn)/ FX(xn)\\n', u'\\n', u'(cid:105)\\n', u'\\n', u'ln\\n', u'\\n', u'(cid:99)IDX =\\n', u'\\n', u'ln(1 + )\\n', u'\\n', u',\\n', u'\\n', u'By setting  = 1, Karger & Ruhls expansion dimension\\n', u'is obtained, while by setting xn as the distance to the k-\\n', u'nearest neighbor and  such as (1 + )xn as the distance to\\n', u'the nearest neighbor, we nd a special case of the MiND\\n', u'family (MiNDml1) [31].\\n', u'Alternatively, by setting J = n, i = 1 for all i  [1..n],\\n', u', the RV\\n', u'\\n', u'and choosing the vector  such that 1 + in = xi\\n', u'xn\\n', u'estimator becomes\\n', u'\\n', u'(cid:80)n\\n', u'(cid:80)n\\n', u'\\n', u'(cid:99)IDX =\\n', u'\\n', u'j=1 ln [j/n]\\n', u'j=1 ln [xj/xn]\\n', u'\\n', u'(cid:80)n\\n', u'\\n', u' ln\\n', u'\\n', u'\\n', u'2n  n\\n', u'j=1 ln [xj/xn]\\n', u'\\n', u'As n  , this converges to the MLE (Hill) estimator pre-\\n', u'sented in Section 4.1, with w = xn.\\n', u'\\n', u'We now turn our attention to an analysis of the variation\\n', u'of RV estimators. First, we introduce an auxiliary function\\n', u'which drives the speed of convergence of the estimator pro-\\n', u'posed in Denition 4. For x  R let X(x) be dened as\\n', u'\\n', u'X(x) (cid:44) x(cid:96)(cid:48)\\n', u'\\n', u'X(x)\\n', u'(cid:96)X(x)\\n', u'\\n', u'.\\n', u'\\n', u'In [11, 12], the auxiliary function is assumed to be regu-\\n', u'larly varying, and the estimation of the corresponding reg-\\n', u'ular variation index is addressed. Within this article, so\\n', u'as to prove the following results, we limit ourselves to the\\n', u'assumption that X is ultimately non-increasing.\\n', u'\\n', u'Theorem 4. Let X be a random distance variable over\\n', u'[0, w) with distribution function FX(x) = x(cid:96)X(1/x), and\\n', u'let max (cid:44) max1jJ j. Furthermore, let n, xn  0 so that\\n', u'\\n', u'n FX(xn)n   and(cid:112)nFX(xn)nX(1/[(1+maxn)xn]) \\n', u'ultimately non-increasing, then (cid:112)nFX(xn)n [IDX (cid:99)IDX]\\n', u'\\n', u'0 as n approaches innity. If the auxiliary function X is\\n', u'\\n', u'converges to a centered Gaussian with variance\\n', u'\\n', u'IDXV, = IDX\\n', u'\\n', u'(cid:62)S\\n', u'((cid:62) )2 ,\\n', u'\\n', u'where Sa,b = (|a|  |b|)(cid:74)ab > 0(cid:75) for (a, b)  {1, . . . , J}2.\\n', u'\\n', u'(A  B denotes the minimum of A and B.)\\n', u'\\n', u'Note that the requirement nFX(xn)n   can be inter-\\n', u'preted as a necessary and sucient condition for the almost\\n', u'sure presence of at least one distance sample in the interval\\n', u'[xn, (1 + jn)xn)]. In addition, the condition\\n', u'\\n', u'(cid:112)nFX(xn)nX(1/[rn(1 + maxn)])  0\\n', u'\\n', u'enforces that the approximation bias X(1/[(1 + n)xn]) is\\n', u'negligible compared to the standard deviation of the esti-\\n', u'\\n', u'mate, 1/(cid:112)nFX(xn)n. We continue the analysis by propos-\\n', u'\\n', u'ing choices of  that minimize the variance in Theorem 4.\\n', u'\\n', u'Lemma 2. The weight vector  = (1, . . . , J )(cid:62) mini-\\n', u'mizing V, is proportional to 0 = S1 = (1, 0, . . . , 0)(cid:62),\\n', u'and the associated optimal variance is given by V0( ) =\\n', u'\\n', u'(cid:0)(cid:62)S1(cid:1)1\\n', u'\\n', u'.\\n', u'\\n', u'Proof. Omitted due to space limitations.\\n', u'For the case J = 1, we see that  = (1)(cid:62) and V0(1) = 1.\\n', u'This indicates that the GED minimizes the variance of es-\\n', u'timation. However, dierent choices can be made regarding\\n', u'the weight vector  and regarding the criterion to use in or-\\n', u'der to optimize the choice of . Minimizing variance is one\\n', u'choice explored in this paper, but other criteria can be used.\\n', u'In general, however, the following condence interval holds\\n', u'for RV estimators:\\n', u'\\n', u'Lemma 3. Let   (0, 1), and assume that the assump-\\n', u'tions of Theorem 4 hold with  = S1 . Let u = 1((1 +\\n', u')/2), where  is the cumulative distribution function of the\\n', u'standard Gaussian distribution. Then\\n', u'\\n', u'(cid:16)\\n', u'nnV0( )(cid:99)IDX FX(xn)\\n', u'\\n', u'(cid:17)1/2\\n', u'\\n', u'IDX  u\\n', u'\\n', u'are the boundaries of the asymptotic condence interval of\\n', u'\\n', u'level  for (cid:99)IDX.\\n', u'\\n', u'Proof. Lemma 3 is a direct consequence of the asymp-\\n', u'totic distribution established in Theorem 4 and the conver-\\n', u'gence of FX(xn) to FX(xn) as n  .\\n', u'5. EXPERIMENTAL FRAMEWORK\\n', u'5.1 Methods\\n', u'\\n', u'The methods used in this study include MLE, MoM,\\n', u'PWM, and RV. The RV estimators are evaluated for the\\n', u'choices J = 1 and J = 2, as follows:\\n', u'\\n', u'(cid:40) ln nln(cid:98)n/2(cid:99)\\n', u'\\n', u'ln xnln x(cid:98)n/2(cid:99) ,\\n', u'ln(cid:98)n/j(cid:99)(p1) ln(cid:98)i/j(cid:99)\\n', u'ln xn/xj +(p1) ln xi/xj\\n', u'\\n', u'if J = 1\\n', u'\\n', u',\\n', u'\\n', u'if J = 2,\\n', u'\\n', u'(cid:99)IDRV =\\n', u'\\n', u'where p = (xi  2xj + xn)/(xn  xj), i = (cid:98)n/2(cid:99), and j =\\n', u'(cid:98)3n/4(cid:99). Note that the estimator RV for J = 1 is a form\\n', u'of generalized expansion dimension (GED) [19]. For every\\n', u'dataset, we report the average of ID estimates across all the\\n', u'points in the dataset. All estimators in our study can be\\n', u'computed in time linear in the number of sample points.\\n', u'\\n', u'Parameters\\n', u'threshold = 0.025\\n', u'k = 100,  = 1, M = 1, N = 10\\n', u'k = 100,  = 1, M = 10, N = 1\\n', u'\\n', u'Method\\n', u'PCA\\n', u'kNNG1\\n', u'kNNG2\\n', u'MiNDml1 None\\n', u'MiNDmli\\n', u'\\n', u'k = 100\\n', u'\\n', u'Table 1: Parameter choices used in the experiments.\\n', u'\\n', u'Our experimental framework includes several state-of-the-\\n', u'art intrinsic dimensionality measures. The global estima-\\n', u'tors consist of a projection method (PCA), fractal methods\\n', u'(CD [6], Hein [15], Takens [34]), and graph-based methods\\n', u'(kNNG1, kNNG2 [8]). The local distance-based estimators\\n', u'are MiNDml1 and MiNDmli [31]. Table 1 summarizes the\\n', u'parameter choices for every method, except for the fractal\\n', u'methods, which do not involve any parameter.\\n', u'\\n', u'The MiND variants makes more restrictive assumptions\\n', u'than our methods: they assume the data to be uniformly dis-\\n', u'tributed on a hypersphere, with a locally isometric smooth\\n', u'\\n', u'33\\x0c(a) ID = 2\\n', u'\\n', u'(b) ID = 8\\n', u'\\n', u'(c) ID = 32\\n', u'\\n', u'(d) ID = 128\\n', u'\\n', u'Figure 1: Comparison of the mean and standard deviation of LID estimates provided by MLE, MoM and RV (for J = 1\\n', u'and J = 2) on increasingly large samples drawn from articially-generated distance distributions. The results cover target\\n', u'dimensionality values of 2, 8, 32, and 128. The values are marked in the corresponding plots.\\n', u'\\n', u'map between the hypersphere and the representational space.\\n', u'MiND uses only the two extreme samples (smallest and\\n', u'largest), and requires knowledge of the dimension of the\\n', u'space (D). In contrast, our approach assumes only that the\\n', u'nearest neighbor distances are in the lower tail of the dis-\\n', u'tance distribution, where EVT estimation can be performed.\\n', u'5.2 Articial Distance Distributions\\n', u'\\n', u'In the following we propose a set of experiments concern-\\n', u'ing articial data, and describe the method employed for the\\n', u'generation of test data.\\n', u'\\n', u'First, consider a point P drawn uniformly at random from\\n', u'within the m-dimensional unit sphere, for some choice of\\n', u'm  N. According to the method of normal variates, we\\n', u'dene P = Z1/mY(cid:107)Y(cid:107)1, where Z is uniformly distributed\\n', u'on [0, 1], and Y is a random vector in Rm whose coecients\\n', u'follow the standard normal distribution. The distance of\\n', u'P, with respect to our choice of reference point at location\\n', u'0  Rm, is distributed as follows.\\n', u'(cid:107)Z1/mY(cid:107)\\n', u'\\n', u'X =\\n', u'\\n', u'(cid:107)Y(cid:107) = Z1/m.\\n', u'\\n', u'Note that, by measuring LID purely based on distance values\\n', u'with respect to a reference point, the model does not require\\n', u'that the data have an underlying spatial representation. As\\n', u'such, non-integer values of m  R can be selected for the\\n', u'generation of distances, if desired.\\n', u'For choices of m  {2, 8, 32, 128}, we draw 100 indepen-\\n', u'dent sequences of sample distance values from the distribu-\\n', u'tion described above, and record the estimates produced by\\n', u'each of our methods for sample sizes n between 10 and 104.\\n', u'5.3 Articial Data\\n', u'\\n', u'The data sets used in our experiments have been proposed\\n', u'in [31]. They consist of 15 manifolds of various stuctures\\n', u'and intrinsic dimensionalities (d) represented in spaces of\\n', u'dierent dimensions (D). They are summarized in Table 2.\\n', u'These datasets were generated in dierent sizes (103, 104,\\n', u'and 105 points) in order to evaluate the eect of the num-\\n', u'\\n', u'Manifold\\n', u'\\n', u'd D Description\\n', u'\\n', u'1\\n', u'2\\n', u'3\\n', u'\\n', u'4\\n', u'5\\n', u'6\\n', u'7\\n', u'8\\n', u'9\\n', u'\\n', u'10a\\n', u'10b\\n', u'10c\\n', u'11\\n', u'12\\n', u'13\\n', u'\\n', u'10\\n', u'3\\n', u'4\\n', u'\\n', u'4\\n', u'2\\n', u'6\\n', u'2\\n', u'12\\n', u'20\\n', u'10\\n', u'17\\n', u'24\\n', u'2\\n', u'20\\n', u'1\\n', u'\\n', u'11 Uniformly sampled sphere.\\n', u'\\n', u'5 Ane space.\\n', u'6 Concentrated gure\\n', u'\\n', u'confusable with a 3d one.\\n', u'\\n', u'8 Non-linear manifold.\\n', u'3\\n', u'\\n', u'2-d Helix\\n', u'\\n', u'36 Non-linear manifold.\\n', u'\\n', u'3\\n', u'\\n', u'Swiss-Roll.\\n', u'\\n', u'72 Non-linear manifold.\\n', u'20 Ane space.\\n', u'11 Uniformly sampled hypercube.\\n', u'18 Uniformly sampled hypercube.\\n', u'25 Uniformly sampled hypercube.\\n', u'3 Mobius band 10-times twisted.\\n', u'\\n', u'Isotropic multivariate Gaussian.\\n', u'\\n', u'20\\n', u'13 Curve.\\n', u'\\n', u'Table 2: Articial datasets used in the experiments.\\n', u'\\n', u'ber of points on the quality of the dierent estimators. For\\n', u'each dataset and for each of the three sizes, we average the\\n', u'estimates over 20 instances.\\n', u'\\n', u'In order to evaluate the robustness of the estimators, we\\n', u'also prepared versions of these datasets with noise added.\\n', u'For each attribute f , we added normally-distributed noise\\n', u'with mean equal to zero and standard deviation n = p  f\\n', u'where f is the standard deviation of the attribute itself,\\n', u'and p  {0.01, 0.04, 0.16, 0.64}. For attributes with f = 0,\\n', u'the noise was generated with standard deviation n = p \\n', u'f\\n', u'where \\n', u'f is the minimum of the nonzero standard deviations\\n', u'over all attributes.\\n', u'\\n', u'5.4 Real Data\\n', u'\\n', u'Not only can a reliable estimation of ID greatly benet the\\n', u'practical performance of many applications, it also serves as\\n', u'a characterization of high-dimensional data sets and the po-\\n', u'tential problems associated with their use in practice. To\\n', u'this end, we investigate the distribution of LID estimates on\\n', u'\\n', u' 1.8 2 2.2 2.4 2.6 2.8 3 3.2 3.4 3.6101102103104Mean Estimated IDNumber of samplesIDMLEMoMRV (J=1)RV (J=2) 0 0.2 0.4 0.6 0.8 1 1.2101102103104Standard Deviation / IDNumber of samplesMLEMoMRV (J=1)RV (J=2) 7 8 9 10 11 12 13101102103104Mean Estimated IDNumber of samplesIDMLEMoMRV (J=1)RV (J=2) 0 0.2 0.4 0.6 0.8 1 1.2101102103104Standard Deviation / IDNumber of samplesMLEMoMRV (J=1)RV (J=2) 30 32 34 36 38 40 42 44 46 48101102103104Mean Estimated IDNumber of samplesIDMLEMoMRV (J=1)RV (J=2) 0 0.2 0.4 0.6 0.8 1 1.2101102103104Standard Deviation / IDNumber of samplesMLEMoMRV (J=1)RV (J=2) 120 130 140 150 160 170 180 190 200 210 220101102103104Mean Estimated IDNumber of samplesIDMLEMoMRV (J=1)RV (J=2) 0 0.2 0.4 0.6 0.8 1 1.2101102103104Standard Deviation / IDNumber of samplesMLEMoMRV (J=1)RV (J=2)34\\x0cFigure 2: Plots of the distribution of LID values across 104 distinct query locations for each data set. The LID values were\\n', u'obtained using the MLE estimator on the size-1000 neighborhoods of the individual reference points.\\n', u'\\n', u'Figure 3: Histograms of LID values across 104 distinct query locations for each data set, obtained using the MLE estimator\\n', u'on the size-1000 neighborhoods of the individual reference points.\\n', u'\\n', u'the following data sets, each taken from a real-world appli-\\n', u'cation scenario.\\n', u'\\n', u'The ALOI (Amsterdam Library of Object Images) data\\n', u'set contains a total of 110250 color photos of 1000 dier-\\n', u'ent objects taken from varying viewpoints under various il-\\n', u'lumination conditions. Each image is described by a 641-\\n', u'dimensional vector of color and texture features [3].\\n', u'\\n', u'The MNIST database [27] contains of 70000 recordings of\\n', u'handwritten digits. The images have been normalized and\\n', u'discretized to a 28  28-pixel grid. The gray-scale values of\\n', u'the resulting 784 pixels are used to form the feature vectors.\\n', u'The ANN SIFT1B data set consists of 128-dimensional\\n', u'SIFT descriptors extracted from a collection of  109 im-\\n', u'ages. This set has been created for the evaluation of nearest-\\n', u'neighbor search strategies at very large scales [23].\\n', u'\\n', u'For each data set, we estimate LID with respect to 104 dis-\\n', u'tinct reference points, based on the distribution of distances\\n', u'to their respective 103-nearest neighbors. For ANN SIFT1B\\n', u'we use a selection of 104 query points that is provided with\\n', u'the data. In the case of ALOI and MNIST, we computed\\n', u'distance samples with respect to 104 points selected uni-\\n', u'formly at random.\\n', u'\\n', u'6. EXPERIMENTAL RESULTS\\n', u'6.1 Articial Distance Distributions\\n', u'\\n', u'We begin our experimental study with an assessment \\n', u'in terms of bias, variance, and convergence  of the ability\\n', u'of each estimator to identify the ID of a sample of distance\\n', u'values generated according to dierent choices of target ID.\\n', u'Note that for these trials, the distributional model asserted\\n', u'in Lemma 1 holds everywhere on the range [0, w) by con-\\n', u'struction (with w = 1).\\n', u'\\n', u'Fig. 1 shows the behavior of MLE, MoM, and RV (for\\n', u'choices of J = 1 and J = 2). The convergence to the tar-\\n', u'get ID value observed in every case empirically conrms the\\n', u'consistency of these estimators. Likewise, PWM is consis-\\n', u'\\n', u'tent however, one should beware of PWMs susceptibility\\n', u'to the eects of numerical instability.\\n', u'\\n', u'We also note that the RV estimator with J = 1 (GED)\\n', u' which asymptotically minimizes variance according to\\n', u'Lemma 2  is not the choice that minimizes variance when\\n', u'the number of samples is limited. Faster initial convergence\\n', u'favors the choice of MLE and MoM for applications where\\n', u'the number of available query-to-neighbor distances is lim-\\n', u'ited, or where time complexity is an issue.\\n', u'6.2 Articial Data\\n', u'\\n', u'In Tables 3 and 4, due to space limitations, we present\\n', u'only a representative selection of the experimental results,\\n', u'averaged over 20 runs each. It should be noted that as PCA\\n', u'and MiNDmli estimates are restricted to integer values, their\\n', u'bias is lower for examples having integer ground-truth intrin-\\n', u'sic dimension, especially when this dimensionality is small.\\n', u'Also, unlike the other estimators tested, MiND estimators\\n', u'also require that an upper bound on the ID be supplied\\n', u'(set to D in these experiments). PCA requires a threshold\\n', u'parameter to be supplied, the value of which can greatly\\n', u'inuence the estimation.\\n', u'\\n', u'The experimental results indicate that local estimators\\n', u'tend to over-estimate dimensionality in the case of non-linear\\n', u'manifolds (sets m3, m4, m5, m6, m7, m8, m11 and m13) and\\n', u'to under-estimate it in the case of linear manifolds (sets m1,\\n', u'm2, m9, m10a, m10b, m10c and m12). For highly non-linear\\n', u'manifolds, such as the Swiss Roll (m7), global estimators\\n', u'have diculty in identifying the intrinsic dimension. The\\n', u'experimental results with higher sampling rates conrm the\\n', u'reduction in bias that would be expected with smaller k-\\n', u'nearest-neighbor distances, as the local manifold structure\\n', u'more closely approximates the tangent space.\\n', u'\\n', u'To show the eects of noise on the estimators, we display\\n', u'in Tables 5, 6 and 7 for each method the deviation of\\n', u'every estimate in the presence of noise as a proportion of\\n', u'the estimate obtained in the absence of noise. On the one\\n', u'hand, we note that global methods, k-NNG in particular, are\\n', u'\\n', u' 0 5 10 15 20 25 30 350K2K4K6K8K10KEstimated IDQueryALOI 0 5 10 15 20 25 30 350K2K4K6K8K10KEstimated IDQueryMNIST 0 5 10 15 20 25 30 350K2K4K6K8K10KEstimated IDQueryANN_SIFT1BABDEFCG0K1K2K3K4K 0 5 10 15 20 25 30 35Abs. FrequencyEstimated IDALOI0K1K2K3K4K 0 5 10 15 20 25 30 35Abs. FrequencyEstimated IDMNIST0K1K2K3K4K 0 5 10 15 20 25 30 35Abs. FrequencyEstimated IDANN_SIFT1B35\\x0cDataset\\n', u'\\n', u'm1\\n', u'm2\\n', u'm3\\n', u'm7\\n', u'm8\\n', u'm9\\n', u'\\n', u'm10a\\n', u'm10c\\n', u'm11\\n', u'm12\\n', u'\\n', u'd D IDMLE\\n', u'8.07\\n', u'10\\n', u'3\\n', u'2.67\\n', u'3.56\\n', u'4\\n', u'2.49\\n', u'2\\n', u'12.29\\n', u'12\\n', u'12.39\\n', u'20\\n', u'7.39\\n', u'10\\n', u'24\\n', u'14.05\\n', u'2.49\\n', u'2\\n', u'20\\n', u'12.48\\n', u'\\n', u'11\\n', u'5\\n', u'6\\n', u'3\\n', u'72\\n', u'20\\n', u'11\\n', u'25\\n', u'3\\n', u'20\\n', u'\\n', u'Dataset\\n', u'\\n', u'm1\\n', u'm2\\n', u'm3\\n', u'm7\\n', u'm8\\n', u'm9\\n', u'\\n', u'm10a\\n', u'm10c\\n', u'm11\\n', u'm12\\n', u'\\n', u'd D IDMLE\\n', u'9.04\\n', u'10\\n', u'2.88\\n', u'3\\n', u'3.86\\n', u'4\\n', u'2\\n', u'1.96\\n', u'13.72\\n', u'12\\n', u'14.47\\n', u'20\\n', u'8.20\\n', u'10\\n', u'16.66\\n', u'24\\n', u'1.99\\n', u'2\\n', u'20\\n', u'15.46\\n', u'\\n', u'11\\n', u'5\\n', u'6\\n', u'3\\n', u'72\\n', u'20\\n', u'11\\n', u'25\\n', u'3\\n', u'20\\n', u'\\n', u'Dataset\\n', u'\\n', u'm1\\n', u'm2\\n', u'm3\\n', u'm7\\n', u'm8\\n', u'm9\\n', u'\\n', u'm10a\\n', u'm10c\\n', u'm11\\n', u'm12\\n', u'\\n', u'd D IDMLE\\n', u'-10.07\\n', u'10\\n', u'2.43\\n', u'3\\n', u'-30.83\\n', u'4\\n', u'-8.67\\n', u'2\\n', u'44.17\\n', u'12\\n', u'20\\n', u'-21.77\\n', u'21.46\\n', u'10\\n', u'7.98\\n', u'24\\n', u'32.16\\n', u'2\\n', u'20\\n', u'-22.83\\n', u'\\n', u'11\\n', u'5\\n', u'6\\n', u'3\\n', u'72\\n', u'20\\n', u'11\\n', u'25\\n', u'3\\n', u'20\\n', u'\\n', u'IDMoM IDPWM IDGED\\n', u'7.91\\n', u'2.65\\n', u'3.55\\n', u'3.22\\n', u'11.97\\n', u'11.96\\n', u'7.28\\n', u'13.52\\n', u'3.05\\n', u'11.85\\n', u'\\n', u'8.14\\n', u'2.68\\n', u'3.59\\n', u'3.04\\n', u'12.51\\n', u'12.50\\n', u'7.47\\n', u'14.22\\n', u'2.94\\n', u'12.43\\n', u'\\n', u'8.08\\n', u'2.67\\n', u'3.56\\n', u'2.80\\n', u'12.33\\n', u'12.40\\n', u'7.40\\n', u'14.07\\n', u'2.74\\n', u'12.46\\n', u'\\n', u'IDRVE MiNDml1 MiNDmli\\n', u'8.95\\n', u'3.00\\n', u'4.00\\n', u'2.00\\n', u'13.00\\n', u'13.50\\n', u'8.00\\n', u'15.35\\n', u'2.00\\n', u'14.00\\n', u'\\n', u'7.79\\n', u'2.60\\n', u'3.49\\n', u'3.12\\n', u'11.79\\n', u'11.79\\n', u'7.16\\n', u'13.32\\n', u'2.97\\n', u'11.67\\n', u'\\n', u'9.50\\n', u'2.94\\n', u'3.88\\n', u'2.00\\n', u'13.49\\n', u'15.03\\n', u'8.50\\n', u'17.69\\n', u'2.01\\n', u'16.79\\n', u'\\n', u'CD Hein Takens\\n', u'9.44\\n', u'9.24\\n', u'2.87\\n', u'2.91\\n', u'3.66\\n', u'3.63\\n', u'1.95\\n', u'1.95\\n', u'11.85\\n', u'11.00\\n', u'14.68\\n', u'12.84\\n', u'8.45\\n', u'8.42\\n', u'16.82\\n', u'16.90\\n', u'2.00\\n', u'1.99\\n', u'13.69\\n', u'13.64\\n', u'\\n', u'5.35\\n', u'2.75\\n', u'3.70\\n', u'1.90\\n', u'3.60\\n', u'4.30\\n', u'8.15\\n', u'6.05\\n', u'2.70\\n', u'3.70\\n', u'\\n', u'kNNG1\\n', u'7.96\\n', u'2.53\\n', u'4.00\\n', u'3.10\\n', u'14.28\\n', u'19.68\\n', u'10.69\\n', u'17.31\\n', u'2.83\\n', u'11.71\\n', u'\\n', u'kNNG2\\n', u'7.02\\n', u'2.52\\n', u'2.88\\n', u'2.86\\n', u'12.56\\n', u'10.84\\n', u'6.65\\n', u'29.77\\n', u'2.59\\n', u'5.13\\n', u'\\n', u'Table 3: ID estimates for 1000 points.\\n', u'\\n', u'IDMoM IDPWM IDGED\\n', u'9.06\\n', u'2.90\\n', u'3.92\\n', u'1.99\\n', u'13.91\\n', u'14.41\\n', u'8.21\\n', u'16.54\\n', u'2.04\\n', u'15.23\\n', u'\\n', u'9.32\\n', u'2.94\\n', u'3.97\\n', u'2.02\\n', u'14.50\\n', u'15.08\\n', u'8.43\\n', u'17.45\\n', u'2.06\\n', u'16.03\\n', u'\\n', u'9.10\\n', u'2.90\\n', u'3.90\\n', u'1.99\\n', u'13.86\\n', u'14.56\\n', u'8.25\\n', u'16.77\\n', u'2.03\\n', u'15.54\\n', u'\\n', u'IDRVE MiNDml1 MiNDmli\\n', u'9.00\\n', u'3.00\\n', u'4.00\\n', u'2.00\\n', u'14.00\\n', u'15.00\\n', u'8.00\\n', u'17.00\\n', u'2.00\\n', u'16.00\\n', u'\\n', u'8.92\\n', u'2.85\\n', u'3.85\\n', u'1.95\\n', u'13.69\\n', u'14.18\\n', u'8.08\\n', u'16.28\\n', u'2.00\\n', u'15.00\\n', u'\\n', u'9.61\\n', u'2.96\\n', u'3.92\\n', u'1.99\\n', u'12.91\\n', u'15.95\\n', u'8.86\\n', u'18.50\\n', u'1.99\\n', u'17.74\\n', u'\\n', u'CD Hein Takens\\n', u'9.59\\n', u'9.56\\n', u'2.98\\n', u'3.08\\n', u'3.76\\n', u'3.75\\n', u'1.97\\n', u'1.98\\n', u'11.92\\n', u'11.95\\n', u'15.74\\n', u'15.69\\n', u'8.92\\n', u'8.87\\n', u'18.13\\n', u'18.08\\n', u'2.00\\n', u'1.99\\n', u'15.04\\n', u'15.00\\n', u'\\n', u'8.95\\n', u'3.55\\n', u'3.90\\n', u'1.95\\n', u'8.10\\n', u'2.65\\n', u'9.10\\n', u'10.90\\n', u'2.00\\n', u'3.70\\n', u'\\n', u'kNNG1\\n', u'9.20\\n', u'2.77\\n', u'3.94\\n', u'1.83\\n', u'14.08\\n', u'10.11\\n', u'6.55\\n', u'15.00\\n', u'1.84\\n', u'37.63\\n', u'\\n', u'kNNG2\\n', u'9.87\\n', u'2.44\\n', u'3.94\\n', u'1.83\\n', u'14.08\\n', u'10.11\\n', u'6.55\\n', u'15.00\\n', u'1.84\\n', u'37.63\\n', u'\\n', u'Table 4: Dimensionality estimates for 10000 points.\\n', u'\\n', u'IDMoM IDPWM IDGED\\n', u'-11.81\\n', u'-3.10\\n', u'-33.16\\n', u'-15.58\\n', u'35.44\\n', u'-24.01\\n', u'20.83\\n', u'6.83\\n', u'28.43\\n', u'-23.90\\n', u'\\n', u'-10.55\\n', u'-1.03\\n', u'-32.05\\n', u'-14.57\\n', u'43.00\\n', u'-22.25\\n', u'21.45\\n', u'7.87\\n', u'29.56\\n', u'-23.10\\n', u'\\n', u'-11.80\\n', u'-3.06\\n', u'-33.25\\n', u'-16.34\\n', u'39.79\\n', u'-24.34\\n', u'21.59\\n', u'7.45\\n', u'28.64\\n', u'-24.52\\n', u'\\n', u'IDRVE MiNDml1 MiNDmli\\n', u'-2.78\\n', u'-11.88\\n', u'0.00\\n', u'-3.51\\n', u'-25.00\\n', u'-33.25\\n', u'0.00\\n', u'-15.90\\n', u'60.71\\n', u'35.65\\n', u'-23.98\\n', u'-17.00\\n', u'25.00\\n', u'20.92\\n', u'11.76\\n', u'6.88\\n', u'0.00\\n', u'28.50\\n', u'-23.93\\n', u'-19.69\\n', u'\\n', u'-1.56\\n', u'36.49\\n', u'-23.47\\n', u'34.17\\n', u'115.49\\n', u'-9.97\\n', u'22.12\\n', u'14.76\\n', u'47.74\\n', u'-16.52\\n', u'\\n', u'CD\\n', u'-11.82\\n', u'12.01\\n', u'-22.13\\n', u'14.21\\n', u'86.53\\n', u'-22.12\\n', u'9.02\\n', u'-2.99\\n', u'41.21\\n', u'-16.22\\n', u'\\n', u'Hein Takens\\n', u'-12.10\\n', u'23.15\\n', u'-22.34\\n', u'9.60\\n', u'85.99\\n', u'-22.62\\n', u'8.07\\n', u'-3.75\\n', u'40.50\\n', u'-16.27\\n', u'\\n', u'-38.55\\n', u'-18.31\\n', u'-41.03\\n', u'10.26\\n', u'25.93\\n', u'167.92\\n', u'-64.29\\n', u'-74.31\\n', u'10.00\\n', u'13.51\\n', u'\\n', u'kNNG1\\n', u'-62.17\\n', u'16.97\\n', u'-35.79\\n', u'-44.81\\n', u'93.68\\n', u'157.17\\n', u'338.17\\n', u'-177.73\\n', u'195.65\\n', u'-84.45\\n', u'\\n', u'kNNG2\\n', u'-64.74\\n', u'32.79\\n', u'-35.79\\n', u'-44.81\\n', u'93.68\\n', u'157.17\\n', u'338.17\\n', u'-177.73\\n', u'195.65\\n', u'-84.45\\n', u'\\n', u'Table 5: Deviation of dimensionality estimates for 10000 manifold points with added noise (p=0.01).\\n', u'\\n', u'Dataset\\n', u'\\n', u'm1\\n', u'm2\\n', u'm3\\n', u'm7\\n', u'm8\\n', u'm9\\n', u'\\n', u'm10a\\n', u'm10c\\n', u'm11\\n', u'm12\\n', u'\\n', u'd D IDMLE\\n', u'-10.18\\n', u'10\\n', u'3\\n', u'2.43\\n', u'-30.57\\n', u'4\\n', u'-8.67\\n', u'2\\n', u'44.24\\n', u'12\\n', u'-21.77\\n', u'20\\n', u'21.46\\n', u'10\\n', u'24\\n', u'8.04\\n', u'32.16\\n', u'2\\n', u'20\\n', u'-22.83\\n', u'\\n', u'11\\n', u'5\\n', u'6\\n', u'3\\n', u'72\\n', u'20\\n', u'11\\n', u'25\\n', u'3\\n', u'20\\n', u'\\n', u'IDMoM IDPWM IDGED\\n', u'-11.92\\n', u'-3.45\\n', u'-33.16\\n', u'-15.58\\n', u'35.59\\n', u'-24.01\\n', u'20.83\\n', u'6.83\\n', u'28.43\\n', u'-23.90\\n', u'\\n', u'-10.66\\n', u'-1.03\\n', u'-32.05\\n', u'-14.57\\n', u'43.07\\n', u'-22.25\\n', u'21.45\\n', u'7.87\\n', u'29.56\\n', u'-23.10\\n', u'\\n', u'-11.91\\n', u'-3.06\\n', u'-33.25\\n', u'-16.83\\n', u'39.86\\n', u'-24.27\\n', u'21.59\\n', u'7.51\\n', u'28.64\\n', u'-24.52\\n', u'\\n', u'IDRVE MiNDml1 MiNDmli\\n', u'-2.78\\n', u'-12.00\\n', u'-3.51\\n', u'0.00\\n', u'-25.00\\n', u'-33.25\\n', u'0.00\\n', u'-15.90\\n', u'60.71\\n', u'35.72\\n', u'-17.33\\n', u'-23.91\\n', u'25.00\\n', u'20.79\\n', u'6.94\\n', u'11.76\\n', u'0.00\\n', u'28.50\\n', u'-23.93\\n', u'-19.37\\n', u'\\n', u'-1.87\\n', u'37.16\\n', u'-23.47\\n', u'34.17\\n', u'116.42\\n', u'-10.22\\n', u'21.78\\n', u'14.49\\n', u'46.73\\n', u'-16.18\\n', u'\\n', u'CD\\n', u'-17.05\\n', u'18.83\\n', u'-26.40\\n', u'15.74\\n', u'86.69\\n', u'-22.31\\n', u'3.04\\n', u'-7.85\\n', u'40.20\\n', u'-16.16\\n', u'\\n', u'Hein Takens\\n', u'-12.20\\n', u'22.82\\n', u'-22.07\\n', u'11.62\\n', u'86.16\\n', u'-22.74\\n', u'7.96\\n', u'-3.53\\n', u'39.00\\n', u'-16.33\\n', u'\\n', u'-63.69\\n', u'-9.86\\n', u'-42.31\\n', u'7.69\\n', u'46.30\\n', u'132.08\\n', u'-48.35\\n', u'-59.17\\n', u'37.50\\n', u'33.78\\n', u'\\n', u'kNNG1\\n', u'-341.09\\n', u'-7.94\\n', u'-31.47\\n', u'-38.25\\n', u'9.52\\n', u'15.73\\n', u'25.65\\n', u'-18.80\\n', u'255.43\\n', u'-174.25\\n', u'\\n', u'kNNG2\\n', u'-324.72\\n', u'4.51\\n', u'-31.47\\n', u'-38.25\\n', u'9.52\\n', u'15.73\\n', u'25.65\\n', u'-18.80\\n', u'255.43\\n', u'-174.25\\n', u'\\n', u'Table 6: Deviation of dimensionality estimates for 10000 manifold points with added noise (p=0.04).\\n', u'\\n', u'Dataset\\n', u'\\n', u'm1\\n', u'm2\\n', u'm3\\n', u'm7\\n', u'm8\\n', u'm9\\n', u'\\n', u'm10a\\n', u'm10c\\n', u'm11\\n', u'm12\\n', u'\\n', u'd D IDMLE\\n', u'-10.18\\n', u'10\\n', u'2.43\\n', u'3\\n', u'-30.83\\n', u'4\\n', u'2\\n', u'-8.67\\n', u'44.17\\n', u'12\\n', u'-21.77\\n', u'20\\n', u'21.46\\n', u'10\\n', u'8.04\\n', u'24\\n', u'31.66\\n', u'2\\n', u'20\\n', u'-22.83\\n', u'\\n', u'11\\n', u'5\\n', u'6\\n', u'3\\n', u'72\\n', u'20\\n', u'11\\n', u'25\\n', u'3\\n', u'20\\n', u'\\n', u'IDMoM IDPWM IDGED\\n', u'-11.81\\n', u'-3.10\\n', u'-33.42\\n', u'-15.58\\n', u'35.44\\n', u'-24.01\\n', u'20.83\\n', u'6.89\\n', u'28.43\\n', u'-23.90\\n', u'\\n', u'-10.66\\n', u'-1.03\\n', u'-32.05\\n', u'-14.57\\n', u'43.00\\n', u'-22.25\\n', u'21.45\\n', u'7.93\\n', u'29.06\\n', u'-23.17\\n', u'\\n', u'-11.80\\n', u'-3.06\\n', u'-33.25\\n', u'-16.34\\n', u'39.79\\n', u'-24.27\\n', u'21.59\\n', u'7.51\\n', u'28.64\\n', u'-24.52\\n', u'\\n', u'IDRVE MiNDml1 MiNDmli\\n', u'-2.78\\n', u'-11.88\\n', u'0.00\\n', u'-3.51\\n', u'-25.00\\n', u'-33.25\\n', u'-15.90\\n', u'0.00\\n', u'59.64\\n', u'35.57\\n', u'-17.00\\n', u'-23.98\\n', u'25.00\\n', u'20.79\\n', u'11.76\\n', u'6.88\\n', u'0.00\\n', u'28.50\\n', u'-23.93\\n', u'-19.69\\n', u'\\n', u'-1.77\\n', u'37.16\\n', u'-22.96\\n', u'34.17\\n', u'115.72\\n', u'-9.66\\n', u'21.22\\n', u'14.43\\n', u'46.73\\n', u'-16.52\\n', u'\\n', u'CD\\n', u'-16.95\\n', u'19.48\\n', u'-31.20\\n', u'19.29\\n', u'85.94\\n', u'-22.12\\n', u'9.02\\n', u'-2.71\\n', u'27.64\\n', u'-16.16\\n', u'\\n', u'Hein Takens\\n', u'-12.10\\n', u'23.49\\n', u'-22.34\\n', u'15.15\\n', u'85.65\\n', u'-22.68\\n', u'8.18\\n', u'-3.42\\n', u'39.00\\n', u'-16.27\\n', u'\\n', u'-35.75\\n', u'-18.31\\n', u'-35.90\\n', u'18.46\\n', u'-11.11\\n', u'100.00\\n', u'-39.56\\n', u'-30.73\\n', u'10.00\\n', u'6.76\\n', u'\\n', u'kNNG1\\n', u'-37.61\\n', u'-24.19\\n', u'-35.03\\n', u'4.37\\n', u'-11.93\\n', u'-907.22\\n', u'34.35\\n', u'-610.20\\n', u'3811.41\\n', u'-835.80\\n', u'\\n', u'kNNG2\\n', u'-41.84\\n', u'-13.93\\n', u'-35.03\\n', u'4.37\\n', u'-11.93\\n', u'-907.22\\n', u'34.35\\n', u'-610.20\\n', u'3811.41\\n', u'-835.80\\n', u'\\n', u'Table 7: Deviation of dimensionality estimates for 10000 manifold points with added noise (p=0.16).\\n', u'\\n', u'PCA\\n', u'11.00\\n', u'3.00\\n', u'5.30\\n', u'3.00\\n', u'24.00\\n', u'20.00\\n', u'10.00\\n', u'24.00\\n', u'3.00\\n', u'20.00\\n', u'\\n', u'PCA\\n', u'11.00\\n', u'3.00\\n', u'5.05\\n', u'3.00\\n', u'24.00\\n', u'20.00\\n', u'10.00\\n', u'24.00\\n', u'3.00\\n', u'20.00\\n', u'\\n', u'PCA\\n', u'-22.73\\n', u'-33.33\\n', u'-60.40\\n', u'-66.67\\n', u'95.21\\n', u'-31.75\\n', u'10.00\\n', u'4.17\\n', u'-35.00\\n', u'-26.00\\n', u'\\n', u'PCA\\n', u'-23.18\\n', u'-33.33\\n', u'-60.40\\n', u'-66.67\\n', u'95.21\\n', u'-31.75\\n', u'10.00\\n', u'4.17\\n', u'-35.00\\n', u'-26.00\\n', u'\\n', u'PCA\\n', u'-22.73\\n', u'-33.33\\n', u'-60.40\\n', u'-66.67\\n', u'95.21\\n', u'-31.75\\n', u'10.00\\n', u'4.17\\n', u'-35.00\\n', u'-26.00\\n', u'\\n', u'36\\x0c(a) Illustration of the distribution of k-nearest neighbor distances\\n', u'for k  [1, 1000] with respect to 7 points of interest.\\n', u'\\n', u'(b) Distribution of LID estimates based on k-nearest neighbor sets\\n', u'for k  [10, 1000] with respect to 7 points of interest.\\n', u'\\n', u'Figure 4: Distribution of IDMLE estimates and distance values across neighborhoods around the points of interest.\\n', u'\\n', u'signicantly aected by noise: their estimates diverge very\\n', u'quickly as noise is being introduced. On the other hand,\\n', u'the local estimators display more resistance to noise in the\\n', u'case of non-linear manifolds; among the local estimators, our\\n', u'EVT estimators tend to outperform the MiND variants.\\n', u'\\n', u'We note that the additive noise considered in this experi-\\n', u'ment does not drastically impact the intrinsic dimensional-\\n', u'ity in the case of hypercubes. (sets m10a, m10b and m10c).\\n', u'That explains why PCA appears resistant to noise for the\\n', u'sets m10a, m10b and m10c.\\n', u'\\n', u'6.3 Real Data\\n', u'\\n', u'Based on our experiments on synthetic data, we expect\\n', u'the performance of our proposed estimators to be largely\\n', u'in agreement with one another. Accordingly, for clarity of\\n', u'presentation, for the experimentation on real data, we show\\n', u'results only for the MLE estimator.\\n', u'\\n', u'Fig. 2 illustrates the distribution of LID estimates across\\n', u'reference points for all three data sets. The scatter plot\\n', u'for the ANN SIFT1B data set furthermore contains several\\n', u'points of interest annotated with their LID values, corre-\\n', u'sponding to objects of interest which we discuss later. First,\\n', u'we clearly observe dierences in the location of the distri-\\n', u'bution of LID values among the three data sets; for exam-\\n', u'ple, the mean value and standard deviation of the LID esti-\\n', u'mates for ALOI are considerably lower than those obtained\\n', u'for ANN SIFT1B. More specically, we observe mean val-\\n', u'ues of ALOI  2.2, MNIST  6.3, and ANN SIFT1B  12.3,\\n', u'with the corresponding standard deviations of ALOI  1.9,\\n', u'MNIST  2.7, and ANN SIFT1B  3.0.\\n', u'It should be noted\\n', u'that the measured ID within the neighborhoods that were\\n', u'tested is far smaller than the dimension of the full feature\\n', u'spaces. By plotting the same data as histograms in Fig. 3,\\n', u'we can furthermore see that the individual distributions of\\n', u'LID values dier in kurtosis and skewness as well.\\n', u'\\n', u'The most striking dierence between the individual points\\n', u'of interest are the distances to their respective k-nearest\\n', u'neighbors. Fig. 4a displays for each point of interest the\\n', u'specic distribution of neighbor-distances for all values of k\\n', u'between 1 and 1000. Interestingly, the ID measured at the\\n', u'points of interest appears to be associated with other prop-\\n', u'erties of the respective objects. For example, distribution of\\n', u'neighbor-distances for objects with high corresponding di-\\n', u'mensionality (D, E and F ) indicate that these points are\\n', u'\\n', u'in some sense outliers. On the other hand, despite their\\n', u'distance distributions being quite dissimilar, the LID values\\n', u'measured at A, B, and C are nearly identical.\\n', u'\\n', u'7. CONCLUSION\\n', u'\\n', u'Our experimental results on synthetic data show that the\\n', u'estimation of LID stabilizes for sample sizes on the order\\n', u'of 100. However, for Theorem 2 to be applicable, one must\\n', u'set a suciently small threshold on the lower tail of the\\n', u'distribution, which may severely limit the number of data\\n', u'objects falling within the tail. Although there is a conict\\n', u'between the accuracy of the estimator and the validity of\\n', u'the model, this conict is resolved as the size of the dataset\\n', u'scales upward; it is in precisely such situations where the\\n', u'applications of ID have the most impact.\\n', u'\\n', u'Estimates of local ID constitute a measure of the complex-\\n', u'ity of data. Along with other indicators such as contrast [33],\\n', u'LID could give researchers and practitioners more insight\\n', u'into the nature of their data, and therefore help them im-\\n', u'prove the eciency and ecacy of their applications. As\\n', u'a tool for guiding learning processes, the proposed estima-\\n', u'tors could serve in many ways. Data collected during the\\n', u'retrieval processes could be automatically ltered out as\\n', u'noise, whenever they are associated with an unusually high\\n', u'ID value. In this way, the quality of query results may be\\n', u'enhanced as well.\\n', u'\\n', u'The performance of content-based retrieval systems is usu-\\n', u'ally assessed in terms of the precision and recall of queries on\\n', u'a ground truth data set. However, in high-dimensional set-\\n', u'tings it is often the case that some points are much less likely\\n', u'to appear in a query result than others. Unlike LID, conven-\\n', u'tional measures of complexity or performance do not account\\n', u'for this diculty. LID has therefore the potential to aid in\\n', u'the design of fair benchmarks that truly reect the power\\n', u'of retrieval systems, according to a sound, mathematically-\\n', u'grounded procedure.\\n', u'\\n', u'8. ACKNOWLEDGMENTS\\n', u'\\n', u'L. Amsaleg and T. Furon supported by French project\\n', u'Secular ANR-12-CORD-0014. O. Chelly, M. E. Houle and\\n', u'K. Kawarabayashi supported by JST ERATO Kawarabaya-\\n', u'shi Project. M. E. Houle supported by JSPS Kakenhi Kiban\\n', u'(A) Research Grant 25240036.\\n', u'\\n', u' 0 20000 40000 60000 80000 0 200 400 600 800 1000Neighbor DistanceNeighbor RankDistribution of Neighbor DistancesABCDEFG 0 5 10 15 20 25 30 35 200 400 600 800 1000Estimated IDNeighborhood SizeDistribution of Neighborhood IDABCDEFG37\\x0c9. REFERENCES\\n', u'\\n', u'[1] A. A. Balkema and L. de Haan. Residual Life Time at\\n', u'Great Age. The Annals of Probability, 2:792804, 1974.\\n', u'\\n', u'[2] N. Bingham, C. Goldie, and J. Teugels. Regular\\n', u'\\n', u'variation, volume 27. Cambridge University Press,\\n', u'1989.\\n', u'\\n', u'[3] N. Boujemaa, J. Fauqueur, M. Ferecatu, F. Fleuret,\\n', u'\\n', u'V. Gouet, B. LeSaux, and H. Sahbi. IKONA:\\n', u'Interactive Specic and Generic Image Retrieval. In\\n', u'MMCBIR, 2001.\\n', u'\\n', u'[4] C. Bouveyron, G. Celeux, and S. Girard. Intrinsic\\n', u'\\n', u'dimension estimation by maximum likelihood in\\n', u'isotropic probabilistic pca. Pattern Recogn. Lett., 32.\\n', u'\\n', u'[5] J. Bruske and G. Sommer. Intrinsic dimensionality\\n', u'\\n', u'estimation with optimally topology preserving maps.\\n', u'PAMI, 20.\\n', u'\\n', u'[6] F. Camastra and A. Vinciarelli. Estimating the\\n', u'intrinsic dimension of data with a fractal-based\\n', u'method. PAMI, 24.\\n', u'\\n', u'[7] S. Coles. An Introduction to Statistical Modeling of\\n', u'\\n', u'Extreme Values. 2001.\\n', u'\\n', u'[8] J. Costa and A. Hero. Entropic graphs for manifold\\n', u'\\n', u'learning. In Asilomar Conf. on Signals, Sys. and\\n', u'Comput.., pages 316320 Vol.1, 2003.\\n', u'\\n', u'[9] T. de Vries, S. Chawla, and M. E. Houle. Finding local\\n', u'\\n', u'anomalies in very high dimensional space. In ICDM,\\n', u'pages 128137, 2010.\\n', u'\\n', u'[10] R. A. Fisher and L. H. C. Tippett. Limiting Forms of\\n', u'the Frequency Distribution of the Largest or Smallest\\n', u'Member of a Sample. Math. Proc. Cambridge Phil.\\n', u'Soc., 24:180190, 1928.\\n', u'\\n', u'[11] M. I. Fraga Alves, L. de Haan, and T. Lin. Estimation\\n', u'of the parameter controlling the speed of convergence\\n', u'in extreme value theory. Math. Methods of Stat., 12.\\n', u'[12] M. I. Fraga Alves, M. I. Gomes, and L. de Haan. A\\n', u'\\n', u'new class of semiparametric estimators of the second\\n', u'order parameter. Portugalia Mathematica, 60:193213,\\n', u'2003.\\n', u'\\n', u'[13] B. V. Gnedenko. Sur la Distribution Limite du Terme\\n', u'\\n', u'Maximum dune Serie Aleatoire. Ann. Math.,\\n', u'44:423453, 1943.\\n', u'\\n', u'[14] A. Gupta, R. Krauthgamer, and J. R. Lee. Bounded\\n', u'\\n', u'Geometries, Fractals, and Low-Distortion\\n', u'Embeddings. In FOCS, pages 534543, 2003.\\n', u'\\n', u'[15] M. Hein and J.-Y. Audibert. Intrinsic dimensionality\\n', u'\\n', u'estimation of submanifolds in r d. In ICML, pages\\n', u'289296, 2005.\\n', u'\\n', u'[16] B. M. Hill. A simple general approach to inference\\n', u'\\n', u'about the tail of a distribution. Ann. Stat.,\\n', u'3(5):11631174, 1975.\\n', u'\\n', u'[17] M. E. Houle. Dimensionality, Discriminability, Density\\n', u'& Distance Distributions. In ICDMW, pages 468473,\\n', u'2013.\\n', u'\\n', u'[18] M. E. Houle. Inlierness, Outlierness, Hubness and\\n', u'\\n', u'Discriminability: an Extreme-Value-Theoretic\\n', u'Foundation. Technical Report 2015-002E, NII, 2015.\\n', u'[19] M. E. Houle, H. Kashima, and M. Nett. Generalized\\n', u'\\n', u'Expansion Dimension. In ICDMW, pages 587594,\\n', u'2012.\\n', u'\\n', u'[20] M. E. Houle, X. Ma, M. Nett, and V. Oria.\\n', u'\\n', u'Dimensional Testing for Multi-Step Similarity Search.\\n', u'In ICDM, pages 299308, 2012.\\n', u'\\n', u'[21] M. E. Houle, X. Ma, V. Oria, and J. Sun. Ecient\\n', u'\\n', u'algorithms for similarity search in axis-aligned\\n', u'subspaces. In SISAP, pages 112, 2014.\\n', u'\\n', u'[22] M. E. Houle and M. Nett. Rank-based similarity\\n', u'\\n', u'search: Reducing the dimensional dependence. PAMI,\\n', u'37(1):136150, 2015.\\n', u'\\n', u'[23] H. Jegou, R. Tavenard, M. Douze, and L. Amsaleg.\\n', u'\\n', u'Searching in One Billion Vectors: Re-rank with Source\\n', u'Coding. In ICASSP, pages 861864, 2011.\\n', u'\\n', u'[24] I. Jollie. Principal Component Analysis. 1986.\\n', u'[25] D. R. Karger and M. Ruhl. Finding Nearest Neighbors\\n', u'\\n', u'in Growth-Restricted Metrics. In STOC, pages\\n', u'741750, 2002.\\n', u'\\n', u'[26] J. Karhunen and J. Joutsensalo. Representation and\\n', u'\\n', u'separation of signals using nonlinear PCA type\\n', u'learning. Neural Networks, 7(1):113127, 1994.\\n', u'\\n', u'[27] Y. LeCun, L. Bottou, Y. Bengio, and P. Haner.\\n', u'\\n', u'Gradient-based Learning applied to Document\\n', u'Recognition. Proceedings of the IEEE,\\n', u'86(11):22782324, 1998.\\n', u'\\n', u'[28] J. Pickands, III. Statistical Inference Using Extreme\\n', u'\\n', u'Order Statistics. Ann. Stat., 3:119131, 1975.\\n', u'[29] C. R. Rao. Linear statistical inference and its\\n', u'\\n', u'applications. 1973.\\n', u'\\n', u'[30] S. T. Roweis and L. K. Saul. Nonlinear Dimensionality\\n', u'\\n', u'Reduction by Locally Linear Embedding. Science,\\n', u'290(5500):23232326, 2000.\\n', u'\\n', u'[31] A. Rozza, G. Lombardi, C. Ceruti, E. Casiraghi, and\\n', u'\\n', u'P. Campadelli. Novel high intrinsic dimensionality\\n', u'estimators. Machine Learning Journal, 89(1-2):3765,\\n', u'2012.\\n', u'\\n', u'[32] B. Scholkopf, A. J. Smola, and K.-R. Muller.\\n', u'\\n', u'Nonlinear Component Analysis as a Kernel Eigenvalue\\n', u'Problem. Neural Computation, 10(5):12991319, 1998.\\n', u'\\n', u'[33] U. Shaft and R. Ramakrishnan. Theory of nearest\\n', u'\\n', u'neighbors indexability. ACM Trans. Database Syst.,\\n', u'31(3):814838, 2006.\\n', u'\\n', u'[34] F. Takens. On the numerical determination of the\\n', u'\\n', u'dimension of an attractor. 1985.\\n', u'\\n', u'[35] J. Tenenbaum, V. D. Silva, and J. Langford. A global\\n', u'\\n', u'geometric framework for non linear dimensionality\\n', u'reduction. Science, 290(5500):23192323, 2000.\\n', u'\\n', u'[36] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A\\n', u'\\n', u'global geometric framework for nonlinear\\n', u'dimensionality reduction. Science,\\n', u'290(5500):23192323, 2000.\\n', u'\\n', u'[37] J. Venna and S. Kaski. Local Multidimensional\\n', u'\\n', u'Scaling. Neural Networks, 19(67):889899, 2006.\\n', u'\\n', u'[38] P. Verveer and R. Duin. An evaluation of intrinsic\\n', u'\\n', u'dimensionality estimators. PAMI, 17(1):8186, 1995.\\n', u'\\n', u'[39] J. von Brunken, M. E. Houle, and A. Zimek. Intrinsic\\n', u'\\n', u'Dimensional Outlier Detection in High-Dimensional\\n', u'Data. Technical Report 2015-003E, NII, 2015.\\n', u'\\n', u'38\\x0c']\n"
     ]
    }
   ],
   "source": [
    "print re.findall('.*[^ ]',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Estimating Local Intrinsic Dimensionality\\n\\nLaurent Amsaleg\\nEquipe LINKMEDIA,\\n\\nCNRS/IRISA Rennes, France\\n\\nCampus Universitaire de\\n\\nBeaulieu\\n\\n35042 Rennes Cedex, France\\nlaurent.amsaleg@irisa.fr\\n\\nStphane Girard\\nEquipe MISTIS, INRIA\\n\\nGrenoble, France\\n\\nInovalle, 655, Montbonnot\\n38334 Saint-Ismier Cedex,\\nstephane.girard@inria.fr\\n\\nFrance\\n\\nTeddy Furon\\n\\nEquipe LINKMEDIA,\\n\\nINRIA/IRISA Rennes, France\\n\\nCampus Universitaire de\\n\\nBeaulieu\\n\\n35042 Rennes Cedex, France\\n\\nteddy.furon@inria.fr\\n\\nKen-ichi Kawarabayashi\\n\\nNational Institute of\\nInformatics, Japan\\n2-1-2 Hitotsubashi,\\n\\nChiyoda-ku\\n\\nTokyo 101-8430, Japan\\nk_keniti@nii.ac.jp\\n\\nOussama Chelly\\nNational Institute of\\nInformatics, Japan\\n2-1-2 Hitotsubashi,\\n\\nChiyoda-ku\\n\\nTokyo 101-8430, Japan\\n\\nchelly@nii.ac.jp\\nMichael E. Houle\\nNational Institute of\\nInformatics, Japan\\n2-1-2 Hitotsubashi,\\n\\nChiyoda-ku\\n\\nTokyo 101-8430, Japan\\n\\nmeh@nii.ac.jp\\nMichael Nett\\nGoogle, Japan\\n\\n6-10-1 Roppongi, Minato-ku\\n\\nTokyo 106-6126, Japan\\nmnett@google.com\\n\\nABSTRACT\\nThis paper is concerned with the estimation of a local mea-\\nsure of intrinsic dimensionality (ID) recently proposed by\\nHoule. The local model can be regarded as an extension of\\nKarger and Ruhls expansion dimension to a statistical set-\\nting in which the distribution of distances to a query point\\nis modeled in terms of a continuous random variable. This\\nform of intrinsic dimensionality can be particularly useful in\\nsearch, classication, outlier detection, and other contexts in\\nmachine learning, databases, and data mining, as it has been\\nshown to be equivalent to a measure of the discriminative\\npower of similarity functions. Several estimators of local ID\\nare proposed and analyzed based on extreme value theory,\\nusing maximum likelihood estimation (MLE), the method\\nof moments (MoM), probability weighted moments (PWM),\\nand regularly varying functions (RV). An experimental eval-\\nuation is also provided, using both real and articial data.\\n\\nCategories and Subject Descriptors\\nG.3 [Mathematics of Computing]: Prob'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['itting at starbucks.  I wonder what we all are doing in this god-forsaken place.  Can you under']\n"
     ]
    }
   ],
   "source": [
    "print re.findall('^S(.*)s', \"Sitting at starbucks.  I wonder what we all are doing in this god-forsaken place.  Can you understand what I'm doing?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?<=Abstract)(.+)(?=Categories and Subject Descriptors)\n",
      "(?<=Abstract)(.+)(?=Categories & Subject Descriptors)\n",
      "(?<=Abstract)(.+)(?=CATEGORIES AND SUBJECT DESCRIPTORS)\n",
      "(?<=Abstract)(.+)(?=CATEGORIES & SUBJECT DESCRIPTORS)\n",
      "(?<=Abstract)(.+)(?=categories and subject descriptors)\n",
      "(?<=Abstract)(.+)(?=categories & subject descriptors)\n",
      "(?<=Abstract)(.+)(?=permission to make)\n",
      "(?<=Abstract)(.+)(?=Keywords)\n",
      "(?<=Abstract)(.+)(?=KEYWORDS)\n",
      "(?<=Abstract)(.+)(?=keywords)\n",
      "(?<=Abstract)(.+)(?=Introduction  1.)\n",
      "(?<=Abstract)(.+)(?=INTRODUCTION  1.)\n",
      "(?<=Abstract)(.+)(?=introduction  1.)\n",
      "(?<=Abstract)(.+)(?=Introduction)\n",
      "(?<=Abstract)(.+)(?=INTRODUCTION)\n",
      "(?<=Abstract)(.+)(?=introduction)\n",
      "(?<=Abstract)(.+)(?=\\\\n)\n",
      "(?<=ABSTRACT)(.+)(?=Categories and Subject Descriptors)\n",
      "(?<=ABSTRACT)(.+)(?=Categories & Subject Descriptors)\n",
      "(?<=ABSTRACT)(.+)(?=CATEGORIES AND SUBJECT DESCRIPTORS)\n",
      "(?<=ABSTRACT)(.+)(?=CATEGORIES & SUBJECT DESCRIPTORS)\n",
      "(?<=ABSTRACT)(.+)(?=categories and subject descriptors)\n",
      "(?<=ABSTRACT)(.+)(?=categories & subject descriptors)\n",
      "(?<=ABSTRACT)(.+)(?=permission to make)\n",
      "(?<=ABSTRACT)(.+)(?=Keywords)\n",
      "(?<=ABSTRACT)(.+)(?=KEYWORDS)\n",
      "(?<=ABSTRACT)(.+)(?=keywords)\n",
      "(?<=ABSTRACT)(.+)(?=Introduction  1.)\n",
      "(?<=ABSTRACT)(.+)(?=INTRODUCTION  1.)\n",
      "(?<=ABSTRACT)(.+)(?=introduction  1.)\n",
      "(?<=ABSTRACT)(.+)(?=Introduction)\n",
      "(?<=ABSTRACT)(.+)(?=INTRODUCTION)\n",
      "(?<=ABSTRACT)(.+)(?=introduction)\n",
      "(?<=ABSTRACT)(.+)(?=\\\\n)\n",
      "(?<=abstract)(.+)(?=Categories and Subject Descriptors)\n",
      "(?<=abstract)(.+)(?=Categories & Subject Descriptors)\n",
      "(?<=abstract)(.+)(?=CATEGORIES AND SUBJECT DESCRIPTORS)\n",
      "(?<=abstract)(.+)(?=CATEGORIES & SUBJECT DESCRIPTORS)\n",
      "(?<=abstract)(.+)(?=categories and subject descriptors)\n",
      "(?<=abstract)(.+)(?=categories & subject descriptors)\n",
      "(?<=abstract)(.+)(?=permission to make)\n",
      "(?<=abstract)(.+)(?=Keywords)\n",
      "(?<=abstract)(.+)(?=KEYWORDS)\n",
      "(?<=abstract)(.+)(?=keywords)\n",
      "(?<=abstract)(.+)(?=Introduction  1.)\n",
      "(?<=abstract)(.+)(?=INTRODUCTION  1.)\n",
      "(?<=abstract)(.+)(?=introduction  1.)\n",
      "(?<=abstract)(.+)(?=Introduction)\n",
      "(?<=abstract)(.+)(?=INTRODUCTION)\n",
      "(?<=abstract)(.+)(?=introduction)\n",
      "(?<=abstract)(.+)(?=\\\\n)\n"
     ]
    }
   ],
   "source": [
    "section1=[\"Abstract\",\"ABSTRACT\",\"abstract\"]\n",
    "target = \"\"   \n",
    "section2=[\"Categories and Subject Descriptors\",\"Categories & Subject Descriptors\",\"Categories and Subject Descriptors\".upper(),\"Categories & Subject Descriptors\".upper(),\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"Keywords\",\"keywords\".upper(),\"keywords\",\"introduction  1.\".title(),\"introduction  1.\".upper(),\"introduction  1.\",\"introduction\".title(),\"introduction\".upper(),\"introduction\", \"\\\\\\\\n\"]\n",
    "for sect1 in section1:\n",
    "    for sect2 in section2:\n",
    "        part1= \"(?<=\"+str(sect1)+\")(.+)\"\n",
    "        part2 = \"(?=\"+str(sect2)+\")\"\n",
    "        print part1+part2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part1= \"(?<=\"+str(section1)+\")(.+)\"'(?<=references)(.+)'\n",
    "\n",
    "                    for sect in section1:\n",
    "                        try:\n",
    "                            p=re.compile(part1)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entities = nltk.chunk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(p19['top'].title())))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
