{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A ~~Quick~~ Survey and Comparison of Open Source Named Entity Extractor Tools for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity extraction is a core subtask of building knowledge from semi/unstructured text sources<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup>.  Considering recent increases in computing power and decreases in the costs of data storage, data scientists and developers can build large knowledge bases that contain millions of entities and hundreds of millions of facts about them.  These knowledge bases are key contributors to intelligence computer behavior<sup><a href=\"#fn2\" id=\"ref2\">2</a></sup>.  Therefore, named entity extraction is at the core of several popular technologies such as smart assistants ([Siri](http://www.apple.com/ios/siri/), [Google Now](https://www.google.com/landing/now/)), machine reading, and deep interpretation of natural language<sup><a href=\"#fn3\" id=\"ref3\">3</a></sup>.\n",
    "\n",
    "With a realization of how essential it is to recognize information units like names, including person, organization and location names, and numeric expressions including time, date, money\n",
    "and percent expressions, several questions come to mind.  How do you perform named entity extraction, which is formally called “[Named Entity Recognition and Classification (NERC)](https://benjamins.com/catalog/bct.19)”?  What tools are out there?  How can you evaluate their performance?  And most important, what works with Python (shamelessly exposing my bias)?  \n",
    "\n",
    "This post will survey openly available NERC tools and compare the results against hand labeled data for precision, accuracy, and recall.  The tools and basic information extraction principles in this discussion begin the process of structuring unstructured data.\n",
    "\n",
    "We will specifically learn to:\n",
    "1. follow the data science pipeline (see image below)\n",
    "2. prepare semistructured natural language data for ingest using regex\n",
    "3. create a custom corpus in [Natural Language Toolkit](http://www.nltk.org/) \n",
    "4. use a suite of openly available NERC tools to extract entities and store in json format \n",
    "5. compare the performance of NERC tools on our corpus\n",
    "\n",
    "<br>\n",
    "<a href=\"#pipe\" id=\"pipeline\"><center><h3>The Data Science Pipeline:<br>Georgetown Data Science Certificate Program</h3></center></a>\n",
    "<div class=\"image\">\n",
    "\n",
    "      <img src=\"./files/data_science_pipeline.png\" alt=\"Data Science Pipeline\" height=\"300\" width=\"450\" top:\"35\" left:\"170\" />\n",
    "      \n",
    "      \n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data: Peer Reviewed Journals and Keynote Speaker Abstracts from KDD 2014 and 2015\n",
    "\n",
    "Before delving into the pipeline, we need a good dataset.  Jason Brownlee of www.machinelearningmastery.com had some good suggestions in his [August 2015 article](http://machinelearningmastery.com/practice-machine-learning-with-small-in-memory-datasets-from-the-uci-machine-learning-repository/) on picking a dataset for machine learning exercises:  \n",
    "\n",
    "* **Real-World**: The datasets should be drawn from the real world (rather than being contrived). This will keep them interesting and introduce the challenges that come with real data.\n",
    "\n",
    "* **Small**: The datasets need to be small so that you can inspect and understand them and that you can run many models quickly to accelerate your learning cycle.\n",
    "\n",
    "* **Well-Understood**: There should be a clear idea of what the data contains, why it was collected, what the problem is that needs to be solved so that you can frame your investigation.\n",
    "\n",
    "* **Baseline**: It is also important to have an idea of what algorithms are known to perform well and the scores they achieved so that you have a useful point of comparison. This is important when you are getting started and learning because you need quick feedback as to how well you are performing (close to state-of-the-art or something is broken).\n",
    "\n",
    "* **Plentiful**: You need many datasets to choose from, both to satisfy the traits you would like to investigate and (if possible) your natural curiosity and interests. \n",
    "\n",
    "Luckily, we have a dataset that meets nearly all of these requirements.  I attended the Knowledge Discovery and Data Mining (KDD) conferences in [New York City (2014)](http://www.kdd.org/kdd2014/) and [Sydney, Australia (2015)](http://www.kdd.org/kdd2015/).  Both years, attendees received a USB with the conference proceedings.  Each repository contains over 230 peer reviewed journal articles and keynote speaker abstracts on data mining, knowledge discovery, big data, data science and their applications. The full conference proceedings can be purchased for \\$60 at the [Association for Computing Machinery's Digital Library](https://dl.acm.org/purchase.cfm?id=2783258&CFID=740512201&CFTOKEN=34489585) (includes ACM membership). This post will work with a dataset that is equivalent to the conference proceedings.  It's important to note that this dataset recreates a real word data science exercise that is instructive of big data problems.  We will take semi-structured data (PDF journal articles and abstracts in publication format), strip text from the files, and add more structure to the data that would facilitate follow on analysis. \n",
    "\n",
    "<blockquote cite=\"https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/KDDwebscrape.ipynb\">\n",
    "Interested parties looking for a free option can use the <a href=\"https://pypi.python.org/pypi/beautifulsoup4/4.4.1\">beautifulsoup</a> and <a href=\"https://pypi.python.org/pypi/requests/2.9.1\">request</a> libraries to scrape the <a href=\"http://dl.acm.org/citation.cfm?id=2785464&CFID=740512201&CFTOKEN=3448958\">ACM website for KDD 2015 conference data</a> that can be used in natural language processing pipelines.  I have some <a href=\"https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/KDDwebscrape.ipynb\">skeleton web scraping code</a> to generate lists of all abstracts, author names, and journal/keynote address titles.    \n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration: Getting the number of files, and file type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored locally in the following directory:\n",
    "```python\n",
    ">>> import os\n",
    ">>> print os.getcwd()\n",
    "/Users/linwood/Desktop/KDD_15/docs\n",
    "```\n",
    "Let's explore the number of files we have and naming conventions. We begin with the administrative tasks of loading modules, establishing paths, etc.  \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Importing what we need\n",
    "#**********************************************************************\n",
    "import os\n",
    "import time\n",
    "from os import walk\n",
    "\n",
    "#**********************************************************************\n",
    "# Administrative code to set the path for file loading\n",
    "#**********************************************************************\n",
    "\n",
    "path        = os.path.abspath(os.getcwd())\n",
    "TESTDIR     = os.path.normpath(os.path.join(os.path.expanduser(\"~\"),\"Desktop\",\"KDD_15\",\"docs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Next we iterate over the files in the directory and store those names in the empty list we created called *files*.  We time the operation, print list with the file names and also print out the length of the list (gives number of target files).<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 4.05 µs\n",
      "\n",
      "253\n",
      "\n",
      "[p1.pdf, p1005.pdf, p1015.pdf, p1025.pdf, p1035.pdf, p1045.pdf, p1055.pdf, p1065.pdf, p1075.pdf, p1085.pdf, p109.pdf, p1095.pdf, p1105.pdf, p1115.pdf, p1125.pdf, p1135.pdf, p1145.pdf, p1155.pdf, p1165.pdf, p1175.pdf, p1185.pdf, p119.pdf, p1195.pdf, p1205.pdf, p1215.pdf, p1225.pdf, p1235.pdf, p1245.pdf, p1255.pdf, p1265.pdf, p1275.pdf, p1285.pdf, p129.pdf, p1295.pdf, p1305.pdf, p1315.pdf, p1325.pdf, p1335.pdf, p1345.pdf, p1355.pdf, p1365.pdf, p1375.pdf, p1385.pdf, p139.pdf, p1395.pdf, p1405.pdf, p1415.pdf, p1425.pdf, p1435.pdf, p1445.pdf, p1455.pdf, p1465.pdf, p1475.pdf, p1485.pdf, p149.pdf, p1495.pdf, p1503.pdf, p1513.pdf, p1523.pdf, p1533.pdf, p1543.pdf, p1553.pdf, p1563.pdf, p1573.pdf, p1583.pdf, p159.pdf, p1593.pdf, p1603.pdf, p1621.pdf, p1623.pdf, p1625.pdf, p1627.pdf, p1629.pdf, p1631.pdf, p1633.pdf, p1635.pdf, p1637.pdf, p1639.pdf, p1641.pdf, p1651.pdf, p1661.pdf, p1671.pdf, p1681.pdf, p169.pdf, p1691.pdf, p1701.pdf, p1711.pdf, p1721.pdf, p1731.pdf, p1741.pdf, p1751.pdf, p1759.pdf, p1769.pdf, p1779.pdf, p1789.pdf, p179.pdf, p1799.pdf, p1809.pdf, p1819.pdf, p1829.pdf, p1839.pdf, p1849.pdf, p1859.pdf, p1869.pdf, p1879.pdf, p1889.pdf, p189.pdf, p1899.pdf, p19.pdf, p1909.pdf, p1919.pdf, p1929.pdf, p1939.pdf, p1949.pdf, p1959.pdf, p1969.pdf, p1979.pdf, p1989.pdf, p199.pdf, p1999.pdf, p2009.pdf, p2019.pdf, p2029.pdf, p2039.pdf, p2049.pdf, p2059.pdf, p2069.pdf, p2079.pdf, p2089.pdf, p209.pdf, p2099.pdf, p2109.pdf, p2119.pdf, p2127.pdf, p2137.pdf, p2147.pdf, p2157.pdf, p2167.pdf, p2177.pdf, p2187.pdf, p219.pdf, p2197.pdf, p2207.pdf, p2217.pdf, p2227.pdf, p2237.pdf, p2247.pdf, p2257.pdf, p2267.pdf, p2277.pdf, p2287.pdf, p229.pdf, p2297.pdf, p2307.pdf, p2309.pdf, p2311.pdf, p2313.pdf, p2315.pdf, p2317.pdf, p2319.pdf, p2321.pdf, p2323.pdf, p2325.pdf, p2327.pdf, p2329.pdf, p239.pdf, p249.pdf, p259.pdf, p269.pdf, p279.pdf, p289.pdf, p29.pdf, p299.pdf, p3.pdf, p309.pdf, p319.pdf, p329.pdf, p339.pdf, p349.pdf, p359.pdf, p369.pdf, p379.pdf, p387.pdf, p39.pdf, p397.pdf, p407.pdf, p417.pdf, p427.pdf, p437.pdf, p447.pdf, p457.pdf, p467.pdf, p477.pdf, p487.pdf, p49.pdf, p497.pdf, p5.pdf, p507.pdf, p517.pdf, p527.pdf, p537.pdf, p547.pdf, p557.pdf, p567.pdf, p577.pdf, p587.pdf, p59.pdf, p597.pdf, p607.pdf, p617.pdf, p627.pdf, p635.pdf, p645.pdf, p655.pdf, p665.pdf, p675.pdf, p685.pdf, p69.pdf, p695.pdf, p7.pdf, p705.pdf, p715.pdf, p725.pdf, p735.pdf, p745.pdf, p755.pdf, p765.pdf, p775.pdf, p785.pdf, p79.pdf, p805.pdf, p815.pdf, p825.pdf, p835.pdf, p845.pdf, p855.pdf, p865.pdf, p875.pdf, p885.pdf, p89.pdf, p895.pdf, p9.pdf, p905.pdf, p915.pdf, p925.pdf, p935.pdf, p945.pdf, p955.pdf, p965.pdf, p975.pdf, p985.pdf, p99.pdf, p995.pdf]\n"
     ]
    }
   ],
   "source": [
    "# Establish an empty list to append filenames as we iterate over the directory with filenames\n",
    "files = []\n",
    "\n",
    "%time\n",
    "start_time = time.time()\n",
    "\n",
    "#**********************************************************************\n",
    "# Core \"workerbee\" code for this section to iterate over directory files\n",
    "#**********************************************************************\n",
    "\n",
    "# Iterate over the directory of filenames and add to list.  Inspection shows our target filenames begin with 'p' and end with 'pdf'\n",
    "for dirName, subdirList, fileList in os.walk(TESTDIR):\n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            files.append(fileName)\n",
    "end_time = time.time()\n",
    "\n",
    "#**********************************************************************\n",
    "# Output\n",
    "#**********************************************************************\n",
    "print\n",
    "print len(files) # Print the number of files\n",
    "print \n",
    "print '[%s]' % ', '.join(map(str, files)) # print the list of filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>There are 253 total files in the directory. We examine the pdf file in its rawest form to get an idea of the format. Here is one example:<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"./files/journalscreencap.png\" alt=\"Sample of Journal Format\" height=\"700\" width=\"700\" top:\"35\" left:\"170\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br><br>We learn a few things immediately. Our data is in PDF format and it's semistructured (follows journal article format with sections like \"abstract\", \"title\").  PDFs are a wonderful human readable presentation of data. But for data analyisis, they are extremely difficult to work with.  If you have an option to get the data BEFORE it was converted to or added to PDF, go for that option.  If it's your only option, be prepared for a lot of these moments:\n",
    "\n",
    "![Pulling hair out](http://i1012.photobucket.com/albums/af243/njmike731/man-pulling-hair-out-2-773892-1.jpg)\n",
    "\n",
    "In today's exercise, we have no alternatives outside of the web scraping code linked above.  In full disclosure, that code is imperfect because we get an incomplete dataset.  The abstracts and authors are not matched to the papers and we don't pull in the references section. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion: Stripping text from PDFs and creating a custom NLTK corpus\n",
    "\n",
    "The first step in the <href id=\"pipe\"><a href=\"#pipeline\" title=\"Jump back to data science pipeline graphic.\">data science pipeline</a> is to ingest our data.  We use several Python tools which include:\n",
    "\n",
    "* [pdfminer](https://pypi.python.org/pypi/pdfminer/) - this is the tool that makes it ALL happen.  It has a command line tool called \"pdf2text.py\" that extract text contents from a PDF. **This must be installed on your computer BEFORE executing this code**.  Visit the [pdfminer homepage](http://euske.github.io/pdfminer/index.html#pdf2txt) for instructions\n",
    "\n",
    "* [subprocess](https://docs.python.org/2/library/subprocess.html) - a standard library module that allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes.  In this excerise, we use it to invoke the pdf2texy.py command line tool within our code.  \n",
    "\n",
    "* [nltk](http://www.nltk.org/) - another work horse in this exercise.  The Natural Language ToolKit (NLTK) is one of Python's leading platforms to analyze natural language data.  The [NLTK Book](http://www.nltk.org/book/) provides practical guidance on how to handle just about any natural language preprocessing job.  \n",
    "\n",
    "* [string](https://docs.python.org/2/library/string.html) - used for variable substitutions and value formatting to strip non printable characters from the output of the text extracted from our journal article PDFs\n",
    "\n",
    "* [unicodedata](https://docs.python.org/2/library/unicodedata.html) - some unicode characters won't extract nicely. This library allows latin unicode characters to degrade gracefully into ASCII.\n",
    "\n",
    "We are now going to iterate over each file in our raw data directory, strip the text, and write the *.txt* file to newly created directory.  Then we will follow the instructions from [Section 1.9, Chapter 2 of NLTK's Book](http://www.nltk.org/book/ch02.html) to build a custom corpus from our text files.  Having our target documents loaded as an NLTK corpus brings the power of NLTK to our analysis goals.  Let's begin with administrative tasks such as loading modules and creating the necessary directories.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Importing what we need\n",
    "#**********************************************************************\n",
    "import string\n",
    "import unicodedata\n",
    "import subprocess\n",
    "import nltk\n",
    "import os, os.path\n",
    "import re\n",
    "\n",
    "#**********************************************************************\n",
    "# Create the directory we will write the .txt files to after stripping text\n",
    "#**********************************************************************\n",
    "\n",
    "corpuspath = os.path.normpath(os.path.expanduser('~/Desktop/KDD_corpus/'))\n",
    "if not os.path.exists(corpuspath):\n",
    "    os.mkdir(corpuspath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Now we are to the big task of stripping text from the PDFs.  In the code below, we walk down the directory, and strip text from the files with names that begin with 'p' and end with 'pdf'.  We use the *fileName* variable to name the files we write to disk.  This will come in handy when we load data into NLTK.  Keep in mind, this task takes the longest, so be prepared to wait a a few minutes depending on good your computer is.  If you are doing this in an environment where you can spin up compute resources, your time will be drastically reduced.  Let's begin.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Core code to iterate over files in the directory\n",
    "#**********************************************************************\n",
    "\n",
    "# We start from the code to iterate over the files\n",
    "%timeit\n",
    "for dirName, subdirList, fileList in os.walk(TESTDIR):\n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            if os.path.exists(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\"))):\n",
    "                pass\n",
    "            else:\n",
    "            \n",
    "            \n",
    "#**********************************************************************\n",
    "# This code strips the text from the PDFs\n",
    "#**********************************************************************\n",
    "                try:\n",
    "                    document = filter(lambda x: x in string.printable,unicodedata.normalize('NFKD', (unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore'))).encode('ascii','ignore').decode('unicode_escape').encode('ascii','ignore'))\n",
    "                except UnicodeDecodeError:\n",
    "                    document = unicodedata.normalize('NFKD', unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore')).encode('ascii','ignore')    \n",
    "\n",
    "                if len(document)<300:\n",
    "                    pass\n",
    "                else:\n",
    "                    # used this for assistance http://stackoverflow.com/questions/2967194/open-in-python-does-not-create-a-file-if-it-doesnt-exist\n",
    "                    if not os.path.exists(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\"))):\n",
    "                        file = open(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\")), 'w+')\n",
    "                        file.write(document)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "kddcorpus= nltk.corpus.PlaintextCorpusReader(corpuspath, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kddcorpus= nltk.corpus.PlaintextCorpusReader(corpuspath, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>This is a pretty big step.  We have a semi-structured data set in a format where we can query and analyze different pieces of data.  All of our data is loaded as an NLTK corpus, meaning we could try tons of techniques outlined in the [NLTK book](http://www.nltk.org/book/) or use the NLTK APIs to pass data into [scikit-learn machine learning pipelines for text](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) (maybe for a later blog). Let's see how many words (including stop words) we have in our entire corpus.  <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2795267\n"
     ]
    }
   ],
   "source": [
    "wordcount = 0\n",
    "for fileid in kddcorpus.fileids():\n",
    "    wordcount += len(kddcorpus.words(fileid))\n",
    "print wordcount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step didn't come off without it's errors.  We got a little bit of gobbledygook (that is a [real word](http://www.merriam-webster.com/dictionary/gobbledygook) by the way). Here are the first 1000 characters of document 2157:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")UDX'HWHFWRU\u001d",
      "\u0003$\u0003*UDSK\u00100LQLQJ\u0010EDVHG\u0003)UDPHZRUN\u0003\u0003\n",
      "\n",
      "IRU\u0003)UDXGXOHQW\u00033KRQH\u0003&DOO\u0003'HWHFWLRQ\u0003\n",
      "\n",
      "9LQFHQW\u00036\u0011\u00037VHQJ\u0014\r",
      "\u000f\u0003-RVK\u0003-LD\u0010&KLQJ\u0003<LQJ\u0014\u000f\u0003&KH\u0010:HL\u0003+XDQJ\u0015\u000f\u0003<LPLQ\u0003.DR\u0016\u000f\u0003DQG\u0003.XDQ\u00107D\u0003&KHQ\u0017\u0003\n",
      "\n",
      "\u0014\u0003'HSDUWPHQW\u0003RI\u0003&RPSXWHU\u00036FLHQFH\u000f\u00031DWLRQDO\u0003&KLDR\u00037XQJ\u00038QLYHUVLW\\\u000f\u00037DLZDQ\u000f\u000352&\u0003\n",
      "\n",
      "\u0015\u0003'HSDUWPHQW\u0003RI\u0003&RPSXWHU\u00036FLHQFH\u0003DQG\u0003,QIRUPDWLRQ\u0003(QJLQHHULQJ\u000f\u00031DWLRQDO\u0003&KHQJ\u0003.XQJ\u00038QLYHUVLW\\\u000f\u00037DLZDQ\u000f\u000352&\u0003\n",
      "\n",
      "\u0016\u0003*RJRORRN\u0003&R\u0011\u0003/WG\u0011\u000f\u00037DLZDQ\u000f\u000352&\u0003\n",
      "\n",
      "\u0017\u0003,QVWLWXWH\u0003RI\u0003,QIRUPDWLRQ\u00036FLHQFH\u000f\u0003$FDGHPLD\u00036LQLFD\u000f\u00037DLZDQ\u000f\u000352&\u0003\n",
      "\n",
      "MDVK\\LQJ#JPDLO\u0011FRP\u000f\u0003ZHLLER\\#LGE\u0011FVLH\u0011QFNX\u0011HGX\u0011WZ\u000f\u0003\\LPLQNDR#JRJRORRN\u0011FRP\u000f\u0003VZF#LLV\u0011VLQLFD\u0011HGX\u0011WZ\u0003\n",
      "\n",
      "\r",
      "&RUUHVSRQGHQFH\u001d",
      " YWVHQJ#FV\u0011QFWX\u0011HGX\u0011WZ\u000f\u0003\n",
      "\n",
      "$%675$&7\u0003\n",
      ",Q\u0003UHFHQW\u0003\\HDUV\u000f\u0003IUDXG\u0003LV\u0003LQFUHDVLQJ\u0003UDSLGO\\\u0003ZLWK\u0003WKH\u0003GHYHORSPHQW\u0003RI\u0003\n",
      "PRGHUQ\u0003 WHFKQRORJ\\\u0003 DQG\u0003 JOREDO\u0003 FRPPXQLFDWLRQ\u0011\u0003 $OWKRXJK\u0003 PDQ\\\u0003\n",
      "OLWHUDWXUHV\u0003 KDYH\u0003 DGGUHVVHG\u0003 WKH\u0003 IUDXG\u0003 GHWHFWLRQ\u0003 SUREOHP\u000f\u0003 WKHVH\u0003\n",
      "H[LVWLQJ\u0003 ZRUNV\u0003 IRFXV\u0003 RQO\\\u0003 RQ\u0003 IRUPXODWLQJ\u0003 WKH\u0003 IUDXG\u0003 GHWHFWLRQ\u0003\n",
      "SUREOHP\u0003 DV\u0003 D\u0003 ELQDU\\\u0003 FODVVLILFDWLRQ\u0003 SUREOHP\u0011\u0003 'XH\u0003 WR\u0003 OLPLWDWLRQ\u0003 RI\u0003\n",
      "LQIRUPDWLRQ\u0003SURYLGHG\u0003E\\\u0003WHOHFRPPXQL\n"
     ]
    }
   ],
   "source": [
    "print kddcorpus.raw(\"p2157.txt\")[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>The NLTK book has an [excellent section on processing raw text and unicode issues](http://www.nltk.org/book/ch03.html#fig-unicode). I could never figure out what caused the error above but that's a dose of real world data problems.   Let's move on.  To begin our exploration of regular expressions (aka \"regex\"), it's important to point out some good resources to brush up on the topic.  The best resource I ever had was in [Videos 1-3, Week 4, Getting and Cleaning Data, Data Science Specialization Track](https://www.coursera.org/learn/data-cleaning) (At Coursera by Johns Hopkins University).  The instruction and examples in these helped me UNDERSTAND how to use regex vice googling [\"how to match text between two strings python regex\"](https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=how+to+match+text+between+two+strings+python+regex) and hacking away until getting the desired output.  When you understand regex, you will start to use metacharacter expression matches vice using literal matches, and crush any text matching requirment.  Here are some learning resources listed in my own subjective order of usefulness and relevance to python:\n",
    "* http://regexone.com/ (interactive teaching)\n",
    "* https://regex101.com/ (interactive testing; you can paste your text and test expressions)\n",
    "* http://regexr.com/ (interactive testing like above)\n",
    "* http://www.learnpython.org/en/Regular_Expressions (not very intuitive at first glimpse, but useful)\n",
    "* https://docs.python.org/2/library/re.html (default Python library documentation on regex)\n",
    "\n",
    "<br>As a quick test, we extract some \"good enough\" titles from the first 26 documents. I say \"good enough\" because some author names get caught up int he extractions below.  <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online Controlled Experiments: Lessons from Running A/B/n Tests for 12 Years\n",
      "Mining Frequent Itemsets through Progressive Sampling with Rademacher Averages\n",
      "Why It Happened: Identifying and Modeling the Reasons of the Happening of Social Events\n",
      "Matrix Completion with Queries Natali Ruchansky\n",
      "Stochastic Divergence Minimization for Online Collapsed Variational Bayes Zero Inference\n",
      "Bayesian Poisson Tensor Factorization for Inferring Multilateral Relations from Sparse Dyadic Event Counts\n",
      "TimeCrunch: Interpretable Dynamic Graph Summarization Neil Shah\n",
      "Inside Jokes: Identifying Humorous Cartoon Captions Dafna Shahaf\n",
      "Community Detection based on Distance Dynamics Junming Shao\n",
      "Discovery of Meaningful Rules in Time Series Mohammad Shokoohi-Yekta    Yanping Chen    Bilson Campana    Bing Hu\n",
      "On the Formation of Circles in Co-authorship Networks Tanmoy Chakraborty1, Sikhar Patranabis2, Pawan Goyal3, Animesh Mukherjee4\n",
      "An Evaluation of Parallel Eccentricity Estimation Algorithms on Undirected Real-World Graphs\n",
      "Efcient Latent Link Recommendation in Signed Networks\n",
      "Turn Waste into Wealth: On Simultaneous Clustering and Cleaning over Dirty Data\n",
      "Set Cover at Web Scale Stergios Stergiou\n",
      "Exploiting Relevance Feedback in Knowledge Graph Search\n",
      "LINKAGE: An Approach for Comprehensive Risk Prediction for Care Management\n",
      "Transitive Transfer Learning Ben Tan\n",
      "PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks\n",
      "An Effective Marketing Strategy for Revenue Maximization with a Quantity Constraint\n",
      "Scaling Up Stochastic Dual Coordinate Ascent Kenneth Tran\n",
      "Heterogeneous Network Embedding via Deep Architectures\n",
      "Discovering Valuable Items from Massive Data Hastagiri P Vanchinathan\n",
      "Deep Learning Architecture with Dynamically Programmed Layers for Brain Connectome Prediction\n",
      "Incorporating World Knowledge to Document Clustering via Heterogeneous Information Networks\n"
     ]
    }
   ],
   "source": [
    "# This title extraction is probably unnecessarily complex, but it gets the job done; we make use of the metacharacters vice literal matches\n",
    "\n",
    "p=re.compile('^(.*)([\\s]){2}[A-z]+[\\s]+[\\s]?.+')# matches text, starting from beginning of line, followed by at least two\n",
    "for fileid in kddcorpus.fileids()[:25]:\n",
    "    print re.search('^(.*)[\\s]+[\\s]?(.*)?',kddcorpus.raw(fileid)).group(1).strip()+\" \"+re.search('^(.*)[\\s]+[\\s]?(.*)?',kddcorpus.raw(fileid)).group(2).strip()\n",
    "      # use .strip() to remove whitespace from beginning and end of string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling and computation: Using Regular Expressions to extract specific sections of the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are close to the NERC portion.  But, there's a bit more wrangling to do (remember, PDFs are tough work).  For simplicity, let's focus the NERC on two sections of the paper:\n",
    "* the top section which includes authors and schools\n",
    "* the references section of the paper (keynote speaker abstracts do not have an abstract)\n",
    "\n",
    "The tools of choice to extract sections are the [\"positive lookbehind\" and \"positive lookahead\"](https://docs.python.org/2/library/re.html) expressions. Here is an example of code to extract the abstract only:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The collapsed variational Bayes zero (CVB0) inference is a vari- ational inference improved by marginalizing out parameters, the same as with the collapsed Gibbs sampler. A drawback of the CVB0 inference is the memory requirements. A probability vec- tor must be maintained for latent topics for every token in a corpus. When the total number of tokens is N and the number of topics is K, the CVB0 inference requires O(N K) memory. A stochas- tic approximation of the CVB0 (SCVB0) inference can reduce O(N K) to O(V K), where V denotes the vocabulary size. We re- formulate the existing SCVB0 inference by using the stochastic di- vergence minimization algorithm, with which convergence can be analyzed in terms of Martingale convergence theory. We also reveal the property of the CVB0 inference in terms of the leave-one-out perplexity, which leads to the estimation algorithm of the Dirichlet distribution parameters. The predictive performance of the propose SCVB0 inference is better than that of the original SCVB0 infer- ence in four datasets.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set our regular expression\n",
    "p= re.compile('(?<=ABSTRACT)(.+)(?=Categories and Subject Descriptors)')\n",
    "try:\n",
    "    abstract= p.search(re.sub('[\\s]',\" \",kddcorpus.raw('p1035.txt'))).group(1)\n",
    "except AttributeError:\n",
    "    # include a lowercase regex match incase consistency is a problem\n",
    "    p=re.compile('(?<=abstract)(.+)(?=categories and subject descriptors)')\n",
    "    abstract=p.search(re.sub('[\\s]',\" \",holder.lower())).group(1)\n",
    "else:\n",
    "    pass\n",
    "unicodedata.normalize('NFKD', abstract).encode('ascii','ignore').strip() # convert output from unicode to string and strip leading and trailing whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!  Now, to be \"pythonic\" we build two functions that can extract the top and references section of the documents.  For fun, I also made other function to extract the keywords and abstract sections of the documents.  We could do the same for any section of paper although I must provide a warning.  **Working with natural language is a messy ordeal!**  This is a top notch organization (ACM) and a top notch conference (KDD) but human error sitll makes it way into the picture:\n",
    "\n",
    "![Human Error](http://www.process-improvement-institute.com/wp-content/uploads/2015/05/Accounting-for-Human-Error-Probability-in-SIL-Verification.jpg)\n",
    "\n",
    "Specifically in our case:\n",
    "* paper 1 header section = \"Categories and Subject Descriptors\"\n",
    "* paper 2 header section = \"Categories & Subject Descriptors\"\n",
    "\n",
    "Very small difference but these types of differences cause TONS of headaches.  The result?  You have a decision to make: **account for these differences or ignore them**.  I worked to include AS MUCH of the 253 corpus as possible in the results but it's never perfect.  There are also some documents that will be missing sections altogether (i.e. keynote speaker documents do not have a references section.  Our two functions will:\n",
    "\n",
    "1. Extract only the relevant text for the section we seek\n",
    "2. Extract a character count for the section\n",
    "3. Make additonal calculations or extractions\n",
    "  * the top section extraction also extract emails\n",
    "  * we count the number of references and store that value\n",
    "  * as added benefit, we create a simple \"word per reference\" calculation\n",
    "4. Store all the above data as a nested dictionary with the filename as a key\n",
    "\n",
    "These are loooooong blocks of code to accomplish the task above.  For now, we will only show the code to extract the references and perform the quick analysis mentioned above.  The other functions will be in the appendix.  In fairness, all functions could be reduced down to one function composed of nested function calls.  We will save that for later and get the \"functionality\" working before optimizing the code. See the comments below to follow along or just skip to the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code to pull the ferences section only, store a character count, number of references, and \"word per reference\" calculation\n",
    "\n",
    "def refpull(docnum=None,section='references',full = False):\n",
    "    \n",
    "    # Establish an empty dictionary to hold values\n",
    "    ans={}\n",
    "    \n",
    "    # Establish an empty list to hold document ids that don't make the cut (i.e. missing reference section or different format)\n",
    "    # This comes in handy when you are trying to improve your code to catch outliers\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    \n",
    "    # Admin code to set default values and raise an exception if there's human error on input\n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        # Setting the target document and the text we will extract from \n",
    "        text=kddcorpus.raw(docnum)\n",
    "        \n",
    "        \n",
    "        # This first condtional is for pulling the target section for ALL documents in the corpus\n",
    "        if full == True:\n",
    "            \n",
    "            # Iterate over the corpus to get the id; this is possible from loading our docs into a custom NLTK corpus\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid)\n",
    "                \n",
    "                # These lines of code build our regular expression.\n",
    "                # In the other functions for abstract or keywords, you see how I use this technique to create different regex arugments\n",
    "                if section == \"references\":\n",
    "                    section1=[\"REFERENCES\"] \n",
    "                    \n",
    "                    # Just in case, making sure our target string is empty before we pass data into it; just a check\n",
    "                    target = \"\"   \n",
    "\n",
    "                    #We now build our lists iteratively to build our regex\n",
    "                    for sect in section1:\n",
    "                        \n",
    "                        # We embed exceptions to remove the possibility of our code stopping; we pass failed passes into a list\n",
    "                        try:\n",
    "                            \n",
    "                            # our machine built regex\n",
    "                            part1= \"(?<=\"+sect+\")(.+)\"\n",
    "                            p=re.compile(part1)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                            \n",
    "                            # Conditoin to make sure we don't get any empty string\n",
    "                            if len(target) > 50:\n",
    "\n",
    "                                # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                                try:\n",
    "                                    refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                                except:\n",
    "                                    print \"This file does not appear to have a references section\"\n",
    "                                    pass\n",
    "                                \n",
    "                                #These are all our values; we build a nested dictonary and store the calculated values\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"references\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                ans[str(fileid)][\"refcount\"]= refnum\n",
    "                                ans[str(fileid)][\"wordperRef\"]=round(float(len(nltk.word_tokenize(text)))/float(refnum))\n",
    "                                #print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                                break\n",
    "                            else:\n",
    "\n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "\n",
    "            return ans\n",
    "            return failids\n",
    "                              \n",
    "        # This is to perform the same operations on just one document; same functionality as above.\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum)\n",
    "        \n",
    "        if section == \"references\":\n",
    "            section1=[\"REFERENCES\"] \n",
    "            target = \"\"   \n",
    "            for sect in section1:\n",
    "                try:\n",
    "                    part1= \"(?<=\"+sect+\")(.+)\"\n",
    "                    p=re.compile(part1)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    if len(target) > 50:\n",
    "                        # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                        try:\n",
    "                            refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                        except:\n",
    "                            print \"This file does not appear to have a references section\"\n",
    "                            pass\n",
    "                        ans[str(docnum)]={}\n",
    "                        ans[str(docnum)][\"references\"]=target.strip()\n",
    "                        ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                        ans[str(docnum)][\"refcount\"]= refnum\n",
    "                        ans[str(docnum)][\"wordperRef\"]=float(len(nltk.word_tokenize(text)))/float(refnum)\n",
    "\n",
    "\n",
    "                        #print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                        break\n",
    "                    else:\n",
    "\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(docnum)\n",
    "                    pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        return ans\n",
    "        return failids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a big block of code!  Don't fret, there are several similar blocks in the appendix to extract the abstract and keywords.  Data is messy; this is what cleaning looks like.  In the code above, we also make use of the *nltk.word_tokenize* tool to create the \"word per reference\" figure.  Let's test our function and some output (the word_tokenize calculation will take some time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# call our function, setting \"full=True\" extracts ALL references in corpus\n",
    "test = refpull(full=True)\n",
    "\n",
    "# To get a quick glimpse, I use the example from this page: http://stackoverflow.com/questions/7971618/python-return-first-n-keyvalue-pairs-from-dict\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "man = collections.OrderedDict(test)\n",
    "\n",
    "x = itertools.islice(man.items(), 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename      Character Count    Number of references    Words per Reference\n",
      "----------  -----------------  ----------------------  ---------------------\n",
      "p2277.txt                6295                      33                    326\n",
      "p835.txt                 5347                      38                    319\n",
      "p865.txt                 5269                      27                    399\n",
      "p2089.txt                8734                      45                    181\n",
      "p1759.txt                3677                      31                    405\n",
      "p29.txt                  5101                      40                    265\n",
      "p2227.txt               10345                      36                    332\n",
      "p2099.txt                3949                      28                    374\n",
      "p725.txt                 5771                      37                    304\n",
      "p2019.txt                9101                      60                    171\n"
     ]
    }
   ],
   "source": [
    "# Let's use a nifty table module to print this all pretty like: https://pypi.python.org/pypi/tabulate\n",
    "# The joy of Python and open source: someone has created something to do what you want; Google is your friend.  \n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "# A quick list comprehension to follow the example on the tabulate pypi page\n",
    "table = [[key,value['charcount'],value['refcount'], value['wordperRef']] for key,value in x]\n",
    "\n",
    "# print the pretty table; we invoke the \"header\" argument and assign custom header!!!!\n",
    "print tabulate(table,headers=[\"filename\",\"Character Count\", \"Number of references\",\"Words per Reference\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data computation and analyses: Using NERC tools and examining for accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are in the spot where data scientists WANT to live: computation and analyses!!!  In truth, most spend their time ingesting, wrangling, and munging data, as you see above. \n",
    "\n",
    "We are ready to test how well some open source NERC tools extract names, places, and organizations from the top and reference sections of our corpus.  As an added benefit (using the web scraping code from above), we can do a comparison to see how well our pdf-ingest-scrape-regex-NERC pipeline works compared to old-fashioned web scraping.  \n",
    "\n",
    "We start with a few hand labeled documents.  Hand labeling is an expensive and tedious process; the entities for two documents I labeled (yea..it's only 2 but that was 295 cut-and-pastes not counting writing the list names):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 authors\n",
      "\n",
      "There are 3 author organizations\n",
      "\n",
      "There are 3 author locations\n",
      "\n",
      "There are 152 authors in the references\n"
     ]
    }
   ],
   "source": [
    "# filename p19.txt\n",
    "\n",
    "p19pdf_authors=['Tim Althoff','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang']\n",
    "p19pdf_author_organizations=['Computer Science Department','Stanford University','Google']\n",
    "p19pdf_author_locations=['Stanford, CA','1600 Amphitheatre Parkway, Mountain View, CA 94043','Mountain View']\n",
    "\n",
    "p19pdf_references_authors =['A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']\n",
    "\n",
    "p19pdf_allauthors=['Tim Althoff','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang','A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']\n",
    "\n",
    "print \"There are %r authors\" % len(p19pdf_authors)\n",
    "print  # white space\n",
    "print \"There are %r author organizations\" %len(p19pdf_author_organizations)\n",
    "print \n",
    "print \"There are %r author locations\" % len(p19pdf_author_locations)\n",
    "print  \n",
    "print \"There are %r authors in the references\" %len(p19pdf_references_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 authors\n",
      "\n",
      "There are 6 author organizations\n",
      "\n",
      "There are 8 author locations\n",
      "\n",
      "There are 106 authors in the references\n"
     ]
    }
   ],
   "source": [
    "# filename p29.txt\n",
    "\n",
    "p29pdf_authors=['Laurent Amsaleg','Stéphane Girard','Oussama Chelly','Teddy Furon','Michael E. Houle','Ken-ichi Kawarabayashi',\n",
    "               'Michael Nett']\n",
    "p29pdf_author_organizations=['Equipe LINKMEDIA','Campus Universitaire de Beaulieu','CNRS/IRISA Rennes','National Institute of Informatics',\n",
    "                             'Equipe MISTIS INRIA','Google']\n",
    "p29pdf_author_locations=['Campus Universitaire de Beaulieu','35042 Rennes Cedex, France','France','-1-2 Hitotsubashi, Chiyoda-ku Tokyo 101-8430, Japan',\n",
    "                        'Japan','6-10-1 Roppongi, Minato-ku Tokyo 106-6126','Inovallée, 655, Montbonnot 38334 Saint-Ismier Cedex','Tokyo']\n",
    "\n",
    "p29pdf_references_authors =['A. A. Balkema','L. de Haan','N. Bingham', 'C. Goldie','J. Teugels','N. Boujemaa', 'J. Fauqueur', 'M. Ferecatu', 'F. Fleuret',\n",
    "                            'V. Gouet', 'B. LeSaux','H. Sahbi','C. Bouveyron', 'G. Celeux', 'S. Girard','J. Bruske', 'G. Sommer',\n",
    "                           'F. Camastra','A. Vinciarelli','S. Coles','J. Costa' ,'A. Hero','T. de Vries', 'S. Chawla','M. E. Houle',\n",
    "                           'R. A. Fisher','L. H. C. Tippett','M. I. Fraga Alves', 'L. de Haan','T. Lin','M. I. Fraga Alves', 'M. I. Gomes','L. de Haan',\n",
    "                           'B. V. Gnedenko',' A. Gupta', 'R. Krauthgamer','J. R. Lee','A. Gupta', 'R. Krauthgamer','J. R. Lee','M. Hein','J.-Y. Audibert',\n",
    "                           'B. M. Hill','M. E. Houle','M. E. Houle','M. E. Houle','M. E. Houle', 'H. Kashima', 'M. Nett','M. E. Houle', 'X. Ma', 'M. Nett',\n",
    "                            'V. Oria','M. E. Houle', 'X. Ma', 'V. Oria','J. Sun','M. E. Houle','M. Nett','H. Jegou', 'R. Tavenard', 'M. Douze','L. Amsaleg',\n",
    "                           'I. Jollie','D. R. Karger','M. Ruhl','J. Karhunen','J. Joutsensalo','Y. LeCun', 'L. Bottou', 'Y. Bengio', 'P. Haner',\n",
    "                           'J. Pickands, III','C. R. Rao','S. T. Roweis','L. K. Saul','A. Rozza', 'G. Lombardi', 'C. Ceruti', 'E. Casiraghi', 'P. Campadelli',\n",
    "                           'B. Scholkopf', 'A. J. Smola','K.-R. Muller','U. Shaft','R. Ramakrishnan',' F. Takens','J. Tenenbaum', 'V. D. Silva','J. Langford',\n",
    "                           'J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. Venna','S. Kaski',\n",
    "                           'P. Verveer','R. Duin','J. von Brunken', 'M. E. Houle', 'A. Zimek','J. von Brunken', 'M. E. Houle','A. Zimek']\n",
    "\n",
    "p29pdf_allauthors=['Laurent Amsaleg','Stéphane Girard','Oussama Chelly','Teddy Furon','Michael E. Houle','Ken-ichi Kawarabayashi',\n",
    "               'Michael Nett','A. A. Balkema','L. de Haan','N. Bingham', 'C. Goldie','J. Teugels','N. Boujemaa', 'J. Fauqueur', 'M. Ferecatu', 'F. Fleuret',\n",
    "                            'V. Gouet', 'B. LeSaux','H. Sahbi','C. Bouveyron', 'G. Celeux', 'S. Girard','J. Bruske', 'G. Sommer',\n",
    "                           'F. Camastra','A. Vinciarelli','S. Coles','J. Costa' ,'A. Hero','T. de Vries', 'S. Chawla','M. E. Houle',\n",
    "                           'R. A. Fisher','L. H. C. Tippett','M. I. Fraga Alves', 'L. de Haan','T. Lin','M. I. Fraga Alves', 'M. I. Gomes','L. de Haan',\n",
    "                           'B. V. Gnedenko',' A. Gupta', 'R. Krauthgamer','J. R. Lee','A. Gupta', 'R. Krauthgamer','J. R. Lee','M. Hein','J.-Y. Audibert',\n",
    "                           'B. M. Hill','M. E. Houle','M. E. Houle','M. E. Houle','M. E. Houle', 'H. Kashima', 'M. Nett','M. E. Houle', 'X. Ma', 'M. Nett',\n",
    "                            'V. Oria','M. E. Houle', 'X. Ma', 'V. Oria','J. Sun','M. E. Houle','M. Nett','H. Jegou', 'R. Tavenard', 'M. Douze','L. Amsaleg',\n",
    "                           'I. Jollie','D. R. Karger','M. Ruhl','J. Karhunen','J. Joutsensalo','Y. LeCun', 'L. Bottou', 'Y. Bengio', 'P. Haner',\n",
    "                           'J. Pickands, III','C. R. Rao','S. T. Roweis','L. K. Saul','A. Rozza', 'G. Lombardi', 'C. Ceruti', 'E. Casiraghi', 'P. Campadelli',\n",
    "                           'B. Scholkopf', 'A. J. Smola','K.-R. Muller','U. Shaft','R. Ramakrishnan',' F. Takens','J. Tenenbaum', 'V. D. Silva','J. Langford',\n",
    "                           'J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. Venna','S. Kaski',\n",
    "                           'P. Verveer','R. Duin','J. von Brunken', 'M. E. Houle', 'A. Zimek','J. von Brunken', 'M. E. Houle','A. Zimek']\n",
    "\n",
    "\n",
    "print \"There are %r authors\" % len(p29pdf_authors)\n",
    "print  # white space\n",
    "print \"There are %r author organizations\" %len(p29pdf_author_organizations)\n",
    "print \n",
    "print \"There are %r author locations\" % len(p29pdf_author_locations)\n",
    "print  \n",
    "print \"There are %r authors in the references\" %len(p29pdf_references_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we can programmatically access just about all of the corpus, we are free to hand label as much as we want to do the test.  Our measureable test:\n",
    "\n",
    "* Compare machice extracted list of persons, places, and organizations to hand labeled lists\n",
    "* Compute precision, accuracy and recall\n",
    "* Compare different NERC tool scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the performance of three open source NERC tools.  All of them can be trained to improve performance, but for now we will test \"out of the box\" performance:\n",
    "\n",
    "1.  [NLTK's standard chunker](http://www.nltk.org/api/nltk.chunk.html); read more in [the NLTK book](http://www.nltk.org/book/ch07.html)\n",
    "2. [Standard's Named Entity Recognizer](http://nlp.stanford.edu/software/CRF-NER.shtml), which can be accessed as an API via the NLTK tool\n",
    "3. [Polyglot NER](http://polyglot.readthedocs.org/en/latest/index.html) which is natural language pipeline that supports massive multilingual applications.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin to chunk our data using the benefits of having our texts loaded into NLTK.  We first get the data for our test documents.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need the top and references sections from p19.txt and p29.txt\n",
    "\n",
    "p19top = toppull(\"p19.txt\")\n",
    "p19ref = refpull(\"p19.txt\")\n",
    "\n",
    "p29top=toppull(\"p29.txt\")\n",
    "p29ref=refpull(\"p29.txt\")\n",
    "\n",
    "p19={}\n",
    "p19['top']=p19top['p19.txt']['top']\n",
    "p19['references']=p19ref['p19.txt']['references']\n",
    "\n",
    "\n",
    "p29={}\n",
    "p29['top']=p29top['p29.txt']['top']\n",
    "p29['references']=p29ref['p29.txt']['references']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>All the munging and wrangling paid off; we can access any document and pull out a section with a few lines of code.  We use all three of our NERC tools to extract information.  First is the NLTK standard chunker.  We extract entities and build lists using the \"nltktreelist\" function in the appendix. <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extracting entities from teh top and references section using NLTK's standard chunker\n",
    "\n",
    "nltkstandard_p19ents = {'top': nltktreelist(p19['top']),'references': nltktreelist(p19['references'])}\n",
    "nltkstandard_p29ents = {'top': nltktreelist(p29['top']),'references': nltktreelist(p29['references'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> To compare the hand labeled data to nltk standard chunk extractions, we use the [sets](https://docs.python.org/2/library/sets.html) module in the standard library.  This module returns and ordered set of elements that are common to both objects. I'll also make use of the nifty [tabulate](https://pypi.python.org/pypi/tabulate) library.  <br><br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractions from document \"p19.txt\" using NLTK Standard Chunker\n",
      "\n",
      "persons          locations    genpurp  organizations\n",
      "-------------  -----------  ---------  -------------------\n",
      "Timeline                               Generation\n",
      "Tim Althoff                            Stanford University\n",
      "Xin Luna Dong\n",
      "Kevin Murphy\n",
      "Safa Alai\n",
      "Van Dang\n",
      "Wei Zhang\n",
      "Stanford\n",
      "Mountain View\n",
      "\n",
      "\n",
      "Hand labeled extractions from document \"p19.txt\"\n",
      "\n",
      "persons        locations                                           organizations\n",
      "-------------  --------------------------------------------------  ---------------------------\n",
      "Tim Althoff    Stanford, CA                                        Computer Science Department\n",
      "Xin Luna Dong  1600 Amphitheatre Parkway, Mountain View, CA 94043  Stanford University\n",
      "Kevin Murphy   Mountain View                                       Google\n",
      "Safa Alai\n",
      "Van Dang\n",
      "Wei Zhang\n",
      "\n",
      "\n",
      "Common persons in each list (These are the entities that matched exactly!)\n",
      "\n",
      "['Wei Zhang', 'Tim Althoff', 'Xin Luna Dong', 'Van Dang', 'Kevin Murphy', 'Safa Alai']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print \"Extractions from document \\\"p19.txt\\\" using NLTK Standard Chunker\"\n",
    "print\n",
    "print tabulate(nltkstandard_p19ents['top'], headers=\"keys\")\n",
    "print \n",
    "print\n",
    "truth19 = {}\n",
    "truth19['persons'] = p19pdf_authors\n",
    "truth19['locations'] = p19pdf_author_locations\n",
    "truth19['organizations'] = p19pdf_author_organizations\n",
    "print \"Hand labeled extractions from document \\\"p19.txt\\\"\"\n",
    "print\n",
    "print tabulate(truth19,headers=\"keys\")\n",
    "print\n",
    "print \n",
    "print \"Common persons in each list (These are the entities that matched exactly!)\"\n",
    "print \n",
    "print list(set(nltkstandard_p19ents['top']['persons']) & set(truth19['persons']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use the Stanford Named Entity Recognizer and part of speech tagger.  NLTK provides an [interface to the Stanford NERC tool](http://www.nltk.org/_modules/nltk/tag/stanford.html).  Details for [using the tool are on the NLTK page](http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford). The jar files can be downloaded [here](http://nlp.stanford.edu/software/index.shtml).  I use a function called *\"get_continuous_chunks\"* to extract entities using the Stanford NERC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Import what we need\n",
    "# Administrative code with path to jar file holding trained model\n",
    "#**********************************************************************\n",
    "from nltk.tag import StanfordNERTagger, StanfordPOSTagger\n",
    "stner = StanfordNERTagger('/Users/linwood/stanford-corenlp-full/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "       '/Users/linwood/stanford-corenlp-full/stanford-corenlp-3.5.2.jar',\n",
    "       encoding='utf-8')\n",
    "stpos = StanfordPOSTagger('/Users/linwood/stanford-postagger-full/models/english-bidirectional-distsim.tagger','/Users/linwood/stanford-postagger-full/stanford-postagger.jar') \n",
    "\n",
    "#**********************************************************************\n",
    "# Core code to use Stanford POS tagger and NER\n",
    "#**********************************************************************\n",
    "\n",
    "stan_p19ents = {'top': get_continuous_chunks(p19['top']), 'references': get_continuous_chunks(p19['references'])}\n",
    "stan_p29ents = {'top': get_continuous_chunks(p29['top']), 'references': get_continuous_chunks(p29['references'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>We use the convention established earlier to get a quick glimpse for how well these NERC tool performed on one of our documents.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractions from document \"p19.txt\" using Stanford Chunker\n",
      "\n",
      "persons          locations  organizations\n",
      "-------------  -----------  --------------------------------------\n",
      "Tim Althoff                 Wei Zhang *Computer Science Department\n",
      "Xin Luna Dong               Stanford University\n",
      "Kevin Murphy\n",
      "\n",
      "\n",
      "Hand labeled extractions from document \"p19.txt\"\n",
      "\n",
      "persons        locations                                           organizations\n",
      "-------------  --------------------------------------------------  ---------------------------\n",
      "Tim Althoff    Stanford, CA                                        Computer Science Department\n",
      "Xin Luna Dong  1600 Amphitheatre Parkway, Mountain View, CA 94043  Stanford University\n",
      "Kevin Murphy   Mountain View                                       Google\n",
      "Safa Alai\n",
      "Van Dang\n",
      "Wei Zhang\n",
      "\n",
      "\n",
      "Common persons in each list (These are the entities that matched exactly!)\n",
      "\n",
      "[u'Kevin Murphy', u'Tim Althoff', u'Xin Luna Dong']\n"
     ]
    }
   ],
   "source": [
    "print \"Extractions from document \\\"p19.txt\\\" using Stanford Chunker\"\n",
    "print\n",
    "print tabulate(stan_p19ents['top'], headers=\"keys\")\n",
    "print \n",
    "print\n",
    "truth19 = {}\n",
    "truth19['persons'] = p19pdf_authors\n",
    "truth19['locations'] = p19pdf_author_locations\n",
    "truth19['organizations'] = p19pdf_author_organizations\n",
    "print \"Hand labeled extractions from document \\\"p19.txt\\\"\"\n",
    "print\n",
    "print tabulate(truth19,headers=\"keys\")\n",
    "print\n",
    "print \n",
    "print \"Common persons in each list (These are the entities that matched exactly!)\"\n",
    "print \n",
    "print list(set(stan_p19ents['top']['persons']) & set(truth19['persons']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Next up is the Polyglot NERC tool.  Similar to the other capabilities, we have a function in the appendix that we use to produce a clean output.<br><br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly_p19ents = {'top': extraction(p19['top']), 'references': extraction(p19['references'])}\n",
    "poly_p29ents = {'top': extraction(p29['top']), 'references': extraction(p29['references'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Similar to the other two NERC tools, we will compare the results quickly to the hand labeled data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractions from document \"p19.txt\" using Polyglot Chunker\n",
      "\n",
      "persons        locations    organizations\n",
      "-------------  -----------  ---------------------------\n",
      "Tim Althoff    CA           Computer Science Department\n",
      "Xin Luna Dong  View ,       Stanford University\n",
      "Kevin Murphy   kpmurphy     Stanford\n",
      "Safa           safa\n",
      "Van Dang       vandang\n",
      "Wei Zhang\n",
      "\n",
      "\n",
      "Hand labeled extractions from document \"p19.txt\"\n",
      "\n",
      "persons        locations                                           organizations\n",
      "-------------  --------------------------------------------------  ---------------------------\n",
      "Tim Althoff    Stanford, CA                                        Computer Science Department\n",
      "Xin Luna Dong  1600 Amphitheatre Parkway, Mountain View, CA 94043  Stanford University\n",
      "Kevin Murphy   Mountain View                                       Google\n",
      "Safa Alai\n",
      "Van Dang\n",
      "Wei Zhang\n",
      "\n",
      "\n",
      "Common persons in each list (These are the entities that matched exactly!)\n",
      "\n",
      "['Kevin Murphy', 'Tim Althoff', 'Xin Luna Dong', 'Van Dang', 'Wei Zhang']\n"
     ]
    }
   ],
   "source": [
    "print \"Extractions from document \\\"p19.txt\\\" using Polyglot Chunker\"\n",
    "print\n",
    "print tabulate(poly_p19ents['top'], headers=\"keys\")\n",
    "print \n",
    "print\n",
    "truth19 = {}\n",
    "truth19['persons'] = p19pdf_authors\n",
    "truth19['locations'] = p19pdf_author_locations\n",
    "truth19['organizations'] = p19pdf_author_organizations\n",
    "print \"Hand labeled extractions from document \\\"p19.txt\\\"\"\n",
    "print\n",
    "print tabulate(truth19,headers=\"keys\")\n",
    "print\n",
    "print \n",
    "print \"Common persons in each list (These are the entities that matched exactly!)\"\n",
    "print \n",
    "print list(set(poly_p19ents['top']['persons']) & set(truth19['persons']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sup id=\"fn1\">1. [(2014). Text Mining and its Business Applications - CodeProject. Retrieved December 26, 2015, from http://www.codeproject.com/Articles/822379/Text-Mining-and-its-Business-Applications.]<a href=\"#ref1\" title=\"Jump back to footnote 1 in the text.\">↩</a></sup>\n",
    "\n",
    "<sup id=\"fn2\">2. [Suchanek, F., & Weikum, G. (2013). Knowledge harvesting in the big-data era. Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data. ACM.]<a href=\"#ref2\" title=\"Jump back to footnote 2 in the text.\">↩</a></sup>\n",
    "\n",
    "\n",
    "<sup id =\"fn3\">3. [Nadeau, D., & Sekine, S. (2007). A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1), 3-26.]<a href=\"#ref3\" title = \"Jump back to footnote 3 in the text\">↩</a></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Parking Lot of links, leftover paragraphs, ideas, etc.</span>\n",
    "\n",
    "Describe the data -> Data available here http://dl.acm.org/citation.cfm?id=2783258# \n",
    "\n",
    "Computer Vision - ECCV 2008 pdf download online free. Retrieved December 31, 2015, from http://pdf12.mono-ebook.org/pdf/computer-vision-eccv-2008_12glgt.pdf.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<href id=\"pipe\"><a href=\"#pipeline\" title=\"Jump back to data science pipeline graphic.\">data science pipeline</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ben's Outline from email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ~~Give a brief introduction to the task, and why it's interesting, important. Then begin to discuss the data set, how you acquired, and where a reader can get access to it.~~ \n",
    "\n",
    "* ~~You then could have a data exploration section where you show the number of documents, perform a word count, show snippets of data (e.g. references) etc that are of interest.~~\n",
    "\n",
    "* ~~You can then go through one or a few of your \"code to get\" sections. These functions all follow basically the same pattern, so you could probably merge them into a single function, that appropriately selects the right regular expression.~~ \n",
    "\n",
    "* ~~The next step is to discuss, demonstrate your \"truth tests\" for text extraction accuracy.~~ \n",
    "\n",
    "* Finally, you can get to an introduction of your three methods for NERC, and show how do do each of them. Then compare (visually) the results of the three according to the evaluation mechanism discussed above. \n",
    "\n",
    "* You could then conclude with a discussion about NLTK chunk vs. hand labelled entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kddcorpus_bigrams=[]\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "for fileid in kddcorpus.fileids():\n",
    "    for l in (BigramCollocationFinder.from_words(kddcorpus.words(fileid)).nbest(bigram_measures.pmi, 10)):\n",
    "        kddcorpus_bigrams.append(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Prototype Holder</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extraction(corpus):\n",
    "    import itertools\n",
    "    import unicodedata\n",
    "    from polyglot.text import Text\n",
    "    \n",
    "    corpus=corpus\n",
    "    # extract entities from a single string; remove whitespace characters\n",
    "    try:\n",
    "        e = Text(corpus).entities\n",
    "    except:\n",
    "        pass #e = Text(re.sub(\"(r'(x0)',\" \",\"(re.sub('[\\s]',\" \",corpus)))).entities\n",
    "    \n",
    "    current_person =[]\n",
    "    persons =[]\n",
    "    current_org=[]\n",
    "    organizations=[]\n",
    "    current_loc=[]\n",
    "    locations=[]\n",
    "\n",
    "    for l in e:\n",
    "        if l.tag == 'I-PER':\n",
    "            for m in l:\n",
    "                current_person.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_person: # if the current chunk is not empty\n",
    "                        persons.append(\" \".join(current_person))\n",
    "                        current_person = []\n",
    "        elif l.tag == 'I-ORG':\n",
    "            for m in l:\n",
    "                current_org.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_org: # if the current chunk is not empty\n",
    "                        organizations.append(\" \".join(current_org))\n",
    "                        current_org = []\n",
    "        elif l.tag == 'I-LOC':\n",
    "            for m in l:\n",
    "                current_loc.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_loc: # if the current chunk is not empty\n",
    "                        locations.append(\" \".join(current_loc))\n",
    "                        current_loc = []\n",
    "    results = {}\n",
    "    results['persons']=persons\n",
    "    results['organizations']=organizations\n",
    "    results['locations']=locations\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_continuous_chunks(string):\n",
    "    string = string\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for token, tag in stner.tag(string.split()):\n",
    "        if tag != \"O\":\n",
    "            current_chunk.append((token, tag))\n",
    "        else:\n",
    "            if current_chunk: # if the current chunk is not empty\n",
    "                continuous_chunk.append(current_chunk)\n",
    "                current_chunk = []\n",
    "    # Flush the final current_chunk into the continuous_chunk, if any.\n",
    "    if current_chunk:\n",
    "        continuous_chunk.append(current_chunk)\n",
    "    named_entities = continuous_chunk\n",
    "    named_entities_str = [\" \".join([token for token, tag in ne]) for ne in named_entities]\n",
    "    named_entities_str_tag = [(\" \".join([token for token, tag in ne]), ne[0][1]) for ne in named_entities]\n",
    "    persons = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"PERSON\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    persons.append(n.strip(\"*\"))\n",
    "    organizations = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"ORGANIZATION\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    organizations.append(n.strip(\"*\"))\n",
    "    locations = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"LOCATION\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    locations.append(n.strip(\"*\"))\n",
    "    dates = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"DATE\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    dates.append(n.strip(\"*\"))\n",
    "    money = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"MONEY\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    money.append(n.strip(\"*\"))\n",
    "    time = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"TIME\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    money.append(n.strip(\"*\"))\n",
    "                    \n",
    "    percent = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"PERCENT\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    money.append(n.strip(\"*\"))\n",
    "\n",
    "    entities={}\n",
    "    entities['persons']= persons\n",
    "    entities['organizations']= organizations\n",
    "    entities['locations']= locations\n",
    "    #entities['dates']= dates\n",
    "    #entities['money']= money\n",
    "    #entities['time']= time\n",
    "    #entities['percent']= percent\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_continuous_chunks(p29['top'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nltktreelist(text):\n",
    "    from operator import itemgetter\n",
    "    \n",
    "    text = text\n",
    "    \n",
    "    \n",
    "    persons = []\n",
    "    organizations = []\n",
    "    locations =[]\n",
    "    genpurp = []\n",
    "\n",
    "    for l in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))):\n",
    "        if isinstance(l,nltk.tree.Tree):\n",
    "            if l.label() == 'PERSON':\n",
    "                if len(l)== 1:\n",
    "                    if l[0][0] in persons:\n",
    "                        pass\n",
    "                    else:\n",
    "                        persons.append(l[0][0])\n",
    "                else:\n",
    "                    if \" \".join(map(itemgetter(0), l)) in persons:\n",
    "                        pass\n",
    "                    else:\n",
    "                        persons.append(\" \".join(map(itemgetter(0), l)).strip(\"*\"))\n",
    "   \n",
    "\n",
    "    for o in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))):\n",
    "        if isinstance(o,nltk.tree.Tree):\n",
    "            if o.label() == 'ORGANIZATION':\n",
    "                if len(o)== 1:\n",
    "                    if o[0][0] in organizations:\n",
    "                        pass\n",
    "                    else:\n",
    "                        organizations.append(o[0][0])\n",
    "                else:\n",
    "                    if \" \".join(map(itemgetter(0), o)) in organizations:\n",
    "                        pass\n",
    "                    else:\n",
    "                        organizations.append(\" \".join(map(itemgetter(0), o)).strip(\"*\"))\n",
    "    \n",
    "\n",
    "    for o in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))):\n",
    "        if isinstance(o,nltk.tree.Tree):\n",
    "            if o.label() == 'LOCATION':\n",
    "                if len(o)== 1:\n",
    "                    if o[0][0] in locations:\n",
    "                        pass\n",
    "                    else:\n",
    "                        locations.append(o[0][0])\n",
    "                else:\n",
    "                    if \" \".join(map(itemgetter(0), o)) in locations:\n",
    "                        pass\n",
    "                    else:\n",
    "                        locations.append(\" \".join(map(itemgetter(0), o)).strip(\"*\"))\n",
    "    \n",
    "    for e in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))):\n",
    "        if isinstance(o,nltk.tree.Tree):\n",
    "            if o.label() == 'GPE':\n",
    "                if len(o)== 1:\n",
    "                    if o[0][0] in genpurp:\n",
    "                        pass\n",
    "                    else:\n",
    "                        genpurp.append(o[0][0])\n",
    "                else:\n",
    "                    if \" \".join(map(itemgetter(0), o)) in genpurp:\n",
    "                        pass\n",
    "                    else:\n",
    "                        genpurp.append(\" \".join(map(itemgetter(0), o)).strip(\"*\"))\n",
    "                        \n",
    "       \n",
    "\n",
    "\n",
    "    results = {}\n",
    "    results['persons']=persons\n",
    "    results['organizations']=organizations\n",
    "    results['locations']=locations\n",
    "    results['genpurp'] = genpurp\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genpurp': [],\n",
       " 'locations': [],\n",
       " 'organizations': [u'Generation', u'Stanford University'],\n",
       " 'persons': [u'Timeline',\n",
       "  u'Tim Althoff*',\n",
       "  u'Xin Luna Dong',\n",
       "  u'Kevin Murphy',\n",
       "  u'Safa Alai',\n",
       "  u'Van Dang',\n",
       "  u'Wei Zhang',\n",
       "  u'Stanford',\n",
       "  u'Mountain View']}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltktreelist(p19['top'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normal case test, abstract gold\n",
    "failids = []\n",
    "docnum = 'p3.txt'\n",
    "text=kddcorpus.raw(docnum)\n",
    "full = True\n",
    "section = \"abstract\"\n",
    "if full == True: \n",
    "    text = kddcorpus.raw(fileid)\n",
    "    if section == \"abstract\":\n",
    "        section1=[\"ABSTRACT\", \"Abstract \"]\n",
    "        target = \"\"   \n",
    "        section2=[\"Categories and Subject Descriptors\",\"Categories & Subject Descriptors\",\"Keywords\",\"INTRODUCTION\"]\n",
    "        for fileid in kddcorpus.fileids():\n",
    "            text = kddcorpus.raw(fileid)\n",
    "\n",
    "\n",
    "            for sect1 in section1:\n",
    "                for sect2 in section2:\n",
    "                    part1= \"(?<=\"+str(sect1)+\")(.+)\"\n",
    "                    part2 = \"(?=\"+str(sect2)+\")\"\n",
    "                    p = re.compile(part1+part2)\n",
    "                    try:\n",
    "                        target=p.search(re.sub('[\\s]',\" \",text)).group()\n",
    "                        if len(target) > 50:\n",
    "\n",
    "                            print [fileid,len(target),len(text)]\n",
    "                            break\n",
    "                        else:\n",
    "                            print fileid,\"Failed\"\n",
    "                            pass\n",
    "                    except AttributeError:\n",
    "                        pass                             \n",
    "else:\n",
    "    \n",
    "    section = \"abstract\"\n",
    "    if section == \"abstract\":\n",
    "        section1=[\"ABSTRACT\", \"Abstract \"]\n",
    "        target = \"\"   \n",
    "        section2=[\"Categories and Subject Descriptors\",\"Categories & Subject Descriptors\",\"Keywords\",\"INTRODUCTION\"]\n",
    "        for sect1 in section1:\n",
    "            for sect2 in section2:\n",
    "                part1= \"(?<=\"+str(sect1)+\")(.+?)\"\n",
    "                part2 = \"(?=\"+str(sect2)+\"[\\s]?)\"\n",
    "                p = re.compile(part1+part2)\n",
    "                try:\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group()\n",
    "                    if len(target) > 50:\n",
    "\n",
    "                        print [docnum,len(target),len(text)]\n",
    "                        break\n",
    "                    else:\n",
    "                        print fileid,\"Failed\"\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    pass      \n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# completed gold standard for keywords, got all of them..no stragglers\n",
    "docnum = 'p29.txt'\n",
    "text = kddcorpus.raw(docnum)\n",
    "failids = []\n",
    "full = True\n",
    "section = \"keywords\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid)\n",
    "        if section == \"keywords\":\n",
    "            section1=\"Keywords\"\n",
    "            target = \"\"   \n",
    "            section2=[\"Bio\",\"1.  INTRODUCTION  \",\"1.  INTROD \",\"1. MOTIVATION\",\"Permission to make \",\"1.MOTIVATION\",'1.Motivation' ]\n",
    "        \n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    if len(target) >50:\n",
    "                        if len(target) > 300:\n",
    "                            target = target[:200]\n",
    "                        else:\n",
    "                            target = target\n",
    "                        \n",
    "                        print [fileid,target.strip(),len(text)]\n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        failids.append(fileid)\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "else:\n",
    "    section = \"keywords\"\n",
    "    \n",
    "    if section == \"keywords\":\n",
    "        section1=\"Keywords\"\n",
    "        target = \"\"   \n",
    "        section2=[\"Bio\",\"1.  INTRODUCTION  \",\"1.  INTROD \",\"1. MOTIVATION\",\"Permission to make \",\"1.MOTIVATION\",'1.Motivation' ]\n",
    "        \n",
    "        part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "        for sect in section2:\n",
    "            try:\n",
    "                part2 = \"(?=\"+str(sect)+\")\"\n",
    "                p=re.compile(part1+part2)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                if target > 3:                 \n",
    "                    break                  \n",
    "            except:\n",
    "                pass\n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "failids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# completed gold standard for references, counts number of references and does \"word per reference\" score\n",
    "docnum=\"p19.txt\"\n",
    "failids = []\n",
    "text=kddcorpus.raw(docnum)\n",
    "\n",
    "full = False\n",
    "section = \"references\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid)\n",
    "        if section == \"references\":\n",
    "            section1=[\"REFERENCES\"] \n",
    "            target = \"\"   \n",
    "\n",
    "            \n",
    "\n",
    "            for sect in section1:\n",
    "                try:\n",
    "                    part1= \"(?<=\"+sect+\")(.+)\"\n",
    "                    p=re.compile(part1)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    if len(target) > 50:\n",
    "\n",
    "                        # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                        try:\n",
    "                            refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                        except:\n",
    "                            print \"This file does not appear to have a references section\"\n",
    "                            pass\n",
    "                        \n",
    "\n",
    "                        print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                        break\n",
    "                    else:\n",
    "\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "\n",
    "    \n",
    "\n",
    "# to return output from one document\n",
    "else:\n",
    "    ans = {}\n",
    "    failids=[]\n",
    "    text = kddcorpus.raw(docnum)\n",
    "    if section == \"references\":\n",
    "        section1=[\"REFERENCES\"] \n",
    "        target = \"\"   \n",
    "        for sect in section1:\n",
    "            try:\n",
    "                part1= \"(?<=\"+sect+\")(.+)\"\n",
    "                p=re.compile(part1)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                if len(target) > 50:\n",
    "                    # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                    try:\n",
    "                        refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                    except:\n",
    "                        print \"This file does not appear to have a references section\"\n",
    "                        pass\n",
    "\n",
    "\n",
    "                    print [docnum,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                    break\n",
    "                else:\n",
    "\n",
    "                    pass\n",
    "            except AttributeError:\n",
    "                failids.append(docnum)\n",
    "                pass\n",
    "\n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(failids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gold standard to get top section\n",
    "\n",
    "from emailextractor import file_to_str, get_emails # paste code to .py file from following link and save within your environment path to call it: https://gist.github.com/dideler/5219706\n",
    "\n",
    "failids = []\n",
    "docnum =\"p1.txt\"\n",
    "text=kddcorpus.raw(docnum)\n",
    "\n",
    "full = True\n",
    "section = \"top\"\n",
    "if full == True:\n",
    "    if section == 'top':\n",
    "        section = [\"ABSTRACT\",\"Abstract\",\"Bio\",\"Panel Summary\"]\n",
    "        for fileid in kddcorpus.fileids():\n",
    "            text = kddcorpus.raw(fileid)\n",
    "            for sect in section:\n",
    "                try:\n",
    "                    part1=\"(.+)(?=\"+s+\")\"\n",
    "                    #print \"re.compile\"+\"(\"+part1+\")\"\n",
    "                    p=re.compile(part1)\n",
    "                    target = p.search(re.sub('[\\s]',\" \", text)).group()\n",
    "                    #print docnum,len(target),len(text)\n",
    "\n",
    "                    emails = tuple(get_emails(target))\n",
    "                    print [fileid,len(target),len(text), emails]\n",
    "                    break\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "\n",
    "                               \n",
    "        # to return output from one document\n",
    "else:\n",
    "\n",
    "    failids=[]\n",
    "    text = kddcorpus.raw(docnum)\n",
    "\n",
    "    if section == \"top\":\n",
    "        section = [\"ABSTRACT\",\"Abstract\",\"Bio\",\"Panel Summary\"]\n",
    "        docnum=\"p19.txt\"\n",
    "        text = kddcorpus.raw(docnum)\n",
    "        for sect in section:\n",
    "            try:\n",
    "                part1=\"(.+)(?=\"+s+\")\"\n",
    "                print \"re.compile\"+\"(\"+part1+\")\"\n",
    "                p=re.compile(part1)\n",
    "                target = p.search(re.sub('[\\s]',\" \", text)).group()\n",
    "                print docnum,len(target),len(text)\n",
    "\n",
    "                emails = tuple(get_emails(target))\n",
    "\n",
    "                print [fileid,len(target),len(text),emails]\n",
    "                break\n",
    "\n",
    "            except AttributeError:\n",
    "                failids.append(fileid)\n",
    "                pass\n",
    "\n",
    "\n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "failids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:violet\">Drawing Board/Assembly Line</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attempting function with gold keywords....\n",
    "\n",
    "def keypull(docnum=None,section='keywords',full = False):\n",
    "    \n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "\n",
    "        \n",
    "\n",
    "        # to return output from entire corpus\n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid).lower()\n",
    "                if section == \"keywords\":\n",
    "                    section1=\"keywords\"\n",
    "                    target = \"\"   \n",
    "                    section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"(1. tutorial )\",\" permission to make \",\"  permission to make\",\"(  permission to make digital )\",\"    bio  \",\"abstract:  \",\"1.motivation\" ]\n",
    "\n",
    "                    part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "                    for sect in section2:\n",
    "                        try:\n",
    "                            part2 = \"(?=\"+str(sect)+\")\"\n",
    "                            p=re.compile(part1+part2)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                            if len(target) >50:\n",
    "                                if len(target) > 300:\n",
    "                                    target = target[:200]\n",
    "                                else:\n",
    "                                    target = target\n",
    "\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"keywords\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                #print [fileid,len(target),len(text)]\n",
    "                                break\n",
    "                            else:\n",
    "                                if len(target)==0:\n",
    "                                     failids.append(fileid)   \n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "            set(failids)\n",
    "            return ans\n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        if full == False:\n",
    "            if section == \"keywords\":\n",
    "                section1=\"keywords\"\n",
    "                target = \"\"   \n",
    "                section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"permission to make \",\"1.motivation\" ]\n",
    "\n",
    "                part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "                for sect in section2:\n",
    "                    try:\n",
    "                        part2 = \"(?=\"+str(sect)+\")\"\n",
    "                        p=re.compile(part1+part2)\n",
    "                        target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                        if len(target) >50:\n",
    "                            if len(target) > 300:\n",
    "                                target = target[:200]\n",
    "                            else:\n",
    "                                target = target\n",
    "                            ans[docnum]={}\n",
    "                            ans[docnum][\"keywords\"]=target.strip()\n",
    "                            ans[docnum][\"charcount\"]=len(target)\n",
    "                            break                  \n",
    "                    except:\n",
    "                        pass\n",
    "    return ans\n",
    "    return failids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attempting function with gold abstracts...Normal case done\n",
    "\n",
    "def abpull(docnum=None,section='abstract',full = False):\n",
    "    \n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        # to return output from entire corpus\n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid)\n",
    "                if section == \"abstract\":\n",
    "                    section1=[\"ABSTRACT\", \"Abstract \"]\n",
    "                    target = \"\"   \n",
    "                    section2=[\"Categories and Subject Descriptors\",\"Categories & Subject Descriptors\",\"Keywords\",\"INTRODUCTION\"]\n",
    "                    for fileid in kddcorpus.fileids():\n",
    "                        text = kddcorpus.raw(fileid)\n",
    "\n",
    "\n",
    "                        for sect1 in section1:\n",
    "                            for sect2 in section2:\n",
    "                                part1= \"(?<=\"+str(sect1)+\")(.+)\"\n",
    "                                part2 = \"(?=\"+str(sect2)+\")\"\n",
    "                                p = re.compile(part1+part2)\n",
    "                                try:\n",
    "                                    target=p.search(re.sub('[\\s]',\" \",text)).group()\n",
    "                                    if len(target) > 50:\n",
    "                                        ans[str(fileid)]={}\n",
    "                                        ans[str(fileid)][\"abstract\"]=target.strip()\n",
    "                                        ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                        #print [fileid,len(target),len(text)]\n",
    "                                        break\n",
    "                                    else:\n",
    "                                        failids.append(fileid)\n",
    "                                        pass\n",
    "                                except AttributeError:\n",
    "                                    pass \n",
    "                \n",
    "            return ans\n",
    "                              \n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum).lower()\n",
    "        if section == \"abstract\":\n",
    "            section1=[\"ABSTRACT\", \"Abstract \"]\n",
    "            target = \"\"   \n",
    "            section2=[\"Categories and Subject Descriptors\",\"Categories & Subject Descriptors\",\"Keywords\",\"INTRODUCTION\"]\n",
    "            for sect1 in section1:\n",
    "                for sect2 in section2:\n",
    "                    part1= \"(?<=\"+str(sect1)+\")(.+?)\"\n",
    "                    part2 = \"(?=\"+str(sect2)+\"[\\s]?)\"\n",
    "                    p = re.compile(part1+part2)\n",
    "                    try:\n",
    "                        target=p.search(re.sub('[\\s]',\" \",text)).group()\n",
    "                        if len(target) > 50:\n",
    "                            ans[str(docnum)]={}\n",
    "                            ans[str(docnum)][\"abstract\"]=target.strip()\n",
    "                            ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                            #print [docnum,len(target),len(text)]\n",
    "                            break\n",
    "                        else:\n",
    "                            failids.append(docnum)\n",
    "                            pass\n",
    "                    except AttributeError:\n",
    "                        pass\n",
    "        return ans\n",
    "        return failids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abpull('p19.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attempting function with gold top section...Normal case done\n",
    "\n",
    "def toppull(docnum=None,section='top',full = False):\n",
    "    from emailextractor import file_to_str, get_emails # paste code to .py file from following link and save within your environment path to call it: https://gist.github.com/dideler/5219706\n",
    "\n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        # to return output from entire corpus\n",
    "        \n",
    "        if full == True:\n",
    "            if section == 'top':\n",
    "                section = [\"ABSTRACT\",\"Abstract\",\"Bio\",\"Panel Summary\"]\n",
    "                for fileid in kddcorpus.fileids():\n",
    "                    text = kddcorpus.raw(fileid)\n",
    "                    for sect in section:\n",
    "                        try:\n",
    "                            part1=\"(.+)(?=\"+sect+\")\"\n",
    "                            #print \"re.compile\"+\"(\"+part1+\")\"\n",
    "                            p=re.compile(part1)\n",
    "                            target = p.search(re.sub('[\\s]',\" \", text)).group()\n",
    "                            #print docnum,len(target),len(text)\n",
    "\n",
    "                            emails = tuple(get_emails(target))\n",
    "                            ans[str(fileid)]={}\n",
    "                            ans[str(fileid)][\"top\"]=target.strip()\n",
    "                            ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                            ans[str(fileid)][\"emails\"]=emails\n",
    "                            #print [fileid,len(target),len(text)]\n",
    "                            break\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "        return ans\n",
    "        return failids\n",
    "                               \n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum)\n",
    "\n",
    "        if section == \"top\":\n",
    "            section = [\"ABSTRACT\",\"Abstract\",\"Bio\",\"Panel Summary\"]\n",
    "            text = kddcorpus.raw(docnum)\n",
    "            for sect in section:\n",
    "                try:\n",
    "                    part1=\"(.+)(?=\"+sect+\")\"\n",
    "                    #print \"re.compile\"+\"(\"+part1+\")\"\n",
    "                    p=re.compile(part1)\n",
    "                    target = p.search(re.sub('[\\s]',\" \", text)).group()\n",
    "                    #print docnum,len(target),len(text)\n",
    "\n",
    "                    emails = tuple(get_emails(target))\n",
    "                    ans[str(docnum)]={}\n",
    "                    ans[str(docnum)][\"top\"]=target.strip()\n",
    "                    ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                    ans[str(docnum)][\"emails\"]=emails\n",
    "                    #print [fileid,len(target),len(text)]\n",
    "                    break\n",
    "\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "\n",
    "        return ans\n",
    "        return failids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attempting function with gold references section\n",
    "\n",
    "def refpull(docnum=None,section='references',full = False):\n",
    "    \n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum)\n",
    "        # to return output from entire corpus\n",
    "        \n",
    "        \n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid)\n",
    "                if section == \"references\":\n",
    "                    section1=[\"REFERENCES\"] \n",
    "                    target = \"\"   \n",
    "\n",
    "\n",
    "\n",
    "                    for sect in section1:\n",
    "                        try:\n",
    "                            part1= \"(?<=\"+sect+\")(.+)\"\n",
    "                            p=re.compile(part1)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                            if len(target) > 50:\n",
    "\n",
    "                                # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                                try:\n",
    "                                    refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                                except:\n",
    "                                    print \"This file does not appear to have a references section\"\n",
    "                                    pass\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"references\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                ans[str(fileid)][\"refcount\"]= refnum\n",
    "                                ans[str(fileid)][\"wordperRef\"]=round(float(len(nltk.word_tokenize(text)))/float(refnum))\n",
    "                                #print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                                break\n",
    "                            else:\n",
    "\n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "\n",
    "            return ans\n",
    "            return failids\n",
    "                              \n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum)\n",
    "        \n",
    "        if section == \"references\":\n",
    "            section1=[\"REFERENCES\"] \n",
    "            target = \"\"   \n",
    "            for sect in section1:\n",
    "                try:\n",
    "                    part1= \"(?<=\"+sect+\")(.+)\"\n",
    "                    p=re.compile(part1)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    if len(target) > 50:\n",
    "                        # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                        try:\n",
    "                            refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                        except:\n",
    "                            print \"This file does not appear to have a references section\"\n",
    "                            pass\n",
    "                        ans[str(docnum)]={}\n",
    "                        ans[str(docnum)][\"references\"]=target.strip()\n",
    "                        ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                        ans[str(docnum)][\"refcount\"]= refnum\n",
    "                        ans[str(docnum)][\"wordperRef\"]=float(len(nltk.word_tokenize(text)))/float(refnum)\n",
    "\n",
    "\n",
    "                        #print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                        break\n",
    "                    else:\n",
    "\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(docnum)\n",
    "                    pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        return ans\n",
    "        return failids\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Testing Station</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltkstandard_top_entities_p19 = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(p19['top'])))\n",
    "nltkstandard_top_entities_p19 = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(p19['references'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltkstandard_top_entities_p19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "persons, organizations, locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news = \"Iran\\’s supreme leader warned Sunday that Saudi Arabia would face divine vengeance for the execution of an outspoken Shiite cleric, a day after Iranian protesters ransacked the Saudi Embassy in Tehran in outrage over the execution.\\“God\\’s hand of retaliation will grip the neck of Saudi politicians,” said the supreme leader, Ayatollah Ali Khamenei, in comments reported on his official website.  Despite the rhetoric, however, the Iranians seemed to be taking steps to prevent the dispute from escalating further. Forty Iranians were arrested on Saturday night for the violence — a sign that the authorities were trying to keep public outrage from getting out of control.Iran\\’s president, Hassan Rouhani, on Sunday condemned the execution, but also said that the attacks on the Saudi embassy in Tehran and the Saudi consulate in Mashhad had damaged Iran’s reputation.Firefighters battled a blaze at the Saudi Embassy in Tehran on Saturday after Iranian protesters entered the building.Iranian Protesters Ransack Saudi Embassy After Execution of Shiite ClericJAN. 2, 2016\\“We do not allow rogue groups to commit illegal actions and damage the holy reputation of Islamic Republic of Iran,\\” he said in a statement. \\“What happened last night in Mashhad and Tehran and collateral damages in Saudi consulate and Embassy is not acceptable and justifiable.\\”\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unicodedata.normalize('NFKD', news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter(lambda x: x in string.printable,unicodedata.normalize('NFKD', (unicode(news,errors='ignore'))).encode('ascii','ignore').decode('unicode_escape').encode('ascii','ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsent = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize((news.decode('unicode_escape').encode('ascii','ignore')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.chunk.api.ChunkParserI(nltk.pos_tag(nltk.word_tokenize((news.decode('unicode_escape').encode('ascii','ignore')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.chunk.api.ChunkParserI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chiefs = nltk.corpus.PlaintextCorpusReader(chiefpath, 'KCChiefs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chiefs.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw = chiefs.raw('KCChiefs').decode('unicode_escape').encode('ascii','ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw ='This post was originally titled, \"Should we root for the Chiefs to lose?\", and was written when our Kansas City Chiefs were 3-5.\\n\\nThat may seem like an eternity ago, but there was a general sense during the week before the Chiefs played the Broncos in Denver that the season was lost. The focus here at Arrowhead Pride went quickly from the Super Bowl to mock drafts. Arguments that the Chiefs just tank the season for a better draft pick were seriously made. It seemed like a waste to even win two straight if it meant improving to a still-worthless 3-5.\\n\\nAfter all, the Andy Reid era Chiefs had never beaten Denver. And, even if they did, what were they supposed to do, go on a nine-game win streak and make the playoffs?\\n\\nStill, even within that atmosphere of capitulation as the Chiefs headed into their Week 9 bye to prepare for a trip to Denver, no one wanted them to lose that game. With the hated enemies one Sunday away, publishing a post asking for patience and a little faith in our 2015 Kansas City Chiefs seemed silly. AP was unanimous about going 1- 0, even if that meant only for one week. We could get back to rooting for the Chiefs to go 4-12 next week.\\n\\nI wrote an article prior to the Denver game saying the Chiefs are a better team than Denver and could beat them on the road. It ended with talk of the season\\'s rough start and provided optimism moving forward:\\n\\nBut that should make winning all the more awesome when it happens, right? And the Chiefs are very, very close to winning. This is a good team finally on the right side of an unfortunate first half of football.\\n\\nSo I tucked away this post you\\'re about to read. And I waited. And waited. And kept on waiting. The Chiefs kept on winning. And the Chiefs kept on winning some more. They kept on winning until this post, written near the nadir of 2015, was suddenly arguing for a perspective that everyone had since acquired.\\n\\nBut now Week 17 is upon us, and Kansas City is guaranteed at least two more games. The game will occur in Denver, Colorado at 5:30 pm.  This post could wait no longer. Take it as a reminder of how far our Chiefs have come since Week 9.\\n\\nI present, unedited from its original text, an argument for why we should stop calling for Andy Reid\\'s head.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stner.tag(raw.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'locations': [[u'Kansas City Chiefs'],\n",
       "  [u'Denver'],\n",
       "  [u'Arrowhead'],\n",
       "  [u'Denver'],\n",
       "  [u'Denver'],\n",
       "  [u'Kansas City'],\n",
       "  [u'Colorado']],\n",
       " 'organizations': [],\n",
       " 'persons': [[u'Andy Reid']]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_continuous_chunks(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I-ORG([u'Chiefs']),\n",
       " I-ORG([u'Kansas', u'City', u'Chiefs']),\n",
       " I-ORG([u'Chiefs']),\n",
       " I-ORG([u'Broncos']),\n",
       " I-LOC([u'Denver']),\n",
       " I-ORG([u'Arrowhead', u'Pride']),\n",
       " I-ORG([u'Super']),\n",
       " I-ORG([u'Chiefs']),\n",
       " I-PER([u'Andy', u'Reid']),\n",
       " I-ORG([u'Chiefs']),\n",
       " I-ORG([u'Denver']),\n",
       " I-ORG([u'Chiefs']),\n",
       " I-LOC([u'Denver']),\n",
       " I-ORG([u'Kansas', u'City', u'Chiefs']),\n",
       " I-ORG([u'Chiefs']),\n",
       " I-LOC([u'Denver']),\n",
       " I-ORG([u'Chiefs']),\n",
       " I-ORG([u'Denver']),\n",
       " I-ORG([u'Chiefs']),\n",
       " I-ORG([u'Chiefs']),\n",
       " I-ORG([u'Chiefs']),\n",
       " I-LOC([u'Kansas']),\n",
       " I-ORG([u'Kansas', u'City']),\n",
       " I-LOC([u'Denver', u',', u'Colorado']),\n",
       " I-ORG([u'Chiefs']),\n",
       " I-PER([u'Andy', u\"Reid's\"])]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from polyglot.text import Text\n",
    "Text(raw).entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string = \"I am the man, you will,do what, I say\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am the man', 'and', ' you will ', 'do what I say']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.split(';|,',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am the man', ' you will', 'do what, I say']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.split(\",\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-44023faa7ab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Tim Althoff*\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Billy the Kid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "name = [\"Tim Althoff*\", \"Billy the Kid\"]\n",
    "name.strip(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
