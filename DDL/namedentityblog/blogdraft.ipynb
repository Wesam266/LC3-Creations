{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick Survey and Comparison of Open Source Named Entity Extractor Tools for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity extraction is a core subtask of building knowledge from semi/unstructured text sources<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup>.  Considering recent increases in computing power and decreases in the costs of data storage, data scientists and developers can build large knowledge bases that contain millions of entities and hundreds of millions of facts about them.  These knowledge bases are key contributors to intelligence computer behavior<sup><a href=\"#fn2\" id=\"ref2\">2</a></sup>.  Therefore, named entity extraction is at the core of several popular technologies such as smart assistants ([Siri](http://www.apple.com/ios/siri/), [Google Now](https://www.google.com/landing/now/)), machine reading, and deep interpretation of natural language<sup><a href=\"#fn3\" id=\"ref3\">3</a></sup>.\n",
    "\n",
    "With a realization of how essential it is to recognize information units like names, including person, organization and location names, and numeric expressions including time, date, money\n",
    "and percent expressions, several questions come to mind.  How do you perform named entity extraction, which is formally called “[Named Entity Recognition and Classification (NERC)](https://benjamins.com/catalog/bct.19)”?  What tools are out there?  How can you evaluate their performance?  And most important, what works with Python (shamelessly exposing my bias)?  \n",
    "\n",
    "This post will survey openly available NERC tools and compare the results against hand labeled data for precision, accuracy, and recall.  The tools and basic information extraction principles in this discussion begin the process of structuring unstructured data.\n",
    "\n",
    "We will specifically learn to:\n",
    "1. follow the data science pipeline (see image below)\n",
    "2. prepare semistructured natural language data for ingest using regex\n",
    "3. create a custom corpus in [Natural Language Toolkit](http://www.nltk.org/) \n",
    "4. use a suite of openly available NERC tools to extract entities and store in json format \n",
    "5. compare the performance of NERC tools on our corpus\n",
    "\n",
    "<br>\n",
    "<a href=\"#pipe\" id=\"pipeline\"><center><h3>The Data Science Pipeline:<br>Georgetown Data Science Certificate Program</h3></center></a>\n",
    "<div class=\"image\">\n",
    "\n",
    "      <img src=\"./files/data_science_pipeline.png\" alt=\"Data Science Pipeline\" height=\"300\" width=\"450\" top:\"35\" left:\"170\" />\n",
    "      \n",
    "      \n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data: Peer Reviewed Journals and Keynote Speaker Abstracts from KDD 2014 and 2015\n",
    "\n",
    "Before delving into the pipeline, we need a good dataset.  Jason Brownlee of www.machinelearningmastery.com had some good suggestions in his [August 2015 article](http://machinelearningmastery.com/practice-machine-learning-with-small-in-memory-datasets-from-the-uci-machine-learning-repository/) on picking a dataset for machine learning exercises:  \n",
    "\n",
    "* **Real-World**: The datasets should be drawn from the real world (rather than being contrived). This will keep them interesting and introduce the challenges that come with real data.\n",
    "\n",
    "* **Small**: The datasets need to be small so that you can inspect and understand them and that you can run many models quickly to accelerate your learning cycle.\n",
    "\n",
    "* **Well-Understood**: There should be a clear idea of what the data contains, why it was collected, what the problem is that needs to be solved so that you can frame your investigation.\n",
    "\n",
    "* **Baseline**: It is also important to have an idea of what algorithms are known to perform well and the scores they achieved so that you have a useful point of comparison. This is important when you are getting started and learning because you need quick feedback as to how well you are performing (close to state-of-the-art or something is broken).\n",
    "\n",
    "* **Plentiful**: You need many datasets to choose from, both to satisfy the traits you would like to investigate and (if possible) your natural curiosity and interests. \n",
    "\n",
    "Luckily, we have a dataset that meets nearly all of these requirements.  I attended the Knowledge Discovery and Data Mining (KDD) conferences in [New York City (2014)](http://www.kdd.org/kdd2014/) and [Sydney, Australia (2015)](http://www.kdd.org/kdd2015/).  Both years, attendees received a USB with the conference proceedings.  Each repository contains over 230 peer reviewed journal articles and keynote speaker abstracts on data mining, knowledge discovery, big data, data science and their applications. The full conference proceedings can be purchased for \\$60 at the [Association for Computing Machinery's Digital Library](https://dl.acm.org/purchase.cfm?id=2783258&CFID=740512201&CFTOKEN=34489585) (includes ACM membership). This post will work with a dataset that is equivalent to the conference proceedings.  It's important to note that this dataset recreates a real word data science exercise that is instructive of big data problems.  We will take semi-structured data (PDF journal articles and abstracts in publication format), strip text from the files, and add more structure to the data that would facilitate follow on analysis. \n",
    "\n",
    "<blockquote cite=\"https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/KDDwebscrape.ipynb\">\n",
    "Interested parties looking for a free option can use the <a href=\"https://pypi.python.org/pypi/beautifulsoup4/4.4.1\">beautifulsoup</a> and <a href=\"https://pypi.python.org/pypi/requests/2.9.1\">request</a> libraries to scrape the <a href=\"http://dl.acm.org/citation.cfm?id=2785464&CFID=740512201&CFTOKEN=3448958\">ACM website for KDD 2015 conference data</a> that can be used in natural language processing pipelines.  I have some <a href=\"https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/KDDwebscrape.ipynb\">skeleton web scraping code</a> to generate lists of all abstracts, author names, and journal/keynote address titles.    \n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration: Getting the number of files, and file type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored locally in the following directory:\n",
    "```python\n",
    ">>> import os\n",
    ">>> print os.getcwd()\n",
    "/Users/linwood/Desktop/KDD_15/docs\n",
    "```\n",
    "Let's explore the number of files we have and naming conventions. We begin with the administrative tasks of loading modules, establishing paths, etc.  \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Importing what we need\n",
    "#**********************************************************************\n",
    "import os\n",
    "import time\n",
    "from os import walk\n",
    "\n",
    "#**********************************************************************\n",
    "# Administrative code to set the path for file loading\n",
    "#**********************************************************************\n",
    "\n",
    "path        = os.path.abspath(os.getcwd())\n",
    "TESTDIR     = os.path.normpath(os.path.join(os.path.expanduser(\"~\"),\"Desktop\",\"KDD_15\",\"docs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Next we iterate over the files in the directory and store those names in the empty list we created called *files*.  We time the operation, print list with the file names and also print out the length of the list (gives number of target files).<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n",
      "\n",
      "253\n",
      "\n",
      "[p1.pdf, p1005.pdf, p1015.pdf, p1025.pdf, p1035.pdf, p1045.pdf, p1055.pdf, p1065.pdf, p1075.pdf, p1085.pdf, p109.pdf, p1095.pdf, p1105.pdf, p1115.pdf, p1125.pdf, p1135.pdf, p1145.pdf, p1155.pdf, p1165.pdf, p1175.pdf, p1185.pdf, p119.pdf, p1195.pdf, p1205.pdf, p1215.pdf, p1225.pdf, p1235.pdf, p1245.pdf, p1255.pdf, p1265.pdf, p1275.pdf, p1285.pdf, p129.pdf, p1295.pdf, p1305.pdf, p1315.pdf, p1325.pdf, p1335.pdf, p1345.pdf, p1355.pdf, p1365.pdf, p1375.pdf, p1385.pdf, p139.pdf, p1395.pdf, p1405.pdf, p1415.pdf, p1425.pdf, p1435.pdf, p1445.pdf, p1455.pdf, p1465.pdf, p1475.pdf, p1485.pdf, p149.pdf, p1495.pdf, p1503.pdf, p1513.pdf, p1523.pdf, p1533.pdf, p1543.pdf, p1553.pdf, p1563.pdf, p1573.pdf, p1583.pdf, p159.pdf, p1593.pdf, p1603.pdf, p1621.pdf, p1623.pdf, p1625.pdf, p1627.pdf, p1629.pdf, p1631.pdf, p1633.pdf, p1635.pdf, p1637.pdf, p1639.pdf, p1641.pdf, p1651.pdf, p1661.pdf, p1671.pdf, p1681.pdf, p169.pdf, p1691.pdf, p1701.pdf, p1711.pdf, p1721.pdf, p1731.pdf, p1741.pdf, p1751.pdf, p1759.pdf, p1769.pdf, p1779.pdf, p1789.pdf, p179.pdf, p1799.pdf, p1809.pdf, p1819.pdf, p1829.pdf, p1839.pdf, p1849.pdf, p1859.pdf, p1869.pdf, p1879.pdf, p1889.pdf, p189.pdf, p1899.pdf, p19.pdf, p1909.pdf, p1919.pdf, p1929.pdf, p1939.pdf, p1949.pdf, p1959.pdf, p1969.pdf, p1979.pdf, p1989.pdf, p199.pdf, p1999.pdf, p2009.pdf, p2019.pdf, p2029.pdf, p2039.pdf, p2049.pdf, p2059.pdf, p2069.pdf, p2079.pdf, p2089.pdf, p209.pdf, p2099.pdf, p2109.pdf, p2119.pdf, p2127.pdf, p2137.pdf, p2147.pdf, p2157.pdf, p2167.pdf, p2177.pdf, p2187.pdf, p219.pdf, p2197.pdf, p2207.pdf, p2217.pdf, p2227.pdf, p2237.pdf, p2247.pdf, p2257.pdf, p2267.pdf, p2277.pdf, p2287.pdf, p229.pdf, p2297.pdf, p2307.pdf, p2309.pdf, p2311.pdf, p2313.pdf, p2315.pdf, p2317.pdf, p2319.pdf, p2321.pdf, p2323.pdf, p2325.pdf, p2327.pdf, p2329.pdf, p239.pdf, p249.pdf, p259.pdf, p269.pdf, p279.pdf, p289.pdf, p29.pdf, p299.pdf, p3.pdf, p309.pdf, p319.pdf, p329.pdf, p339.pdf, p349.pdf, p359.pdf, p369.pdf, p379.pdf, p387.pdf, p39.pdf, p397.pdf, p407.pdf, p417.pdf, p427.pdf, p437.pdf, p447.pdf, p457.pdf, p467.pdf, p477.pdf, p487.pdf, p49.pdf, p497.pdf, p5.pdf, p507.pdf, p517.pdf, p527.pdf, p537.pdf, p547.pdf, p557.pdf, p567.pdf, p577.pdf, p587.pdf, p59.pdf, p597.pdf, p607.pdf, p617.pdf, p627.pdf, p635.pdf, p645.pdf, p655.pdf, p665.pdf, p675.pdf, p685.pdf, p69.pdf, p695.pdf, p7.pdf, p705.pdf, p715.pdf, p725.pdf, p735.pdf, p745.pdf, p755.pdf, p765.pdf, p775.pdf, p785.pdf, p79.pdf, p805.pdf, p815.pdf, p825.pdf, p835.pdf, p845.pdf, p855.pdf, p865.pdf, p875.pdf, p885.pdf, p89.pdf, p895.pdf, p9.pdf, p905.pdf, p915.pdf, p925.pdf, p935.pdf, p945.pdf, p955.pdf, p965.pdf, p975.pdf, p985.pdf, p99.pdf, p995.pdf]\n"
     ]
    }
   ],
   "source": [
    "# Establish an empty list to append filenames as we iterate over the directory with filenames\n",
    "files = []\n",
    "\n",
    "%time\n",
    "start_time = time.time()\n",
    "\n",
    "#**********************************************************************\n",
    "# Core \"workerbee\" code for this section to iterate over directory files\n",
    "#**********************************************************************\n",
    "\n",
    "# Iterate over the directory of filenames and add to list.  Inspection shows our target filenames begin with 'p' and end with 'pdf'\n",
    "for dirName, subdirList, fileList in os.walk(TESTDIR):\n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            files.append(fileName)\n",
    "end_time = time.time()\n",
    "\n",
    "#**********************************************************************\n",
    "# Output\n",
    "#**********************************************************************\n",
    "print\n",
    "print len(files) # Print the number of files\n",
    "print \n",
    "print '[%s]' % ', '.join(map(str, files)) # print the list of filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>There are 253 total files in the directory. We examine the pdf file in its rawest form to get an idea of the format. Here is one example:<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"./files/journalscreencap.png\" alt=\"Sample of Journal Format\" height=\"700\" width=\"700\" top:\"35\" left:\"170\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br><br>We learn a few things immediately. Our data is in PDF format and it's semistructured (follows journal article format with sections like \"abstract\", \"title\").  PDFs are a wonderful human readable presentation of data. But for data analyisis, they are extremely difficult to work with.  If you have an option to get the data BEFORE it was converted to or added to PDF, go for that option.  Save yourself the headache.  In this case however, we have no alternatives outside of the web scraping code linked above.  The web scraping code is imperfect because it is incomplete (only get abstracts and not full-text of journal ariticle) and unordered (multiple authors need to be aligned to specific articles).<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion: Stripping text from PDFs and creating a custom NLTK corpus\n",
    "\n",
    "The first step in the <href id=\"pipe\"><a href=\"#pipeline\" title=\"Jump back to data science pipeline graphic.\">data science pipeline</a> is to ingest our data.  We use several Python tools which include:\n",
    "\n",
    "* [pdfminer](https://pypi.python.org/pypi/pdfminer/) - this is the tool that makes it ALL happen.  It has a command line tool called \"pdf2text.py\" that extract text contents from a PDF. **This must be installed on your computer BEFORE executing this code**.  Visit the [pdfminer homepage](http://euske.github.io/pdfminer/index.html#pdf2txt) for instructions\n",
    "\n",
    "* [subprocess](https://docs.python.org/2/library/subprocess.html) - a standard library module that allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes.  In this excerise, we use it to invoke the pdf2texy.py command line tool within our code.  \n",
    "\n",
    "* [nltk](http://www.nltk.org/) - another work horse in this exercise.  The Natural Language ToolKit (NLTK) is one of Python's leading platforms to analyze natural language data.  The [NLTK Book](http://www.nltk.org/book/) provides practical guidance on how to handle just about any natural language preprocessing job.  \n",
    "\n",
    "* [string](https://docs.python.org/2/library/string.html) - used for variable substitutions and value formatting to strip non printable characters from the output of the text extracted from our journal article PDFs\n",
    "\n",
    "* [unicodedata](https://docs.python.org/2/library/unicodedata.html) - some unicode characters won't extract nicely. This library allows latin unicode characters to degrade gracefully into ASCII.\n",
    "\n",
    "We are now going to iterate over each file in our raw data directory, strip the text, and write the *.txt* file to newly created directory.  Then we will follow the instructions from [Section 1.9, Chapter 2 of NLTK's Book](http://www.nltk.org/book/ch02.html) to build a custom corpus from our text files.  Having our target documents loaded as an NLTK corpus brings the power of NLTK to our analysis goals.  Let's begin with administrative tasks such as loading modules and creating the necessary directories.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Importing what we need\n",
    "#**********************************************************************\n",
    "import string\n",
    "import unicodedata\n",
    "import subprocess\n",
    "import nltk\n",
    "import os, os.path\n",
    "import re\n",
    "\n",
    "#**********************************************************************\n",
    "# Create the directory we will write the .txt files to after stripping text\n",
    "#**********************************************************************\n",
    "\n",
    "corpuspath = os.path.normpath(os.path.expanduser('~/Desktop/KDD_corpus/'))\n",
    "if not os.path.exists(corpuspath):\n",
    "    os.mkdir(corpuspath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Now we are to the big task of stripping text from the PDFs.  In the code below, we walk down the directory, and strip text from the files with names that begin with 'p' and end with 'pdf'.  We use the *fileName* variable to name the files we write to disk.  This will come in handy when we load data into NLTK.  Keep in mind, this task takes the longest, so be prepared to wait a a few minutes depending on good your computer is.  If you are doing this in an environment where you can spin up compute resources, your time will be drastically reduced.  Let's begin.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Core code to iterate over files in the directory\n",
    "#**********************************************************************\n",
    "\n",
    "# We start from the code to iterate over the files\n",
    "%timeit\n",
    "for dirName, subdirList, fileList in os.walk(TESTDIR):\n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            if os.path.exists(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\"))):\n",
    "                pass\n",
    "            else:\n",
    "            \n",
    "            \n",
    "#**********************************************************************\n",
    "# This code strips the text from the PDFs\n",
    "#**********************************************************************\n",
    "                try:\n",
    "                    document = filter(lambda x: x in string.printable,unicodedata.normalize('NFKD', (unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore'))).encode('ascii','ignore').decode('unicode_escape').encode('ascii','ignore'))\n",
    "                except UnicodeDecodeError:\n",
    "                    document = unicodedata.normalize('NFKD', unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore')).encode('ascii','ignore')    \n",
    "\n",
    "                if len(document)<300:\n",
    "                    pass\n",
    "                else:\n",
    "                    # used this for assistance http://stackoverflow.com/questions/2967194/open-in-python-does-not-create-a-file-if-it-doesnt-exist\n",
    "                    if not os.path.exists(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\"))):\n",
    "                        file = open(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\")), 'w+')\n",
    "                        file.write(document)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "kddcorpus= nltk.corpus.PlaintextCorpusReader(corpuspath, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kddcorpus= nltk.corpus.PlaintextCorpusReader(corpuspath, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>This is a pretty big step.  We have a semi-structured data set in a format where we can query and analyze different pieces of data.  All of our data is loaded as an NLTK corpus, meaning we could try tons of techniques outlined in the [NLTK book](http://www.nltk.org/book/) or use the NLTK APIs to pass data into [scikit-learn machine learning pipelines for text](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) (maybe for a later blog). Let's see how many words (including stop words) we have in our entire corpus.  <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2795267\n"
     ]
    }
   ],
   "source": [
    "wordcount = 0\n",
    "for fileid in kddcorpus.fileids():\n",
    "    wordcount += len(kddcorpus.words(fileid))\n",
    "print wordcount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>To begin exploration of regular expressions, let's extract 'good enough' titles from a few of the documents.  For help on regex, visit https://regex101.com/. Here are the titles for the first 26 papers. <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online Controlled Experiments:\n",
      "Mining Frequent Itemsets through Progressive Sampling\n",
      "Why It Happened: Identifying and Modeling the Reasons of\n",
      "Matrix Completion with Queries\n",
      "Stochastic Divergence Minimization\n",
      "Bayesian Poisson Tensor Factorization for Inferring\n",
      "TimeCrunch: Interpretable Dynamic Graph Summarization\n",
      "Inside Jokes: Identifying Humorous Cartoon Captions\n",
      "Community Detection based on Distance Dynamics\n",
      "Discovery of Meaningful Rules in Time Series\n",
      "On the Formation of Circles in Co-authorship Networks\n",
      "An Evaluation of Parallel Eccentricity Estimation\n",
      "Efcient Latent Link Recommendation in\n",
      "Turn Waste into Wealth: On Simultaneous Clustering and\n",
      "Set Cover at Web Scale\n",
      "Exploiting Relevance Feedback in Knowledge Graph\n",
      "LINKAGE: An Approach for Comprehensive Risk\n",
      "Transitive Transfer Learning\n",
      "PTE: Predictive Text Embedding through Large-scale\n",
      "An Effective Marketing Strategy for Revenue Maximization\n",
      "Scaling Up Stochastic Dual Coordinate Ascent\n",
      "Heterogeneous Network Embedding via Deep\n",
      "Discovering Valuable Items from Massive Data\n",
      "Deep Learning Architecture with Dynamically Programmed\n",
      "Incorporating World Knowledge to Document Clustering\n"
     ]
    }
   ],
   "source": [
    "# code uses regular expression to extract text up to the first new line character\n",
    "\n",
    "p=re.compile('(.+)(\\\\n)')\n",
    "for fileid in kddcorpus.fileids()[:25]:\n",
    "    print p.search(kddcorpus.raw(fileid)).group(1).strip()  # use .strip() to remove whitespace from beginning and end of string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling and computation: Using Regular Expressions to extract specific sections of the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are close to the NERC portion.  But, there's a bit more wrangling to do (remember, PDFs are tough work).  For simplicity, let's focus the NERC on two sections of the paper:\n",
    "* the top section which includes authors and schools\n",
    "* the references section of the paper (keynote speaker abstracts do not have an abstract)\n",
    "\n",
    "The tools of choice to extract sections are the [\"positive lookbehind\" and \"positive lookahead\"](https://docs.python.org/2/library/re.html) expressions. Here is an example of code to extract the abstract only:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The collapsed variational Bayes zero (CVB0) inference is a vari- ational inference improved by marginalizing out parameters, the same as with the collapsed Gibbs sampler. A drawback of the CVB0 inference is the memory requirements. A probability vec- tor must be maintained for latent topics for every token in a corpus. When the total number of tokens is N and the number of topics is K, the CVB0 inference requires O(N K) memory. A stochas- tic approximation of the CVB0 (SCVB0) inference can reduce O(N K) to O(V K), where V denotes the vocabulary size. We re- formulate the existing SCVB0 inference by using the stochastic di- vergence minimization algorithm, with which convergence can be analyzed in terms of Martingale convergence theory. We also reveal the property of the CVB0 inference in terms of the leave-one-out perplexity, which leads to the estimation algorithm of the Dirichlet distribution parameters. The predictive performance of the propose SCVB0 inference is better than that of the original SCVB0 infer- ence in four datasets.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set our regular expression\n",
    "p= re.compile('(?<=ABSTRACT)(.+)(?=Categories and Subject Descriptors)')\n",
    "try:\n",
    "    abstract= p.search(re.sub('[\\s]',\" \",kddcorpus.raw('p1035.txt'))).group(1)\n",
    "except AttributeError:\n",
    "    # include a lowercase regex match incase consistency is a problem\n",
    "    p=re.compile('(?<=abstract)(.+)(?=categories and subject descriptors)')\n",
    "    abstract=p.search(re.sub('[\\s]',\" \",holder.lower())).group(1)\n",
    "else:\n",
    "    pass\n",
    "unicodedata.normalize('NFKD', abstract).encode('ascii','ignore').strip() # convert output from unicode to string and strip leading and trailing whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sup id=\"fn1\">1. [(2014). Text Mining and its Business Applications - CodeProject. Retrieved December 26, 2015, from http://www.codeproject.com/Articles/822379/Text-Mining-and-its-Business-Applications.]<a href=\"#ref1\" title=\"Jump back to footnote 1 in the text.\">↩</a></sup>\n",
    "\n",
    "<sup id=\"fn2\">2. [Suchanek, F., & Weikum, G. (2013). Knowledge harvesting in the big-data era. Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data. ACM.]<a href=\"#ref2\" title=\"Jump back to footnote 2 in the text.\">↩</a></sup>\n",
    "\n",
    "\n",
    "<sup id =\"fn3\">3. [Nadeau, D., & Sekine, S. (2007). A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1), 3-26.]<a href=\"#ref3\" title = \"Jump back to footnote 3 in the text\">↩</a></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Parking Lot of links, leftover paragraphs, ideas, etc.</span>\n",
    "\n",
    "Describe the data -> Data available here http://dl.acm.org/citation.cfm?id=2783258# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<href id=\"pipe\"><a href=\"#pipeline\" title=\"Jump back to data science pipeline graphic.\">data science pipeline</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ben's Outline from email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ~~Give a brief introduction to the task, and why it's interesting, important. Then begin to discuss the data set, how you acquired, and where a reader can get access to it.~~ \n",
    "\n",
    "* ~~You then could have a data exploration section where you show the number of documents, perform a word count, show snippets of data (e.g. references) etc that are of interest.~~\n",
    "\n",
    "* You can then go through one or a few of your \"code to get\" sections. These functions all follow basically the same pattern, so you could probably merge them into a single function, that appropriately selects the right regular expression. \n",
    "\n",
    "* The next step is to discuss, demonstrate your \"truth tests\" for text extraction accuracy. \n",
    "\n",
    "* Finally, you can get to an introduction of your three methods for NERC, and show how do do each of them. Then compare (visually) the results of the three according to the evaluation mechanism discussed above. \n",
    "\n",
    "* You could then conclude with a discussion about NLTK chunk vs. hand labelled entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kddcorpus_bigrams=[]\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "for fileid in kddcorpus.fileids():\n",
    "    for l in (BigramCollocationFinder.from_words(kddcorpus.words(fileid)).nbest(bigram_measures.pmi, 10)):\n",
    "        kddcorpus_bigrams.append(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Code snippets for later....difficult stuff</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# completed gold standard for keywords, got all of them..no stragglers\n",
    "\n",
    "failids = []\n",
    "full = True\n",
    "section = \"keywords\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid).lower()\n",
    "        if section == \"keywords\":\n",
    "            section1=\"keywords\"\n",
    "            target = \"\"   \n",
    "            section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"permission to make \",\"1.motivation\" ]\n",
    "        \n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    if len(target) > 3 and len(target) < 3000:\n",
    "                        print [fileid,len(target),len(text)]\n",
    "\n",
    "                        break\n",
    "                    \n",
    "                        \n",
    "                    else:\n",
    "                        failids.append(fileid)\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "else:\n",
    "    section = \"keywords\"\n",
    "    text = kddcorpus.raw('p19.txt').lower()\n",
    "    if section == \"keywords\":\n",
    "        section1=\"keywords\"\n",
    "        target = \"\"   \n",
    "        section2=[\"  1. introduction  \",\"1. motivation\",\"permission to make \",\"1.motivation\" ]\n",
    "\n",
    "        part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "        for sect in section2:\n",
    "            try:\n",
    "                part2 = \"(?=\"+str(sect)+\")\"\n",
    "                p=re.compile(part1+part2)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                \n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p1.txt', 1363, 3978]\n",
      "['p1005.txt', 864, 53879]\n",
      "['p1015.txt', 1631, 52381]\n",
      "['p1025.txt', 1270, 57404]\n",
      "['p1035.txt', 1052, 47047]\n",
      "['p1045.txt', 1182, 58909]\n",
      "['p1055.txt', 2027, 58898]\n",
      "['p1065.txt', 1238, 56134]\n",
      "['p1075.txt', 1700, 53147]\n",
      "['p1085.txt', 1329, 67605]\n",
      "['p109.txt', 1541, 57142]\n",
      "['p1095.txt', 1131, 72090]\n",
      "['p1105.txt', 2528, 54116]\n",
      "['p1115.txt', 1193, 50911]\n",
      "['p1125.txt', 2023, 41991]\n",
      "['p1135.txt', 1424, 60352]\n",
      "['p1145.txt', 1138, 52282]\n",
      "['p1155.txt', 1374, 55445]\n",
      "['p1165.txt', 1563, 54792]\n",
      "['p1175.txt', 997, 57100]\n",
      "['p1185.txt', 1988, 41459]\n",
      "['p119.txt', 1877, 57719]\n",
      "['p1195.txt', 1508, 56206]\n",
      "['p1205.txt', 2022, 50250]\n",
      "['p1215.txt', 1524, 58584]\n",
      "['p1225.txt', 970, 59347]\n",
      "['p1235.txt', 1309, 52842]\n",
      "['p1245.txt', 2073, 49933]\n",
      "['p1255.txt', 1405, 65544]\n",
      "['p1265.txt', 1588, 50534]\n",
      "['p1275.txt', 1547, 58455]\n",
      "['p1285.txt', 1428, 51589]\n",
      "['p129.txt', 1175, 57292]\n",
      "['p1295.txt', 1443, 54135]\n",
      "['p1305.txt', 1113, 48257]\n",
      "['p1315.txt', 1361, 57078]\n",
      "['p1325.txt', 992, 56424]\n",
      "['p1335.txt', 1749, 63566]\n",
      "['p1345.txt', 1630, 54120]\n",
      "['p1355.txt', 2292, 62751]\n",
      "['p1365.txt', 661, 54373]\n",
      "['p1375.txt', 1121, 54266]\n",
      "['p1385.txt', 2639, 48533]\n",
      "['p139.txt', 1174, 61268]\n",
      "['p1395.txt', 1481, 58600]\n",
      "['p1405.txt', 1657, 54336]\n",
      "['p1415.txt', 1378, 58895]\n",
      "['p1425.txt', 1338, 57946]\n",
      "['p1435.txt', 1510, 50945]\n",
      "['p1445.txt', 1329, 66841]\n",
      "['p1455.txt', 1666, 61716]\n",
      "['p1465.txt', 2080, 58800]\n",
      "['p1475.txt', 2502, 46550]\n",
      "['p1485.txt', 1390, 56633]\n",
      "['p149.txt', 1721, 50133]\n",
      "['p1495.txt', 1338, 35939]\n",
      "['p1503.txt', 1455, 55251]\n",
      "['p1513.txt', 1311, 54389]\n",
      "['p1523.txt', 1447, 58297]\n",
      "['p1533.txt', 1928, 52237]\n",
      "['p1543.txt', 2339, 66668]\n",
      "['p1553.txt', 518, 48869]\n",
      "['p1563.txt', 1355, 69388]\n",
      "['p1573.txt', 1580, 50726]\n",
      "['p1583.txt', 1704, 52238]\n",
      "['p159.txt', 2588, 64241]\n",
      "['p1593.txt', 1278, 62312]\n",
      "['p1603.txt', 1197, 54407]\n",
      "['p1621.txt', 871, 2643]\n",
      "['p1623.txt', 373, 2012]\n",
      "['p1625.txt', 844, 2332]\n",
      "['p1627.txt', 347, 4718]\n",
      "['p1629.txt', 1598, 4368]\n",
      "['p1631.txt', 1065, 2572]\n",
      "['p1633.txt', 1733, 4080]\n",
      "['p1635.txt', 1051, 2664]\n",
      "['p1637.txt', 1040, 2963]\n",
      "['p1639.txt', 852, 3060]\n",
      "['p1641.txt', 1572, 60855]\n",
      "['p1651.txt', 790, 48891]\n",
      "['p1661.txt', 864, 73464]\n",
      "['p1671.txt', 1979, 44143]\n",
      "['p1681.txt', 1138, 57027]\n",
      "['p169.txt', 2200, 61089]\n",
      "['p1691.txt', 2083, 58067]\n",
      "['p1701.txt', 2992, 49784]\n",
      "['p1711.txt', 725, 49862]\n",
      "['p1721.txt', 1350, 52813]\n",
      "['p1731.txt', 1106, 44022]\n",
      "['p1741.txt', 1349, 47065]\n",
      "['p1751.txt', 2227, 39054]\n",
      "['p1759.txt', 1136, 64110]\n",
      "['p1769.txt', 1770, 56097]\n",
      "['p1779.txt', 977, 51592]\n",
      "['p1789.txt', 1874, 53190]\n",
      "['p179.txt', 1226, 49875]\n",
      "['p1799.txt', 2319, 73905]\n",
      "['p1809.txt', 1503, 53144]\n",
      "['p1819.txt', 1447, 162345]\n",
      "['p1829.txt', 1093, 57049]\n",
      "['p1839.txt', 2333, 41707]\n",
      "['p1849.txt', 2681, 60693]\n",
      "['p1859.txt', 1411, 31878]\n",
      "['p1869.txt', 1351, 54704]\n",
      "['p1879.txt', 1305, 42916]\n",
      "['p1889.txt', 634, 39369]\n",
      "['p189.txt', 796, 51930]\n",
      "['p1899.txt', 1118, 54333]\n",
      "['p19.txt', 1264, 65929]\n",
      "['p1909.txt', 939, 53054]\n",
      "['p1919.txt', 1116, 47262]\n",
      "['p1929.txt', 1113, 61240]\n",
      "['p1939.txt', 2054, 45373]\n",
      "['p1949.txt', 1528, 55322]\n",
      "['p1959.txt', 1255, 62457]\n",
      "['p1969.txt', 1374, 46902]\n",
      "['p1979.txt', 2421, 54375]\n",
      "['p1989.txt', 1561, 40708]\n",
      "['p199.txt', 1746, 60517]\n",
      "['p1999.txt', 1205, 60741]\n",
      "['p2009.txt', 506, 51873]\n",
      "['p2019.txt', 1179, 60381]\n",
      "['p2029.txt', 1727, 55772]\n",
      "['p2039.txt', 1137, 37196]\n",
      "['p2049.txt', 1018, 47870]\n",
      "['p2059.txt', 1303, 60614]\n",
      "['p2069.txt', 2031, 50765]\n",
      "['p2079.txt', 917, 47186]\n",
      "['p2089.txt', 1720, 44976]\n",
      "['p209.txt', 1223, 48264]\n",
      "['p2099.txt', 1303, 54288]\n",
      "['p2109.txt', 836, 57236]\n",
      "['p2119.txt', 1253, 35907]\n",
      "['p2127.txt', 1651, 52735]\n",
      "['p2137.txt', 843, 49652]\n",
      "['p2147.txt', 1592, 49740]\n",
      "['p2167.txt', 2094, 49255]\n",
      "['p2177.txt', 1399, 51270]\n",
      "['p2187.txt', 2074, 61255]\n",
      "['p219.txt', 1426, 55773]\n",
      "['p2197.txt', 1428, 50135]\n",
      "['p2207.txt', 2308, 36089]\n",
      "['p2217.txt', 1106, 63975]\n",
      "['p2227.txt', 1470, 70239]\n",
      "['p2237.txt', 1428, 48542]\n",
      "['p2247.txt', 1301, 50094]\n",
      "['p2257.txt', 2097, 60240]\n",
      "['p2267.txt', 5705, 73055]\n",
      "['p2277.txt', 1271, 55170]\n",
      "['p2287.txt', 1285, 41410]\n",
      "['p229.txt', 1996, 48917]\n",
      "['p2297.txt', 1294, 34042]\n",
      "['p2307.txt', 1164, 12434]\n",
      "['p2309.txt', 927, 7606]\n",
      "['p2311.txt', 1808, 7660]\n",
      "['p2313.txt', 1694, 8704]\n",
      "['p2315.txt', 2216, 12536]\n",
      "['p2317.txt', 808, 7663]\n",
      "['p2319.txt', 1922, 10368]\n",
      "['p2321.txt', 1021, 10372]\n",
      "['p2323.txt', 940, 7816]\n",
      "['p2325.txt', 1992, 5370]\n",
      "['p2327.txt', 911, 2506]\n",
      "['p239.txt', 945, 54317]\n",
      "['p249.txt', 1957, 50766]\n",
      "['p259.txt', 1402, 60262]\n",
      "['p269.txt', 1258, 47280]\n",
      "['p279.txt', 1293, 54177]\n",
      "['p289.txt', 1941, 46884]\n",
      "['p29.txt', 949, 51345]\n",
      "['p299.txt', 2063, 58472]\n",
      "['p3.txt', 1700, 3327]\n",
      "['p309.txt', 870, 58333]\n",
      "['p319.txt', 1346, 56493]\n",
      "['p329.txt', 1957, 45108]\n",
      "['p339.txt', 2230, 46223]\n",
      "['p349.txt', 2299, 65545]\n",
      "['p359.txt', 1214, 57407]\n",
      "['p369.txt', 2399, 51113]\n",
      "['p379.txt', 846, 41652]\n",
      "['p387.txt', 1236, 60277]\n",
      "['p39.txt', 1290, 49583]\n",
      "['p397.txt', 1975, 57500]\n",
      "['p407.txt', 975, 44825]\n",
      "['p417.txt', 1070, 62468]\n",
      "['p427.txt', 1227, 51603]\n",
      "['p437.txt', 1014, 62288]\n",
      "['p447.txt', 1593, 59148]\n",
      "['p457.txt', 1732, 53342]\n",
      "['p467.txt', 1274, 53068]\n",
      "['p477.txt', 1224, 55732]\n",
      "['p487.txt', 1470, 52943]\n",
      "['p49.txt', 1406, 67086]\n",
      "['p497.txt', 1601, 49481]\n",
      "['p5.txt', 2530, 12904]\n",
      "['p507.txt', 1236, 54405]\n",
      "['p517.txt', 1531, 57072]\n",
      "['p527.txt', 1217, 61766]\n",
      "['p537.txt', 1183, 58818]\n",
      "['p547.txt', 1989, 60319]\n",
      "['p557.txt', 1065, 50755]\n",
      "['p567.txt', 1312, 58651]\n",
      "['p577.txt', 865, 44050]\n",
      "['p587.txt', 1325, 51982]\n",
      "['p59.txt', 1315, 49722]\n",
      "['p597.txt', 1004, 56924]\n",
      "['p607.txt', 1573, 50677]\n",
      "['p617.txt', 1189, 52712]\n",
      "['p627.txt', 1822, 44603]\n",
      "['p635.txt', 1300, 46482]\n",
      "['p645.txt', 2123, 63353]\n",
      "['p655.txt', 1206, 52264]\n",
      "['p665.txt', 2085, 47531]\n",
      "['p675.txt', 1195, 58660]\n",
      "['p685.txt', 1377, 50222]\n",
      "['p69.txt', 1667, 55036]\n",
      "['p695.txt', 891, 59996]\n",
      "['p7.txt', 1098, 2755]\n",
      "['p705.txt', 1930, 50365]\n",
      "['p715.txt', 1321, 48595]\n",
      "['p725.txt', 1122, 55177]\n",
      "['p735.txt', 1916, 55457]\n",
      "['p745.txt', 1485, 56749]\n",
      "['p755.txt', 4011, 55541]\n",
      "['p765.txt', 1365, 56692]\n",
      "['p775.txt', 1910, 57142]\n",
      "['p785.txt', 2193, 59719]\n",
      "['p79.txt', 1413, 50396]\n",
      "['p805.txt', 1179, 57068]\n",
      "['p815.txt', 2533, 53213]\n",
      "['p825.txt', 1163, 63210]\n",
      "['p835.txt', 1386, 55636]\n",
      "['p845.txt', 1838, 61292]\n",
      "['p855.txt', 1877, 69900]\n",
      "['p865.txt', 1777, 52737]\n",
      "['p875.txt', 1093, 56400]\n",
      "['p885.txt', 1420, 48940]\n",
      "['p89.txt', 1544, 66080]\n",
      "['p895.txt', 1667, 53957]\n",
      "['p9.txt', 967, 53456]\n",
      "['p905.txt', 1404, 55961]\n",
      "['p915.txt', 1870, 54176]\n",
      "['p935.txt', 1278, 63946]\n",
      "['p945.txt', 1528, 49912]\n",
      "['p955.txt', 1024, 55785]\n",
      "['p965.txt', 1697, 71341]\n",
      "['p975.txt', 1463, 52565]\n",
      "['p985.txt', 2126, 60739]\n",
      "['p99.txt', 1285, 44868]\n",
      "['p995.txt', 1285, 65536]\n",
      "entity recognition is an important but challenging research problem. in reality, many text collections are from spe- ci(cid:12)c, dynamic, or emerging domains, which poses signi(cid:12)cant new challenges for entity recognition with increase in name ambiguity and context sparsity, requiring entity detection without domain restriction. in this paper, we investigate entity recognition (er) with distant-supervision and pro- pose a novel relation phrase-based er framework, called clustype, that runs data-driven phrase mining to gen- erate entity mention candidates and relation phrases, and enforces the principle that relation phrases should be softly clustered when propagating type information between their argument entities. then we predict the type of each entity mention based on the type signatures of its co-occurring re- lation phrases and the type indicators of its surface name, as computed over the corpus. speci(cid:12)cally, we formulate a joint optimization problem for two tasks, type propagation with relation phrases and multi-view relation phrase clus- tering. our experiments on multiple genres|news, yelp reviews and tweets|demonstrate the eectiveness and ro- bustness of clustype, with an average of 37% improvement in f1 score over the best compared method.\n"
     ]
    }
   ],
   "source": [
    "# completed gold standard for abstract, got all of em\n",
    "failids = []\n",
    "text=kddcorpus.raw('p1055.txt')\n",
    "\n",
    "full = True\n",
    "section = \"abstract\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid).lower()\n",
    "        if section == \"abstract\":\n",
    "            section1=\"abstract\"\n",
    "            target = \"\"   \n",
    "            section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if len(target) > 50:\n",
    "                        \n",
    "                        print [fileid,len(target),len(text)]\n",
    "                        break\n",
    "                    else:\n",
    "                        failids.append(fileid)\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    \n",
    "                    pass\n",
    "                              \n",
    "else:\n",
    "    \n",
    "    section = \"abstract\"\n",
    "    text = kddcorpus.raw('p1627.txt').lower()\n",
    "    if section == \"abstract\":\n",
    "        section1=\"abstract\"\n",
    "        target = \"\"   \n",
    "        section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "\n",
    "        part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "        for sect in section2:\n",
    "            try:\n",
    "                part2 = \"(?=\"+str(sect)+\")\"\n",
    "                p=re.compile(part1+part2)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                if target > 50:\n",
    "\n",
    "                    \n",
    "                    break\n",
    "            except:\n",
    "                \n",
    "                pass\n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(kddcorpus_bigrams)\n",
    "filtered_words = [word for word in kddcorpus_bigrams if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "holder = kddcorpus.raw('p1035.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p= re.compile('(?<=ABSTRACT)(.+)(?=Categories and Subject Descriptors)')\n",
    "\n",
    "\n",
    "try:\n",
    "    abstract= p.search(re.sub('[\\s]',\" \",holder)).group(1)\n",
    "except AttributeError:\n",
    "    p=re.compile('(?<=abstract)(.+)(?=categories and subject descriptors)')\n",
    "    abstract=p.search(re.sub('[\\s]',\" \",holder.lower())).group(1)\n",
    "else:\n",
    "    pass\n",
    "unicodedata.normalize('NFKD', abstract).encode('ascii','ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "section1 = \"\"\n",
    "part1=\"(?<=\"+str(section1)+\")(.+)\"\n",
    "part2 = \"(?=\"+str(section2)+\")\"\n",
    "part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sectpull(section,text = None,nameofsection = None):\n",
    "    \n",
    "    section = section.lower()    \n",
    "    if text is None:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    text=text.lower()\n",
    "    nameofsection = \"\"\n",
    "    if section == \"abstract\":\n",
    "        while len(nameofsection) <=50:      \n",
    "            try:\n",
    "                section1 = \"abstract\".lower()\n",
    "                section2=[\"categories and subject descriptors\",\"keywords\", \"introduction\",\"\\\\n\"]\n",
    "                part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "                \n",
    "                for sect in section2:\n",
    "                    part2 = \"(?=\"+str(section2)+\")\"\n",
    "                    p=re.compile(part1.lower()+part2.lower())\n",
    "                nameofsection= p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "            #while len(nameofsection)\n",
    "            try:\n",
    "                p=re.compile(part1.title()+part2.title())\n",
    "                nameofsection=p.search(re.sub('[\\s]',\" \",text.title())).group(1)\n",
    "            except:\n",
    "                raise BaseException(\"Nothing worked.  Consider altering your regular expression\")\n",
    "        \n",
    "    return nameofsection\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'  Computers  Are  Making  More  And  More  Decisions  For  Us,  And  Increasingly So In Areas That Require Human Judgment. There Is A  Palpable Increase In Machine Intelligence Across The Touch Points  Of  Our  Lives,  Driven  By  The  Proliferation  Of  Data  Feeding  Into  Intelligent  Algorithms  Capable  Of  Learning  Useful  Patterns  And  Acting  On  Them.  A  Natural  Question  To  Ask  Is  How  We  Should  Be  Thinking  About  The  Role  Of  Computers  In  Managing  Our  Money.  Should We Trust Our Money To A Robot? In An Era Of Big Data And  Machines  To  Make  Sense  Of  It  All,  Do Machines Have An Inherent  Advantage Over Humans? There Is A Surge Of Interest In Artificial  Intelligence For Financial Prediction. Should We Pay Attention? Or  Is  This  An  Area  Where  Human  Judgment  And  Input  Is  Always  Essential?   '"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sectpull(\"abstract\",text = kddcorpus.raw('p1625.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p2157.txt', 'p2329.txt']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "section1 = \"abstract\"\n",
    "section2= \"introduction\"\n",
    "part1=\"(?<=\"+str(section1)+\")(.+)\"\n",
    "part2 = \"(?=\"+str(section2)+\")\"\n",
    "p=re.compile(part1+part2)\n",
    "nameofsection= p.search(re.sub('[\\s]',\" \",kddcorpus.raw(\"p19.txt\")).group(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(failids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Forec\\nYu Zheng\\n\\ncasting \\ng1,2, Xiuwen\\n\\n Fine-G\\nn Yi2,1, Ming\\n\\nGrained \\ng Li1, Ruiyua\\n1Microsoft Re\\n2Southwest\\nt Jiaotong Uni\\n3\\n3Fudan Unive\\n-xiuyi, mingl, v\\nv-ruiyli, v-zhas\\n\\n Air Qu\\nan Li1, Zhan\\nesearch, Beijin\\nversity, Chen\\nersity, Shangh\\nsha, echang}@\\n\\nuality Ba\\nngqing Sha\\nng, China \\nngdu, Sichuan\\nhai, China \\n@microsoft.co\\n\\nn, China \\n\\nased on\\nn3,1, Eric Ch\\n\\nn Big D\\nhang1, Tian\\n\\nData\\nnrui Li2 \\n\\n{yuzheng, v-\\n\\nT \\nABSTRACT\\nA\\ne forecast the rea\\nIn\\nn this paper, we\\ns\\nstation  over  the \\nnext  48  hours, \\nt  meteorological\\nc\\nconsiders  current\\nhe  station  and  th\\nq\\nquality  data  of  th\\ners.  Our  predict\\nh\\nhundred  kilomet\\nts: 1) a linear re\\nm\\nmajor componen\\nto\\no model the loca\\nal factors of air q\\nto model global\\ns\\nspatial predictor \\npredictions  of  th\\nc\\ncombining  the  p\\nteorological data\\na\\naccording to met\\nhanges in air qu\\nc\\ncapture sudden ch\\nd\\ndata  from  43  cit\\nties  in  China,  s\\ns.  We  have  dep\\nb\\nbaseline  methods\\nvironmental  Pro\\nM\\nMinistry  of  Env\\nity  forecasts  for\\ng\\ngrained  air  quali\\nst function is als\\nh\\nhour. The forecas\\na\\nand MS cloud pla\\natform Azure. Ou\\nfor other cities. \\na\\napplied globally f\\nand Subject \\nC\\nCategories a\\ne  Management]\\nH.2.8  [Database\\nH\\nm\\nmining, Spatial d\\ndatabases and GI\\nK\\nKeywords \\nU\\nUrban computing\\n1.  INTROD\\n1\\nP\\nPeople are increa\\nh\\nhuman health and\\nc\\ncities  have  built \\na\\nabout urban air q\\nm\\nmatter) and PM1\\nd\\ndemand  for  the  p\\np\\npeoples decision\\nin\\nn  a  park)  and \\np\\npollution alerts or\\nP\\nPredicting  urban \\nf\\nfollowing three re\\nm\\n'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kddcorpus.raw('p2267.txt')[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "badids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "badids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sectpull(section1,section2 = None,text = None):\n",
    "    \n",
    "    if text is None:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "            \n",
    "    \n",
    "    part1=\"(?<=\"+str(section1)+\")(.+)\"\n",
    "    part2 = \"(?=\"+str(section2)+\")\"\n",
    "    \n",
    "    if section2 is None:\n",
    "        p=re.compile(part1)\n",
    "    elif section1 is None:\n",
    "        p=re.compile(part2)\n",
    "    else:\n",
    "        p=re.compile(part1+part2)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p= re.compile('(?<=ABSTRACT)(.+)(?=Categories and Subject Descriptors)')\n",
    "try:\n",
    "    abstract= p.search(re.sub('[\\s]',\" \",kddcorpus.raw(\"p1939.txt\"))).group(1)\n",
    "except AttributeError:\n",
    "    p=re.compile('(?<=abstract)(.+)(?=categories and subject descriptors)')\n",
    "    abstract=p.search(re.sub('[\\s]',\" \",kddcorpus.raw(\"p1939.txt\"))).group(1)\n",
    "else:\n",
    "    pass\n",
    "abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids =[]\n",
    "badids = []\n",
    "goodids = []\n",
    "full = True\n",
    "section = \"abstract\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid).lower()\n",
    "        if section == \"abstract\":\n",
    "            section1=\"abstract\"\n",
    "            target = \"\"   \n",
    "            section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"keyword\", \"introduction \",\"\\\\\\\\n\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    print len(target)\n",
    "                        goodids.append(fileid)\n",
    "                        break\n",
    "                    else:\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    badids.append(fileid)\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dynamic graph; network; clustering; summarization; compression  1.  \n"
     ]
    }
   ],
   "source": [
    "section = \"keywords\"\n",
    "text = kddcorpus.raw('p1055.txt').lower()\n",
    "if section == \"keywords\":\n",
    "    section1=\"keywords\"\n",
    "    target = \"\"   \n",
    "    section2=[\"1. introduction  \",\"introduction  \",\"1. motivation\",\"permission to make \",\"1.motivation\" ]\n",
    "\n",
    "    part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "    for sect in section2:\n",
    "        try:\n",
    "            part2 = \"(?=\"+str(sect)+\")\"\n",
    "            p=re.compile(part1+part2)\n",
    "            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "            if target > 50:\n",
    "                \n",
    "                print target\n",
    "                break\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u' data-center management; modeling and prediction; ma- chine learning; execution experiences; hadoop  '"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = kddcorpus.raw('p1701.txt').lower()\n",
    "p=re.compile(r'(?<=keywords)(.+)(?=1.  introduction  )')\n",
    "abstract=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "failed =[]\n",
    "for fileid in kddcorpus.fileids():\n",
    "    try:\n",
    "        print sectpull(\"abstract\",text=kddcorpus.raw(fileid))\n",
    "    except:\n",
    "        failed.append(fileid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
