{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick Survey and Comparison of Open Source Named Entity Extractor Tools for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity extraction is a core subtask of building knowledge from semi/unstructured text sources<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup>.  Considering recent increases in computing power and decreases in the costs of data storage, data scientists and developers can build large knowledge bases that contain millions of entities and hundreds of millions of facts about them.  These knowledge bases are key contributors to intelligence computer behavior. Therefore, named entity extraction is at the core of several popular technologies such as smart assistants ([Siri](http://www.apple.com/ios/siri/), [Google Now](https://www.google.com/landing/now/)), machine reading, and deep interpretation of natural language.\n",
    "\n",
    "With a realization of how essential it is to recognize information units like names, including person, organization and location names, and numeric expressions including time, date, money\n",
    "and percent expressions, several questions come to mind.  How do you perform named entity extraction, which is formally called “[Named Entity Recognition and Classification (NERC)](https://benjamins.com/catalog/bct.19)”?  What tools are out there?  How can you evaluate their performance?  And most important, what works with Python (shamelessly exposing my bias)?  \n",
    "\n",
    "This post will survey openly available NERC tools and compare the results against hand labeled data for precision, accuracy, and recall.  The tools and basic information extraction principles in this discussion begin the process of structuring unstructured data.\n",
    "\n",
    "We will specifically learn to:\n",
    "1. follow the data science pipeline (see image below)\n",
    "2. prepare semistructured natural language data for ingest using regex\n",
    "3. create a custom corpus in [Natural Language Toolkit](http://www.nltk.org/) \n",
    "4. use a suite of openly available NERC tools to extract entities and store in json format \n",
    "5. compare the performance of NERC tools on our corpus\n",
    "\n",
    "<br>\n",
    "<a href=\"#pipe\" id=\"pipeline\"><center><h3>The Data Science Pipeline:<br>Georgetown Data Science Certificate Program</h3></center></a>\n",
    "<div class=\"image\">\n",
    "\n",
    "      <img src=\"./files/data_science_pipeline.png\" alt=\"Data Science Pipeline\" height=\"300\" width=\"450\" top:\"35\" left:\"170\" />\n",
    "      \n",
    "      \n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data: Peer Reviewed Journals and Keynote Speaker Abstracts from KDD 2014 and 2015\n",
    "\n",
    "Before delving into the pipeline, we need a good dataset.  Jason Brownlee of www.machinelearningmastery.com had some good suggestions in his [August 2015 article](http://machinelearningmastery.com/practice-machine-learning-with-small-in-memory-datasets-from-the-uci-machine-learning-repository/) on picking a dataset for machine learning exercises:  \n",
    "\n",
    "* **Real-World**: The datasets should be drawn from the real world (rather than being contrived). This will keep them interesting and introduce the challenges that come with real data.\n",
    "\n",
    "* **Small**: The datasets need to be small so that you can inspect and understand them and that you can run many models quickly to accelerate your learning cycle.\n",
    "\n",
    "* **Well-Understood**: There should be a clear idea of what the data contains, why it was collected, what the problem is that needs to be solved so that you can frame your investigation.\n",
    "\n",
    "* **Baseline**: It is also important to have an idea of what algorithms are known to perform well and the scores they achieved so that you have a useful point of comparison. This is important when you are getting started and learning because you need quick feedback as to how well you are performing (close to state-of-the-art or something is broken).\n",
    "\n",
    "* **Plentiful**: You need many datasets to choose from, both to satisfy the traits you would like to investigate and (if possible) your natural curiosity and interests. \n",
    "\n",
    "Luckily, we have a dataset that meets nearly all of these requirements.  I attended the Knowledge Discovery and Data Mining (KDD) conferences in [New York City (2014)](http://www.kdd.org/kdd2014/) and [Sydney, Australia (2015)](http://www.kdd.org/kdd2015/).  Both years, attendees received a USB with the conference proceedings.  Each repository contains over 230 peer reviewed journal articles and keynote speaker abstracts on data mining, knowledge discovery, big data, data science and their applications. The full conference proceedings can be purchased for \\$60 at the [Association for Computing Machinery's Digital Library](https://dl.acm.org/purchase.cfm?id=2783258&CFID=740512201&CFTOKEN=34489585) (includes ACM membership). This post will work with a dataset that is equivalent to the conference proceedings.  It's important to note that this dataset recreates a real word data science exercise that is instructive of big data problems.  We will take semi-structured data (PDF journal articles and abstracts in publication format), strip text from the files, and add more structure to the data that would facilitate follow on analysis. \n",
    "\n",
    "<blockquote cite=\"https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/KDDwebscrape.ipynb\">\n",
    "Interested parties looking for a free option can use the <a href=\"https://pypi.python.org/pypi/beautifulsoup4/4.4.1\">beautifulsoup</a> and <a href=\"https://pypi.python.org/pypi/requests/2.9.1\">request</a> libraries to scrape the <a href=\"http://dl.acm.org/citation.cfm?id=2785464&CFID=740512201&CFTOKEN=3448958\">ACM website for KDD 2015 conference data</a> that can be used in natural language processing pipelines.  I have some <a href=\"https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/KDDwebscrape.ipynb\">skeleton web scraping code</a> to generate lists of all abstracts, author names, and journal/keynote address titles.    \n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration: Getting the number of files, file type, and word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored locally in the following directory:\n",
    "```python\n",
    ">>> import os\n",
    ">>> print os.getcwd()\n",
    "/Users/linwood/Desktop/KDD_15/docs\n",
    "```\n",
    "Let's explore the number of files we have and naming conventions. We begin with the administrative tasks of loading modules, establishing paths, etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Importing what we need\n",
    "#**********************************************************************\n",
    "import os\n",
    "import time\n",
    "from os import walk\n",
    "\n",
    "#**********************************************************************\n",
    "# Administrative code to set the path for file loading\n",
    "#**********************************************************************\n",
    "\n",
    "path        = os.path.abspath(os.getcwd())\n",
    "TESTDIR     = os.path.normpath(os.path.join(os.path.expanduser(\"~\"),\"Desktop\",\"KDD_15\",\"docs\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we iterate over the files in the directory and store those names in the empty list we created called *files*.  We time the operation, print list with the file names and also print out the length of the list (gives number of target files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 8.82 µs\n",
      "\n",
      "253\n",
      "\n",
      "[p1.pdf, p1005.pdf, p1015.pdf, p1025.pdf, p1035.pdf, p1045.pdf, p1055.pdf, p1065.pdf, p1075.pdf, p1085.pdf, p109.pdf, p1095.pdf, p1105.pdf, p1115.pdf, p1125.pdf, p1135.pdf, p1145.pdf, p1155.pdf, p1165.pdf, p1175.pdf, p1185.pdf, p119.pdf, p1195.pdf, p1205.pdf, p1215.pdf, p1225.pdf, p1235.pdf, p1245.pdf, p1255.pdf, p1265.pdf, p1275.pdf, p1285.pdf, p129.pdf, p1295.pdf, p1305.pdf, p1315.pdf, p1325.pdf, p1335.pdf, p1345.pdf, p1355.pdf, p1365.pdf, p1375.pdf, p1385.pdf, p139.pdf, p1395.pdf, p1405.pdf, p1415.pdf, p1425.pdf, p1435.pdf, p1445.pdf, p1455.pdf, p1465.pdf, p1475.pdf, p1485.pdf, p149.pdf, p1495.pdf, p1503.pdf, p1513.pdf, p1523.pdf, p1533.pdf, p1543.pdf, p1553.pdf, p1563.pdf, p1573.pdf, p1583.pdf, p159.pdf, p1593.pdf, p1603.pdf, p1621.pdf, p1623.pdf, p1625.pdf, p1627.pdf, p1629.pdf, p1631.pdf, p1633.pdf, p1635.pdf, p1637.pdf, p1639.pdf, p1641.pdf, p1651.pdf, p1661.pdf, p1671.pdf, p1681.pdf, p169.pdf, p1691.pdf, p1701.pdf, p1711.pdf, p1721.pdf, p1731.pdf, p1741.pdf, p1751.pdf, p1759.pdf, p1769.pdf, p1779.pdf, p1789.pdf, p179.pdf, p1799.pdf, p1809.pdf, p1819.pdf, p1829.pdf, p1839.pdf, p1849.pdf, p1859.pdf, p1869.pdf, p1879.pdf, p1889.pdf, p189.pdf, p1899.pdf, p19.pdf, p1909.pdf, p1919.pdf, p1929.pdf, p1939.pdf, p1949.pdf, p1959.pdf, p1969.pdf, p1979.pdf, p1989.pdf, p199.pdf, p1999.pdf, p2009.pdf, p2019.pdf, p2029.pdf, p2039.pdf, p2049.pdf, p2059.pdf, p2069.pdf, p2079.pdf, p2089.pdf, p209.pdf, p2099.pdf, p2109.pdf, p2119.pdf, p2127.pdf, p2137.pdf, p2147.pdf, p2157.pdf, p2167.pdf, p2177.pdf, p2187.pdf, p219.pdf, p2197.pdf, p2207.pdf, p2217.pdf, p2227.pdf, p2237.pdf, p2247.pdf, p2257.pdf, p2267.pdf, p2277.pdf, p2287.pdf, p229.pdf, p2297.pdf, p2307.pdf, p2309.pdf, p2311.pdf, p2313.pdf, p2315.pdf, p2317.pdf, p2319.pdf, p2321.pdf, p2323.pdf, p2325.pdf, p2327.pdf, p2329.pdf, p239.pdf, p249.pdf, p259.pdf, p269.pdf, p279.pdf, p289.pdf, p29.pdf, p299.pdf, p3.pdf, p309.pdf, p319.pdf, p329.pdf, p339.pdf, p349.pdf, p359.pdf, p369.pdf, p379.pdf, p387.pdf, p39.pdf, p397.pdf, p407.pdf, p417.pdf, p427.pdf, p437.pdf, p447.pdf, p457.pdf, p467.pdf, p477.pdf, p487.pdf, p49.pdf, p497.pdf, p5.pdf, p507.pdf, p517.pdf, p527.pdf, p537.pdf, p547.pdf, p557.pdf, p567.pdf, p577.pdf, p587.pdf, p59.pdf, p597.pdf, p607.pdf, p617.pdf, p627.pdf, p635.pdf, p645.pdf, p655.pdf, p665.pdf, p675.pdf, p685.pdf, p69.pdf, p695.pdf, p7.pdf, p705.pdf, p715.pdf, p725.pdf, p735.pdf, p745.pdf, p755.pdf, p765.pdf, p775.pdf, p785.pdf, p79.pdf, p805.pdf, p815.pdf, p825.pdf, p835.pdf, p845.pdf, p855.pdf, p865.pdf, p875.pdf, p885.pdf, p89.pdf, p895.pdf, p9.pdf, p905.pdf, p915.pdf, p925.pdf, p935.pdf, p945.pdf, p955.pdf, p965.pdf, p975.pdf, p985.pdf, p99.pdf, p995.pdf]\n"
     ]
    }
   ],
   "source": [
    "# Establish an empty list to append filenames as we iterate over the directory with filenames\n",
    "files = []\n",
    "\n",
    "%time\n",
    "start_time = time.time()\n",
    "\n",
    "#**********************************************************************\n",
    "# Core \"workerbee\" code for this section to iterate over directory files\n",
    "#**********************************************************************\n",
    "\n",
    "# Iterate over the directory of filenames and add to list.  Inspection shows our target filenames begin with 'p' and end with 'pdf'\n",
    "for dirName, subdirList, fileList in os.walk(TESTDIR):\n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            files.append(fileName)\n",
    "end_time = time.time()\n",
    "\n",
    "#**********************************************************************\n",
    "# Output\n",
    "#**********************************************************************\n",
    "print\n",
    "print len(files) # Print the number of files\n",
    "print \n",
    "print '[%s]' % ', '.join(map(str, files)) # print the list of filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 253 total files in the directory. We examine the pdf file in its rawest form to get an idea of the format. Here is one example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"./files/journalscreencap.png\" alt=\"Sample of Journal Format\" height=\"700\" width=\"700\" top:\"35\" left:\"170\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We learn a few things immediately. Our data is in PDF format and it's semistructured (follows journal article format with sections like \"abstract\", \"title\").  PDFs are a wonderful human readable presentation of data. But for data analyisis, they are extremely difficult to work with.  If you have an option to get the data BEFORE it was converted to or added to PDF, go for that option.  Save yourself the headache.  In this case however, we have no alternatives outside of the web scraping code linked above.  The web scraping code is imperfect because it is incomplete (only get abstracts and not full-text of journal ariticle) and unordered (multiple authors need to be aligned to specific articles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion: Stripping text from PDFs and creating a custom NLTK corpus\n",
    "\n",
    "The first step in the <href id=\"pipe\"><a href=\"#pipeline\" title=\"Jump back to data science pipeline graphic.\">data science pipeline</a> is to ingest our data.  We use several Python tools which include:\n",
    "\n",
    "* [pdfminer](https://pypi.python.org/pypi/pdfminer/) - this is the tool that makes it ALL happen.  It has a command line tool called \"pdf2text.py\" that extract text contents from a PDF. **This must be installed on your computer BEFORE executing this code**.  Visit the [pdfminer homepage](http://euske.github.io/pdfminer/index.html#pdf2txt) for instructions\n",
    "\n",
    "* [subprocess](https://docs.python.org/2/library/subprocess.html) - a standard library module that allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes.  In this excerise, we use it to invoke the pdf2texy.py command line tool within our code.  \n",
    "\n",
    "* [nltk](http://www.nltk.org/) - another work horse in this exercise.  The Natural Language ToolKit (NLTK) is one of Python's leading platforms to analyze natural language data.  The [NLTK Book](http://www.nltk.org/book/) provides practical guidance on how to handle just about any natural language preprocessing job.  \n",
    "\n",
    "* [string](https://docs.python.org/2/library/string.html) - used for variable substitutions and value formatting to strip non printable characters from the output of the text extracted from our journal article PDFs\n",
    "\n",
    "* [unicodedata](https://docs.python.org/2/library/unicodedata.html) - some unicode characters won't extract nicely. This library allows latin unicode characters to degrade gracefully into ASCII.\n",
    "\n",
    "We are now going to iterate over each file in our raw data directory, strip the text, and write the *.txt* file to newly created directory.  Then we will follow the instructions from [Section 1.9, Chapter 2 of NLTK's Book](http://www.nltk.org/book/ch02.html) to build a custom corpus from our text files.  Having our target documents loaded as an NLTK corpus brings the power of NLTK to our analysis goals.  Let's begin with administrative tasks such as loading modules and creating the necessary directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Importing what we need\n",
    "#**********************************************************************\n",
    "import string\n",
    "import unicodedata\n",
    "import subprocess\n",
    "import nltk\n",
    "import os, os.path\n",
    "import re\n",
    "\n",
    "#**********************************************************************\n",
    "# Create the directory we will write the .txt files to after stripping text\n",
    "#**********************************************************************\n",
    "\n",
    "corpuspath = os.path.normpath(os.path.expanduser('~/Desktop/KDD_corpus/'))\n",
    "if not os.path.exists(corpuspath):\n",
    "    os.mkdir(corpuspath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are to the big task of stripping text from the PDFs.  In the code below, we walk down the directory, and strip text from the files with names that begin with 'p' and end with 'pdf'.  We use the *fileName* variable to name the files we write to disk.  This will come in handy when we load data into NLTK.  Keep in mind, this task takes the longest, so be prepared to wait a a few minutes depending on good your computer is.  If you are doing this in an environment where you can spin up compute resources, your time will be drastically reduced.  Let's begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Core code to iterate over files in the directory\n",
    "#**********************************************************************\n",
    "\n",
    "# We start from the code to iterate over the files\n",
    "%timeit\n",
    "for dirName, subdirList, fileList in os.walk(TESTDIR):\n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            \n",
    "            \n",
    "#**********************************************************************\n",
    "# This code strips the text from the PDFs\n",
    "#**********************************************************************\n",
    "            try:\n",
    "                document = filter(lambda x: x in string.printable,unicodedata.normalize('NFKD', (unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore'))).encode('ascii','ignore').decode('unicode_escape').encode('ascii','ignore'))\n",
    "            except UnicodeDecodeError:\n",
    "                document = unicodedata.normalize('NFKD', unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore')).encode('ascii','ignore')    \n",
    "                \n",
    "            if len(document)<300:\n",
    "                pass\n",
    "            else:\n",
    "                if not os.path.exists(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\"))):\n",
    "                    file = open(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\")), 'w+')\n",
    "                    file.write(document)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "kddcorpus= nltk.corpus.PlaintextCorpusReader(corpuspath, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our data is loaded as an NLTK corpus, meaning we could try tons of techniques outlined in the [NLTK book](http://www.nltk.org/book/). Let's see how many words (including stop words) we have in our entire corpus.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2795267\n"
     ]
    }
   ],
   "source": [
    "wordcount = 0\n",
    "for fileid in kddcorpus.fileids():\n",
    "    wordcount += len(kddcorpus.words(fileid))\n",
    "print wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling and computation: Extracting titles and specific sections of the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sup id=\"fn1\">1. [(2014). Text Mining and its Business Applications - CodeProject. Retrieved December 26, 2015, from http://www.codeproject.com/Articles/822379/Text-Mining-and-its-Business-Applications.]<a href=\"#ref1\" title=\"Jump back to footnote 1 in the text.\">↩</a></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parking Lot of links, leftover paragraphs, ideas, etc.\n",
    "\n",
    "Describe the data -> Data available here http://dl.acm.org/citation.cfm?id=2783258# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\"))):\n",
    "    file = open(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\")), 'w+')\n",
    "    file.write(document)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Matrix Completion with Queries/.txt'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=re.compile('(.+)(\\\\n\\\\n)')\n",
    "p.search(document).group(1)\n",
    "re.sub('[\\s]',\"\",p.search(document).group(1)+\"/\"+\".txt\")\n",
    "re.sub(r'[\\/]',\"\",p.search(document).group(1)+\"/\"+\".txt\")\n",
    "p.search(document).group(1)+\"/\"+\".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p1.txt',\n",
       " 'p1005.txt',\n",
       " 'p1015.txt',\n",
       " 'p1025.txt',\n",
       " 'p1035.txt',\n",
       " 'p1045.txt',\n",
       " 'p1055.txt',\n",
       " 'p1065.txt',\n",
       " 'p1075.txt',\n",
       " 'p1085.txt',\n",
       " 'p109.txt',\n",
       " 'p1095.txt',\n",
       " 'p1105.txt',\n",
       " 'p1115.txt',\n",
       " 'p1125.txt',\n",
       " 'p1135.txt',\n",
       " 'p1145.txt',\n",
       " 'p1155.txt',\n",
       " 'p1165.txt',\n",
       " 'p1175.txt',\n",
       " 'p1185.txt',\n",
       " 'p119.txt',\n",
       " 'p1195.txt',\n",
       " 'p1205.txt',\n",
       " 'p1215.txt',\n",
       " 'p1225.txt',\n",
       " 'p1235.txt',\n",
       " 'p1245.txt',\n",
       " 'p1255.txt',\n",
       " 'p1265.txt',\n",
       " 'p1275.txt',\n",
       " 'p1285.txt',\n",
       " 'p129.txt',\n",
       " 'p1295.txt',\n",
       " 'p1305.txt',\n",
       " 'p1315.txt',\n",
       " 'p1325.txt',\n",
       " 'p1335.txt',\n",
       " 'p1345.txt',\n",
       " 'p1355.txt',\n",
       " 'p1365.txt',\n",
       " 'p1375.txt',\n",
       " 'p1385.txt',\n",
       " 'p139.txt',\n",
       " 'p1395.txt',\n",
       " 'p1405.txt',\n",
       " 'p1415.txt',\n",
       " 'p1425.txt',\n",
       " 'p1435.txt',\n",
       " 'p1445.txt',\n",
       " 'p1455.txt',\n",
       " 'p1465.txt',\n",
       " 'p1475.txt',\n",
       " 'p1485.txt',\n",
       " 'p149.txt',\n",
       " 'p1495.txt',\n",
       " 'p1503.txt',\n",
       " 'p1513.txt',\n",
       " 'p1523.txt',\n",
       " 'p1533.txt',\n",
       " 'p1543.txt',\n",
       " 'p1553.txt',\n",
       " 'p1563.txt',\n",
       " 'p1573.txt',\n",
       " 'p1583.txt',\n",
       " 'p159.txt',\n",
       " 'p1593.txt',\n",
       " 'p1603.txt',\n",
       " 'p1621.txt',\n",
       " 'p1623.txt',\n",
       " 'p1625.txt',\n",
       " 'p1627.txt',\n",
       " 'p1629.txt',\n",
       " 'p1631.txt',\n",
       " 'p1633.txt',\n",
       " 'p1635.txt',\n",
       " 'p1637.txt',\n",
       " 'p1639.txt',\n",
       " 'p1641.txt',\n",
       " 'p1651.txt',\n",
       " 'p1661.txt',\n",
       " 'p1671.txt',\n",
       " 'p1681.txt',\n",
       " 'p169.txt',\n",
       " 'p1691.txt',\n",
       " 'p1701.txt',\n",
       " 'p1711.txt',\n",
       " 'p1721.txt',\n",
       " 'p1731.txt',\n",
       " 'p1741.txt',\n",
       " 'p1751.txt',\n",
       " 'p1759.txt',\n",
       " 'p1769.txt',\n",
       " 'p1779.txt',\n",
       " 'p1789.txt',\n",
       " 'p179.txt',\n",
       " 'p1799.txt',\n",
       " 'p1809.txt',\n",
       " 'p1819.txt',\n",
       " 'p1829.txt',\n",
       " 'p1839.txt',\n",
       " 'p1849.txt',\n",
       " 'p1859.txt',\n",
       " 'p1869.txt',\n",
       " 'p1879.txt',\n",
       " 'p1889.txt',\n",
       " 'p189.txt',\n",
       " 'p1899.txt',\n",
       " 'p19.txt',\n",
       " 'p1909.txt',\n",
       " 'p1919.txt',\n",
       " 'p1929.txt',\n",
       " 'p1939.txt',\n",
       " 'p1949.txt',\n",
       " 'p1959.txt',\n",
       " 'p1969.txt',\n",
       " 'p1979.txt',\n",
       " 'p1989.txt',\n",
       " 'p199.txt',\n",
       " 'p1999.txt',\n",
       " 'p2009.txt',\n",
       " 'p2019.txt',\n",
       " 'p2029.txt',\n",
       " 'p2039.txt',\n",
       " 'p2049.txt',\n",
       " 'p2059.txt',\n",
       " 'p2069.txt',\n",
       " 'p2079.txt',\n",
       " 'p2089.txt',\n",
       " 'p209.txt',\n",
       " 'p2099.txt',\n",
       " 'p2109.txt',\n",
       " 'p2119.txt',\n",
       " 'p2127.txt',\n",
       " 'p2137.txt',\n",
       " 'p2147.txt',\n",
       " 'p2157.txt',\n",
       " 'p2167.txt',\n",
       " 'p2177.txt',\n",
       " 'p2187.txt',\n",
       " 'p219.txt',\n",
       " 'p2197.txt',\n",
       " 'p2207.txt',\n",
       " 'p2217.txt',\n",
       " 'p2227.txt',\n",
       " 'p2237.txt',\n",
       " 'p2247.txt',\n",
       " 'p2257.txt',\n",
       " 'p2267.txt',\n",
       " 'p2277.txt',\n",
       " 'p2287.txt',\n",
       " 'p229.txt',\n",
       " 'p2297.txt',\n",
       " 'p2307.txt',\n",
       " 'p2309.txt',\n",
       " 'p2311.txt',\n",
       " 'p2313.txt',\n",
       " 'p2315.txt',\n",
       " 'p2317.txt',\n",
       " 'p2319.txt',\n",
       " 'p2321.txt',\n",
       " 'p2323.txt',\n",
       " 'p2325.txt',\n",
       " 'p2327.txt',\n",
       " 'p2329.txt',\n",
       " 'p239.txt',\n",
       " 'p249.txt',\n",
       " 'p259.txt',\n",
       " 'p269.txt',\n",
       " 'p279.txt',\n",
       " 'p289.txt',\n",
       " 'p29.txt',\n",
       " 'p299.txt',\n",
       " 'p3.txt',\n",
       " 'p309.txt',\n",
       " 'p319.txt',\n",
       " 'p329.txt',\n",
       " 'p339.txt',\n",
       " 'p349.txt',\n",
       " 'p359.txt',\n",
       " 'p369.txt',\n",
       " 'p379.txt',\n",
       " 'p387.txt',\n",
       " 'p39.txt',\n",
       " 'p397.txt',\n",
       " 'p407.txt',\n",
       " 'p417.txt',\n",
       " 'p427.txt',\n",
       " 'p437.txt',\n",
       " 'p447.txt',\n",
       " 'p457.txt',\n",
       " 'p467.txt',\n",
       " 'p477.txt',\n",
       " 'p487.txt',\n",
       " 'p49.txt',\n",
       " 'p497.txt',\n",
       " 'p5.txt',\n",
       " 'p507.txt',\n",
       " 'p517.txt',\n",
       " 'p527.txt',\n",
       " 'p537.txt',\n",
       " 'p547.txt',\n",
       " 'p557.txt',\n",
       " 'p567.txt',\n",
       " 'p577.txt',\n",
       " 'p587.txt',\n",
       " 'p59.txt',\n",
       " 'p597.txt',\n",
       " 'p607.txt',\n",
       " 'p617.txt',\n",
       " 'p627.txt',\n",
       " 'p635.txt',\n",
       " 'p645.txt',\n",
       " 'p655.txt',\n",
       " 'p665.txt',\n",
       " 'p675.txt',\n",
       " 'p685.txt',\n",
       " 'p69.txt',\n",
       " 'p695.txt',\n",
       " 'p7.txt',\n",
       " 'p705.txt',\n",
       " 'p715.txt',\n",
       " 'p725.txt',\n",
       " 'p735.txt',\n",
       " 'p745.txt',\n",
       " 'p755.txt',\n",
       " 'p765.txt',\n",
       " 'p775.txt',\n",
       " 'p785.txt',\n",
       " 'p79.txt',\n",
       " 'p805.txt',\n",
       " 'p815.txt',\n",
       " 'p825.txt',\n",
       " 'p835.txt',\n",
       " 'p845.txt',\n",
       " 'p855.txt',\n",
       " 'p865.txt',\n",
       " 'p875.txt',\n",
       " 'p885.txt',\n",
       " 'p89.txt',\n",
       " 'p895.txt',\n",
       " 'p9.txt',\n",
       " 'p905.txt',\n",
       " 'p915.txt',\n",
       " 'p935.txt',\n",
       " 'p945.txt',\n",
       " 'p955.txt',\n",
       " 'p965.txt',\n",
       " 'p975.txt',\n",
       " 'p985.txt',\n",
       " 'p99.txt',\n",
       " 'p995.txt']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.PlaintextCorpusReader\n",
    "kddcorpus= nltk.corpus.PlaintextCorpusReader(corpuspath, '.*\\.txt')\n",
    "kddcorpus.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<href id=\"pipe\"><a href=\"#pipeline\" title=\"Jump back to data science pipeline graphic.\">data science pipeline</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "...           (target, fileid[:])\n",
    "...           for fileid in kddcorpus.fileids()\n",
    "...           for w in kddcorpus.words(fileid)\n",
    "...           for target in ['maximum', 'factorization']\n",
    "...           if w.lower().startswith(target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Online',\n",
       " u'Controlled',\n",
       " u'Experiments',\n",
       " u':',\n",
       " u'Lessons',\n",
       " u'from',\n",
       " u'Running',\n",
       " u'A/B/n',\n",
       " u'Tests',\n",
       " u'for',\n",
       " u'12',\n",
       " u'Years',\n",
       " u'Ron',\n",
       " u'Kohavi',\n",
       " u'Distinguished',\n",
       " u'Engineer',\n",
       " u'&',\n",
       " u'General',\n",
       " u'Manager',\n",
       " u',',\n",
       " u'Analysis',\n",
       " u'&',\n",
       " u'Experimentation',\n",
       " u'Microsoft',\n",
       " u'Bellevue',\n",
       " u',',\n",
       " u'WA',\n",
       " u',',\n",
       " u'USA',\n",
       " u'RonKohavi',\n",
       " u'@',\n",
       " u'outlook.com',\n",
       " u'in',\n",
       " u'He',\n",
       " u'2005',\n",
       " u'BIO',\n",
       " u'Ronny',\n",
       " u'Kohavi',\n",
       " u'is',\n",
       " u'a',\n",
       " u'Microsoft',\n",
       " u'Distinguished',\n",
       " u'Engineer',\n",
       " u'and',\n",
       " u'for',\n",
       " u'the',\n",
       " u'General',\n",
       " u'Manager',\n",
       " u'and',\n",
       " u'Microsofts',\n",
       " u'Analysis',\n",
       " u'at',\n",
       " u'Experimentation',\n",
       " u'team',\n",
       " u'Microsoft',\n",
       " u'.',\n",
       " u'joined',\n",
       " u'Microsoft',\n",
       " u'and',\n",
       " u'founded',\n",
       " u'the',\n",
       " u'Experimentation',\n",
       " u'Platform',\n",
       " u'team',\n",
       " u'in',\n",
       " u'2006',\n",
       " u'.',\n",
       " u'He',\n",
       " u'was',\n",
       " u'previously',\n",
       " u'the',\n",
       " u'director',\n",
       " u'of',\n",
       " u'data',\n",
       " u'mining',\n",
       " u'and',\n",
       " u'personalization',\n",
       " u'at',\n",
       " u'Amazon.com',\n",
       " u',',\n",
       " u'and',\n",
       " u'the',\n",
       " u'Vice',\n",
       " u'President',\n",
       " u'of',\n",
       " u'Business',\n",
       " u'Intelligence',\n",
       " u'at',\n",
       " u'Blue',\n",
       " u'Martini',\n",
       " u'Software',\n",
       " u',',\n",
       " u'which',\n",
       " u'went',\n",
       " u'public',\n",
       " u'in',\n",
       " u'2000',\n",
       " u',',\n",
       " u'and',\n",
       " u'later',\n",
       " u'acquired',\n",
       " u'by',\n",
       " u'Red',\n",
       " u'Prairie',\n",
       " u'.',\n",
       " u'Prior',\n",
       " u'to',\n",
       " u'joining',\n",
       " u'Blue',\n",
       " u'Martini',\n",
       " u',',\n",
       " u'Kohavi',\n",
       " u'managed',\n",
       " u'MineSet',\n",
       " u'project',\n",
       " u',',\n",
       " u'Silicon',\n",
       " u'Graphics',\n",
       " u'award-winning',\n",
       " u'product',\n",
       " u'for',\n",
       " u'data',\n",
       " u'mining',\n",
       " u'and',\n",
       " u'visualization',\n",
       " u'.',\n",
       " u'He',\n",
       " u'joined',\n",
       " u'Silicon',\n",
       " u'Graphics',\n",
       " u'after',\n",
       " u'getting',\n",
       " u'a',\n",
       " u'Ph.D.',\n",
       " u'in',\n",
       " u'Machine',\n",
       " u'Learning',\n",
       " u'from',\n",
       " u'Stanford',\n",
       " u'University',\n",
       " u',',\n",
       " u'where',\n",
       " u'he',\n",
       " u'led',\n",
       " u'the',\n",
       " u'MLC++',\n",
       " u'project',\n",
       " u',',\n",
       " u'the',\n",
       " u'Machine',\n",
       " u'Learning',\n",
       " u'library',\n",
       " u'in',\n",
       " u'C++',\n",
       " u'used',\n",
       " u'in',\n",
       " u'MineSet',\n",
       " u'and',\n",
       " u'at',\n",
       " u'Blue',\n",
       " u'Martini',\n",
       " u'Software',\n",
       " u'.',\n",
       " u'Kohavi',\n",
       " u'received',\n",
       " u'his',\n",
       " u'BA',\n",
       " u'from',\n",
       " u'the',\n",
       " u'Technion',\n",
       " u',',\n",
       " u'Israel',\n",
       " u'.',\n",
       " u'He',\n",
       " u'was',\n",
       " u'the',\n",
       " u'General',\n",
       " u'Chair',\n",
       " u'for',\n",
       " u'KDD',\n",
       " u'2004',\n",
       " u',',\n",
       " u'co-chair',\n",
       " u'of',\n",
       " u'KDD',\n",
       " u'99s',\n",
       " u'industrial',\n",
       " u'track',\n",
       " u'with',\n",
       " u'Jim',\n",
       " u'Gray',\n",
       " u',',\n",
       " u'and',\n",
       " u'co-chair',\n",
       " u'of',\n",
       " u'the',\n",
       " u'KDD',\n",
       " u'Cup',\n",
       " u'2000',\n",
       " u'with',\n",
       " u'Carla',\n",
       " u'Brodley',\n",
       " u'.',\n",
       " u'He',\n",
       " u'was',\n",
       " u'an',\n",
       " u'invited',\n",
       " u'speaker',\n",
       " u'at',\n",
       " u'the',\n",
       " u'National',\n",
       " u'Academy',\n",
       " u'of',\n",
       " u'Engineering',\n",
       " u'in',\n",
       " u'2000',\n",
       " u',',\n",
       " u'a',\n",
       " u'keynote',\n",
       " u'speaker',\n",
       " u'at',\n",
       " u'PAKDD',\n",
       " u'2001',\n",
       " u',',\n",
       " u'an',\n",
       " u'invited',\n",
       " u'speaker',\n",
       " u'at',\n",
       " u'KDD',\n",
       " u'2001s',\n",
       " u'industrial',\n",
       " u'track',\n",
       " u',',\n",
       " u'a',\n",
       " u'keynote',\n",
       " u'speaker',\n",
       " u'at',\n",
       " u'EC',\n",
       " u'2010',\n",
       " u'and',\n",
       " u'at',\n",
       " u'Recsys',\n",
       " u'2012',\n",
       " u'.',\n",
       " u'His',\n",
       " u'papers',\n",
       " u'have',\n",
       " u'over',\n",
       " u'26,000',\n",
       " u'citations',\n",
       " u'and',\n",
       " u'three',\n",
       " u'of',\n",
       " u'his',\n",
       " u'papers',\n",
       " u'are',\n",
       " u'in',\n",
       " u'the',\n",
       " u'top',\n",
       " u'1,000',\n",
       " u'most-cited',\n",
       " u'papers',\n",
       " u'in',\n",
       " u'Computer',\n",
       " u'Science',\n",
       " u'.',\n",
       " u'Talk',\n",
       " u'slides',\n",
       " u'are',\n",
       " u'available',\n",
       " u'at',\n",
       " u':',\n",
       " u'http',\n",
       " u':',\n",
       " u'//bit.ly/KDD2015Kohavi',\n",
       " u'Abstract',\n",
       " u'The',\n",
       " u'Internet',\n",
       " u'provides',\n",
       " u'developers',\n",
       " u'of',\n",
       " u'connected',\n",
       " u'software',\n",
       " u',',\n",
       " u'including',\n",
       " u'web',\n",
       " u'sites',\n",
       " u',',\n",
       " u'applications',\n",
       " u',',\n",
       " u'and',\n",
       " u'devices',\n",
       " u',',\n",
       " u'an',\n",
       " u'unprecedented',\n",
       " u'opportunity',\n",
       " u'to',\n",
       " u'accelerate',\n",
       " u'innovation',\n",
       " u'by',\n",
       " u'evaluating',\n",
       " u'ideas',\n",
       " u'quickly',\n",
       " u'and',\n",
       " u'accurately',\n",
       " u'using',\n",
       " u'trustworthy',\n",
       " u'controlled',\n",
       " u'experiments',\n",
       " u'(',\n",
       " u'e.g.',\n",
       " u',',\n",
       " u'A/B',\n",
       " u'tests',\n",
       " u'and',\n",
       " u'their',\n",
       " u'generalizations',\n",
       " u')',\n",
       " u'.',\n",
       " u'From',\n",
       " u'front-end',\n",
       " u'user-',\n",
       " u'interface',\n",
       " u'changes',\n",
       " u'to',\n",
       " u'backend',\n",
       " u'recommendation',\n",
       " u'systems',\n",
       " u'and',\n",
       " u'relevance',\n",
       " u'algorithms',\n",
       " u',',\n",
       " u'from',\n",
       " u'search',\n",
       " u'engines',\n",
       " u'(',\n",
       " u'e.g.',\n",
       " u',',\n",
       " u'Google',\n",
       " u',',\n",
       " u'Microsofts',\n",
       " u'Bing',\n",
       " u',',\n",
       " u'Yahoo',\n",
       " u')',\n",
       " u'to',\n",
       " u'retailers',\n",
       " u'(',\n",
       " u'e.g.',\n",
       " u',',\n",
       " u'Amazon',\n",
       " u',',\n",
       " u'eBay',\n",
       " u',',\n",
       " u'Netflix',\n",
       " u',',\n",
       " u'Etsy',\n",
       " u')',\n",
       " u'to',\n",
       " u'social',\n",
       " u'networking',\n",
       " u'services',\n",
       " u'(',\n",
       " u'e.g.',\n",
       " u',',\n",
       " u'Facebook',\n",
       " u',',\n",
       " u'LinkedIn',\n",
       " u',',\n",
       " u'Twitter',\n",
       " u')',\n",
       " u'to',\n",
       " u'Travel',\n",
       " u'services',\n",
       " u'(',\n",
       " u'e.g.',\n",
       " u',',\n",
       " u'Expedia',\n",
       " u',',\n",
       " u'Airbnb',\n",
       " u',',\n",
       " u'Booking.com',\n",
       " u')',\n",
       " u'to',\n",
       " u'many',\n",
       " u'startups',\n",
       " u',',\n",
       " u'online',\n",
       " u'controlled',\n",
       " u'experiments',\n",
       " u'are',\n",
       " u'now',\n",
       " u'utilized',\n",
       " u'to',\n",
       " u'make',\n",
       " u'data-driven',\n",
       " u'decisions',\n",
       " u'at',\n",
       " u'a',\n",
       " u'wide',\n",
       " u'range',\n",
       " u'of',\n",
       " u'companies',\n",
       " u'.',\n",
       " u'While',\n",
       " u'the',\n",
       " u'theory',\n",
       " u'of',\n",
       " u'a',\n",
       " u'controlled',\n",
       " u'experiment',\n",
       " u'is',\n",
       " u'simple',\n",
       " u',',\n",
       " u'and',\n",
       " u'dates',\n",
       " u'back',\n",
       " u'to',\n",
       " u'Sir',\n",
       " u'Ronald',\n",
       " u'A.',\n",
       " u'Fishers',\n",
       " u'experiments',\n",
       " u'at',\n",
       " u'the',\n",
       " u'Rothamsted',\n",
       " u'Agricultural',\n",
       " u'Experimental',\n",
       " u'Station',\n",
       " u'in',\n",
       " u'England',\n",
       " u'in',\n",
       " u'the',\n",
       " u'1920s',\n",
       " u',',\n",
       " u'the',\n",
       " u'deployment',\n",
       " u'and',\n",
       " u'mining',\n",
       " u'of',\n",
       " u'online',\n",
       " u'controlled',\n",
       " u'experiments',\n",
       " u'at',\n",
       " u'scale',\n",
       " u'(',\n",
       " u'e.g.',\n",
       " u',',\n",
       " u'hundreds',\n",
       " u'of',\n",
       " u'experiments',\n",
       " u'run',\n",
       " u'every',\n",
       " u'day',\n",
       " u'at',\n",
       " u'Bing',\n",
       " u')',\n",
       " u'and',\n",
       " u'deployment',\n",
       " u'of',\n",
       " u'online',\n",
       " u'controlled',\n",
       " u'experiments',\n",
       " u'across',\n",
       " u'dozens',\n",
       " u'of',\n",
       " u'web',\n",
       " u'sites',\n",
       " u'and',\n",
       " u'applications',\n",
       " u'has',\n",
       " u'taught',\n",
       " u'us',\n",
       " u'many',\n",
       " u'lessons',\n",
       " u'.',\n",
       " u'We',\n",
       " u'provide',\n",
       " u'an',\n",
       " u'introduction',\n",
       " u',',\n",
       " u'share',\n",
       " u'real',\n",
       " u'examples',\n",
       " u',',\n",
       " u'key',\n",
       " u'lessons',\n",
       " u',',\n",
       " u'and',\n",
       " u'cultural',\n",
       " u'challenges',\n",
       " u'.',\n",
       " u'Categories',\n",
       " u'and',\n",
       " u'Subject',\n",
       " u'Descriptors',\n",
       " u'G.3',\n",
       " u'Probability',\n",
       " u'and',\n",
       " u'Statistics/Experimental',\n",
       " u'Design',\n",
       " u':',\n",
       " u'controlled',\n",
       " u'experiments',\n",
       " u';',\n",
       " u'randomized',\n",
       " u'experiments',\n",
       " u';',\n",
       " u'A/B',\n",
       " u'testing',\n",
       " u'.',\n",
       " u'General',\n",
       " u'Terms',\n",
       " u'Measurement',\n",
       " u';',\n",
       " u'Design',\n",
       " u';',\n",
       " u'Experimentation',\n",
       " u'Keywords',\n",
       " u'Controlled',\n",
       " u'experiments',\n",
       " u';',\n",
       " u'A/B',\n",
       " u'testing',\n",
       " u';',\n",
       " u'online',\n",
       " u'experiments',\n",
       " u'Permission',\n",
       " u'to',\n",
       " u'make',\n",
       " u'digital',\n",
       " u'or',\n",
       " u'hard',\n",
       " u'copies',\n",
       " u'of',\n",
       " u'part',\n",
       " u'or',\n",
       " u'all',\n",
       " u'of',\n",
       " u'this',\n",
       " u'work',\n",
       " u'for',\n",
       " u'personal',\n",
       " u'or',\n",
       " u'classroom',\n",
       " u'use',\n",
       " u'is',\n",
       " u'granted',\n",
       " u'without',\n",
       " u'fee',\n",
       " u'provided',\n",
       " u'that',\n",
       " u'copies',\n",
       " u'are',\n",
       " u'not',\n",
       " u'made',\n",
       " u'or',\n",
       " u'distributed',\n",
       " u'for',\n",
       " u'profit',\n",
       " u'or',\n",
       " u'commercial',\n",
       " u'advantage',\n",
       " u',',\n",
       " u'and',\n",
       " u'that',\n",
       " u'copies',\n",
       " u'bear',\n",
       " u'this',\n",
       " u'notice',\n",
       " u'and',\n",
       " u'the',\n",
       " u'full',\n",
       " u'citation',\n",
       " u'on',\n",
       " u'the',\n",
       " u'first',\n",
       " u'page',\n",
       " u'.',\n",
       " u'Copyrights',\n",
       " u'for',\n",
       " u'third-party',\n",
       " u'components',\n",
       " u'of',\n",
       " u'this',\n",
       " u'work',\n",
       " u'must',\n",
       " u'be',\n",
       " u'honored',\n",
       " u'.',\n",
       " u'For',\n",
       " u'all',\n",
       " u'other',\n",
       " u'uses',\n",
       " u',',\n",
       " u'contact',\n",
       " u'the',\n",
       " u'owner/author',\n",
       " u'(',\n",
       " u's',\n",
       " u')',\n",
       " u'.',\n",
       " u'Copyright',\n",
       " u'is',\n",
       " u'held',\n",
       " u'by',\n",
       " u'the',\n",
       " u'author/owner',\n",
       " u'(',\n",
       " u's',\n",
       " u')',\n",
       " u'.',\n",
       " u'KDD15',\n",
       " u',',\n",
       " u'August',\n",
       " u'1013',\n",
       " u',',\n",
       " u'2015',\n",
       " u',',\n",
       " u'Sydney',\n",
       " u',',\n",
       " u'NSW',\n",
       " u',',\n",
       " u'Australia',\n",
       " u'.',\n",
       " u'ACM',\n",
       " u'978-1-4503-3664-2/15/08..',\n",
       " u'http',\n",
       " u':',\n",
       " u'//dx.doi.org/10.1145/2783258.2785464',\n",
       " u'1']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(kddcorpus.raw(\"p1.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-f27cdc7f6253>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnum_chars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkddcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkddcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnum_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkddcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnum_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkddcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_chars\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/corpus/reader/util.pyc\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;31m# iterate_from() sets self._len when it reaches the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;31m# of the file:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_toknum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/corpus/reader/util.pyc\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_toknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoknum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_blocknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001b[1;32m    293\u001b[0m                 \u001b[0;34m'block reader %s() should return list or tuple.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/corpus/reader/plaintext.pyc\u001b[0m in \u001b[0;36m_read_sent_block\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_para_block_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             sents.extend([self._word_tokenizer.tokenize(sent)\n\u001b[0;32m--> 124\u001b[0;31m                           for sent in self._sent_tokenizer.tokenize(para)])\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \"\"\"\n\u001b[0;32m-> 1226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \"\"\"\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \"\"\"\n\u001b[1;32m   1303\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m    309\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fileid in kddcorpus.fileids():\n",
    "...     num_chars = len(kddcorpus.raw(fileid))\n",
    "...     num_words = len(kddcorpus.words(fileid))\n",
    "...     num_sents = len(kddcorpus.sents(fileid))\n",
    "...     num_vocab = len(set(w.lower() for w in kddcorpus.words(fileid)))\n",
    "print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "corpuspath = os.path.normpath(os.path.expanduser('~/Desktop/KDD_corpus/'))\n",
    "if not os.path.exists(corpuspath):\n",
    "    os.mkdir(corpuspath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# practice to load document into NLTK\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "text = nltk.Text(kddcorpus.words(\"p1055.txt\"))\n",
    "fdist1 = FreqDist(text)\n",
    "fdist1.most_common(50)\n",
    "\n",
    "filtered_words = [word for word in text if word not in nltk.corpus.stopwords.words('english')]\n",
    "fdist1 = FreqDist(filtered_words)\n",
    "fdist1.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ben's Outline from email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ~~Give a brief introduction to the task, and why it's interesting, important. Then begin to discuss the data set, how you acquired, and where a reader can get access to it.~~ \n",
    "\n",
    "* You then could have a data exploration section where you show the number of documents, perform a word count, show snippets of data (e.g. references) etc that are of interest. \n",
    "\n",
    "* You can then go through one or a few of your \"code to get\" sections. These functions all follow basically the same pattern, so you could probably merge them into a single function, that appropriately selects the right regular expression. \n",
    "\n",
    "* The next step is to discuss, demonstrate your \"truth tests\" for text extraction accuracy. \n",
    "\n",
    "* Finally, you can get to an introduction of your three methods for NERC, and show how do do each of them. Then compare (visually) the results of the three according to the evaluation mechanism discussed above. \n",
    "\n",
    "* You could then conclude with a discussion about NLTK chunk vs. hand labelled entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'Online', u'Controlled', u'Experiments', u':'], [u'Lessons', u'from', u'Running', u'A', u'/', u'B', u'/', u'n', u'Tests', u'for', u'12', u'Years'], ...]\n"
     ]
    }
   ],
   "source": [
    "print repr(kddcorpus.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'PlaintextCorpusReader' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-b0686179bf3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkddcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'PlaintextCorpusReader' has no len()"
     ]
    }
   ],
   "source": [
    "num_chars = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2795267"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_chars = 0\n",
    "for fileid in kddcorpus.fileids():\n",
    "    num_chars += len(kddcorpus.words(fileid))\n",
    "num_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
