{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A ~~Quick~~ Survey and Comparison of Open Source Named Entity Extractor Tools for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity extraction is a core subtask of building knowledge from semi/unstructured text sources<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup>.  Considering recent increases in computing power and decreases in the costs of data storage, data scientists and developers can build large knowledge bases that contain millions of entities and hundreds of millions of facts about them.  These knowledge bases are key contributors to intelligence computer behavior<sup><a href=\"#fn2\" id=\"ref2\">2</a></sup>.  Therefore, named entity extraction is at the core of several popular technologies such as smart assistants ([Siri](http://www.apple.com/ios/siri/), [Google Now](https://www.google.com/landing/now/)), machine reading, and deep interpretation of natural language<sup><a href=\"#fn3\" id=\"ref3\">3</a></sup>.\n",
    "\n",
    "With a realization of how essential it is to recognize information units like names, including person, organization and location names, and numeric expressions including time, date, money\n",
    "and percent expressions, several questions come to mind.  How do you perform named entity extraction, which is formally called “[Named Entity Recognition and Classification (NERC)](https://benjamins.com/catalog/bct.19)”?  What tools are out there?  How can you evaluate their performance?  And most important, what works with Python (shamelessly exposing my bias)?  \n",
    "\n",
    "This post will survey openly available NERC tools and compare the results against hand labeled data for precision, accuracy, and recall.  The tools and basic information extraction principles in this discussion begin the process of structuring unstructured data.\n",
    "\n",
    "We will specifically learn to:\n",
    "1. follow the data science pipeline (see image below)\n",
    "2. prepare semistructured natural language data for ingest using regex\n",
    "3. create a custom corpus in [Natural Language Toolkit](http://www.nltk.org/) \n",
    "4. use a suite of openly available NERC tools to extract entities and store in json format \n",
    "5. compare the performance of NERC tools on our corpus\n",
    "\n",
    "<br>\n",
    "<a href=\"#pipe\" id=\"pipeline\"><center><h3>The Data Science Pipeline:<br>Georgetown Data Science Certificate Program</h3></center></a>\n",
    "<div class=\"image\">\n",
    "\n",
    "      <img src=\"./files/data_science_pipeline.png\" alt=\"Data Science Pipeline\" height=\"300\" width=\"450\" top:\"35\" left:\"170\" />\n",
    "      \n",
    "      \n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data: Peer Reviewed Journals and Keynote Speaker Abstracts from KDD 2014 and 2015\n",
    "\n",
    "Before delving into the pipeline, we need a good dataset.  Jason Brownlee of www.machinelearningmastery.com had some good suggestions in his [August 2015 article](http://machinelearningmastery.com/practice-machine-learning-with-small-in-memory-datasets-from-the-uci-machine-learning-repository/) on picking a dataset for machine learning exercises:  \n",
    "\n",
    "* **Real-World**: The datasets should be drawn from the real world (rather than being contrived). This will keep them interesting and introduce the challenges that come with real data.\n",
    "\n",
    "* **Small**: The datasets need to be small so that you can inspect and understand them and that you can run many models quickly to accelerate your learning cycle.\n",
    "\n",
    "* **Well-Understood**: There should be a clear idea of what the data contains, why it was collected, what the problem is that needs to be solved so that you can frame your investigation.\n",
    "\n",
    "* **Baseline**: It is also important to have an idea of what algorithms are known to perform well and the scores they achieved so that you have a useful point of comparison. This is important when you are getting started and learning because you need quick feedback as to how well you are performing (close to state-of-the-art or something is broken).\n",
    "\n",
    "* **Plentiful**: You need many datasets to choose from, both to satisfy the traits you would like to investigate and (if possible) your natural curiosity and interests. \n",
    "\n",
    "Luckily, we have a dataset that meets nearly all of these requirements.  I attended the Knowledge Discovery and Data Mining (KDD) conferences in [New York City (2014)](http://www.kdd.org/kdd2014/) and [Sydney, Australia (2015)](http://www.kdd.org/kdd2015/).  Both years, attendees received a USB with the conference proceedings.  Each repository contains over 230 peer reviewed journal articles and keynote speaker abstracts on data mining, knowledge discovery, big data, data science and their applications. The full conference proceedings can be purchased for \\$60 at the [Association for Computing Machinery's Digital Library](https://dl.acm.org/purchase.cfm?id=2783258&CFID=740512201&CFTOKEN=34489585) (includes ACM membership). This post will work with a dataset that is equivalent to the conference proceedings.  It's important to note that this dataset recreates a real word data science exercise that is instructive of big data problems.  We will take semi-structured data (PDF journal articles and abstracts in publication format), strip text from the files, and add more structure to the data that would facilitate follow on analysis. \n",
    "\n",
    "<blockquote cite=\"https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/KDDwebscrape.ipynb\">\n",
    "Interested parties looking for a free option can use the <a href=\"https://pypi.python.org/pypi/beautifulsoup4/4.4.1\">beautifulsoup</a> and <a href=\"https://pypi.python.org/pypi/requests/2.9.1\">request</a> libraries to scrape the <a href=\"http://dl.acm.org/citation.cfm?id=2785464&CFID=740512201&CFTOKEN=3448958\">ACM website for KDD 2015 conference data</a> that can be used in natural language processing pipelines.  I have some <a href=\"https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/KDDwebscrape.ipynb\">skeleton web scraping code</a> to generate lists of all abstracts, author names, and journal/keynote address titles.    \n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration: Getting the number of files, and file type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored locally in the following directory:\n",
    "```python\n",
    ">>> import os\n",
    ">>> print os.getcwd()\n",
    "/Users/linwood/Desktop/KDD_15/docs\n",
    "```\n",
    "Let's explore the number of files we have and naming conventions. We begin with the administrative tasks of loading modules, establishing paths, etc.  \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Importing what we need\n",
    "#**********************************************************************\n",
    "import os\n",
    "import time\n",
    "from os import walk\n",
    "\n",
    "#**********************************************************************\n",
    "# Administrative code to set the path for file loading\n",
    "#**********************************************************************\n",
    "\n",
    "path        = os.path.abspath(os.getcwd())\n",
    "TESTDIR     = os.path.normpath(os.path.join(os.path.expanduser(\"~\"),\"Desktop\",\"KDD_15\",\"docs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Next we iterate over the files in the directory and store those names in the empty list we created called *files*.  We time the operation, print list with the file names and also print out the length of the list (gives number of target files).<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n",
      "\n",
      "253\n",
      "\n",
      "[p1.pdf, p1005.pdf, p1015.pdf, p1025.pdf, p1035.pdf, p1045.pdf, p1055.pdf, p1065.pdf, p1075.pdf, p1085.pdf, p109.pdf, p1095.pdf, p1105.pdf, p1115.pdf, p1125.pdf, p1135.pdf, p1145.pdf, p1155.pdf, p1165.pdf, p1175.pdf, p1185.pdf, p119.pdf, p1195.pdf, p1205.pdf, p1215.pdf, p1225.pdf, p1235.pdf, p1245.pdf, p1255.pdf, p1265.pdf, p1275.pdf, p1285.pdf, p129.pdf, p1295.pdf, p1305.pdf, p1315.pdf, p1325.pdf, p1335.pdf, p1345.pdf, p1355.pdf, p1365.pdf, p1375.pdf, p1385.pdf, p139.pdf, p1395.pdf, p1405.pdf, p1415.pdf, p1425.pdf, p1435.pdf, p1445.pdf, p1455.pdf, p1465.pdf, p1475.pdf, p1485.pdf, p149.pdf, p1495.pdf, p1503.pdf, p1513.pdf, p1523.pdf, p1533.pdf, p1543.pdf, p1553.pdf, p1563.pdf, p1573.pdf, p1583.pdf, p159.pdf, p1593.pdf, p1603.pdf, p1621.pdf, p1623.pdf, p1625.pdf, p1627.pdf, p1629.pdf, p1631.pdf, p1633.pdf, p1635.pdf, p1637.pdf, p1639.pdf, p1641.pdf, p1651.pdf, p1661.pdf, p1671.pdf, p1681.pdf, p169.pdf, p1691.pdf, p1701.pdf, p1711.pdf, p1721.pdf, p1731.pdf, p1741.pdf, p1751.pdf, p1759.pdf, p1769.pdf, p1779.pdf, p1789.pdf, p179.pdf, p1799.pdf, p1809.pdf, p1819.pdf, p1829.pdf, p1839.pdf, p1849.pdf, p1859.pdf, p1869.pdf, p1879.pdf, p1889.pdf, p189.pdf, p1899.pdf, p19.pdf, p1909.pdf, p1919.pdf, p1929.pdf, p1939.pdf, p1949.pdf, p1959.pdf, p1969.pdf, p1979.pdf, p1989.pdf, p199.pdf, p1999.pdf, p2009.pdf, p2019.pdf, p2029.pdf, p2039.pdf, p2049.pdf, p2059.pdf, p2069.pdf, p2079.pdf, p2089.pdf, p209.pdf, p2099.pdf, p2109.pdf, p2119.pdf, p2127.pdf, p2137.pdf, p2147.pdf, p2157.pdf, p2167.pdf, p2177.pdf, p2187.pdf, p219.pdf, p2197.pdf, p2207.pdf, p2217.pdf, p2227.pdf, p2237.pdf, p2247.pdf, p2257.pdf, p2267.pdf, p2277.pdf, p2287.pdf, p229.pdf, p2297.pdf, p2307.pdf, p2309.pdf, p2311.pdf, p2313.pdf, p2315.pdf, p2317.pdf, p2319.pdf, p2321.pdf, p2323.pdf, p2325.pdf, p2327.pdf, p2329.pdf, p239.pdf, p249.pdf, p259.pdf, p269.pdf, p279.pdf, p289.pdf, p29.pdf, p299.pdf, p3.pdf, p309.pdf, p319.pdf, p329.pdf, p339.pdf, p349.pdf, p359.pdf, p369.pdf, p379.pdf, p387.pdf, p39.pdf, p397.pdf, p407.pdf, p417.pdf, p427.pdf, p437.pdf, p447.pdf, p457.pdf, p467.pdf, p477.pdf, p487.pdf, p49.pdf, p497.pdf, p5.pdf, p507.pdf, p517.pdf, p527.pdf, p537.pdf, p547.pdf, p557.pdf, p567.pdf, p577.pdf, p587.pdf, p59.pdf, p597.pdf, p607.pdf, p617.pdf, p627.pdf, p635.pdf, p645.pdf, p655.pdf, p665.pdf, p675.pdf, p685.pdf, p69.pdf, p695.pdf, p7.pdf, p705.pdf, p715.pdf, p725.pdf, p735.pdf, p745.pdf, p755.pdf, p765.pdf, p775.pdf, p785.pdf, p79.pdf, p805.pdf, p815.pdf, p825.pdf, p835.pdf, p845.pdf, p855.pdf, p865.pdf, p875.pdf, p885.pdf, p89.pdf, p895.pdf, p9.pdf, p905.pdf, p915.pdf, p925.pdf, p935.pdf, p945.pdf, p955.pdf, p965.pdf, p975.pdf, p985.pdf, p99.pdf, p995.pdf]\n"
     ]
    }
   ],
   "source": [
    "# Establish an empty list to append filenames as we iterate over the directory with filenames\n",
    "files = []\n",
    "\n",
    "%time\n",
    "start_time = time.time()\n",
    "\n",
    "#**********************************************************************\n",
    "# Core \"workerbee\" code for this section to iterate over directory files\n",
    "#**********************************************************************\n",
    "\n",
    "# Iterate over the directory of filenames and add to list.  Inspection shows our target filenames begin with 'p' and end with 'pdf'\n",
    "for dirName, subdirList, fileList in os.walk(TESTDIR):\n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            files.append(fileName)\n",
    "end_time = time.time()\n",
    "\n",
    "#**********************************************************************\n",
    "# Output\n",
    "#**********************************************************************\n",
    "print\n",
    "print len(files) # Print the number of files\n",
    "print \n",
    "print '[%s]' % ', '.join(map(str, files)) # print the list of filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>There are 253 total files in the directory. We examine the pdf file in its rawest form to get an idea of the format. Here is one example:<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"./files/journalscreencap.png\" alt=\"Sample of Journal Format\" height=\"700\" width=\"700\" top:\"35\" left:\"170\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br><br>We learn a few things immediately. Our data is in PDF format and it's semistructured (follows journal article format with sections like \"abstract\", \"title\").  PDFs are a wonderful human readable presentation of data. But for data analyisis, they are extremely difficult to work with.  If you have an option to get the data BEFORE it was converted to or added to PDF, go for that option.  If it's your only option, be prepared for a lot of these moments:\n",
    "\n",
    "![Pulling hair out](http://i1012.photobucket.com/albums/af243/njmike731/man-pulling-hair-out-2-773892-1.jpg)\n",
    "\n",
    "In today's exercise, we have no alternatives outside of the web scraping code linked above.  In full disclosure, that code is imperfect because we get an incomplete dataset.  The abstracts and authors are not matched to the papers and we don't pull in the references section. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion: Stripping text from PDFs and creating a custom NLTK corpus\n",
    "\n",
    "The first step in the <href id=\"pipe\"><a href=\"#pipeline\" title=\"Jump back to data science pipeline graphic.\">data science pipeline</a> is to ingest our data.  We use several Python tools which include:\n",
    "\n",
    "* [pdfminer](https://pypi.python.org/pypi/pdfminer/) - this is the tool that makes it ALL happen.  It has a command line tool called \"pdf2text.py\" that extract text contents from a PDF. **This must be installed on your computer BEFORE executing this code**.  Visit the [pdfminer homepage](http://euske.github.io/pdfminer/index.html#pdf2txt) for instructions\n",
    "\n",
    "* [subprocess](https://docs.python.org/2/library/subprocess.html) - a standard library module that allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes.  In this excerise, we use it to invoke the pdf2texy.py command line tool within our code.  \n",
    "\n",
    "* [nltk](http://www.nltk.org/) - another work horse in this exercise.  The Natural Language ToolKit (NLTK) is one of Python's leading platforms to analyze natural language data.  The [NLTK Book](http://www.nltk.org/book/) provides practical guidance on how to handle just about any natural language preprocessing job.  \n",
    "\n",
    "* [string](https://docs.python.org/2/library/string.html) - used for variable substitutions and value formatting to strip non printable characters from the output of the text extracted from our journal article PDFs\n",
    "\n",
    "* [unicodedata](https://docs.python.org/2/library/unicodedata.html) - some unicode characters won't extract nicely. This library allows latin unicode characters to degrade gracefully into ASCII.\n",
    "\n",
    "We are now going to iterate over each file in our raw data directory, strip the text, and write the *.txt* file to newly created directory.  Then we will follow the instructions from [Section 1.9, Chapter 2 of NLTK's Book](http://www.nltk.org/book/ch02.html) to build a custom corpus from our text files.  Having our target documents loaded as an NLTK corpus brings the power of NLTK to our analysis goals.  Let's begin with administrative tasks such as loading modules and creating the necessary directories.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Importing what we need\n",
    "#**********************************************************************\n",
    "import string\n",
    "import unicodedata\n",
    "import subprocess\n",
    "import nltk\n",
    "import os, os.path\n",
    "import re\n",
    "\n",
    "#**********************************************************************\n",
    "# Create the directory we will write the .txt files to after stripping text\n",
    "#**********************************************************************\n",
    "\n",
    "corpuspath = os.path.normpath(os.path.expanduser('~/Desktop/KDD_corpus/'))\n",
    "if not os.path.exists(corpuspath):\n",
    "    os.mkdir(corpuspath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Now we are to the big task of stripping text from the PDFs.  In the code below, we walk down the directory, and strip text from the files with names that begin with 'p' and end with 'pdf'.  We use the *fileName* variable to name the files we write to disk.  This will come in handy when we load data into NLTK.  Keep in mind, this task takes the longest, so be prepared to wait a a few minutes depending on good your computer is.  If you are doing this in an environment where you can spin up compute resources, your time will be drastically reduced.  Let's begin.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "# Core code to iterate over files in the directory\n",
    "#**********************************************************************\n",
    "\n",
    "# We start from the code to iterate over the files\n",
    "%timeit\n",
    "for dirName, subdirList, fileList in os.walk(TESTDIR):\n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            if os.path.exists(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\"))):\n",
    "                pass\n",
    "            else:\n",
    "            \n",
    "            \n",
    "#**********************************************************************\n",
    "# This code strips the text from the PDFs\n",
    "#**********************************************************************\n",
    "                try:\n",
    "                    document = filter(lambda x: x in string.printable,unicodedata.normalize('NFKD', (unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore'))).encode('ascii','ignore').decode('unicode_escape').encode('ascii','ignore'))\n",
    "                except UnicodeDecodeError:\n",
    "                    document = unicodedata.normalize('NFKD', unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore')).encode('ascii','ignore')    \n",
    "\n",
    "                if len(document)<300:\n",
    "                    pass\n",
    "                else:\n",
    "                    # used this for assistance http://stackoverflow.com/questions/2967194/open-in-python-does-not-create-a-file-if-it-doesnt-exist\n",
    "                    if not os.path.exists(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\"))):\n",
    "                        file = open(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\")), 'w+')\n",
    "                        file.write(document)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "kddcorpus= nltk.corpus.PlaintextCorpusReader(corpuspath, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kddcorpus= nltk.corpus.PlaintextCorpusReader(corpuspath, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>This is a pretty big step.  We have a semi-structured data set in a format where we can query and analyze different pieces of data.  All of our data is loaded as an NLTK corpus, meaning we could try tons of techniques outlined in the [NLTK book](http://www.nltk.org/book/) or use the NLTK APIs to pass data into [scikit-learn machine learning pipelines for text](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) (maybe for a later blog). Let's see how many words (including stop words) we have in our entire corpus.  <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2795267\n"
     ]
    }
   ],
   "source": [
    "wordcount = 0\n",
    "for fileid in kddcorpus.fileids():\n",
    "    wordcount += len(kddcorpus.words(fileid))\n",
    "print wordcount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step didn't come off without it's errors.  We got a little bit of gobbledygook (this is a [real word](http://www.merriam-webster.com/dictionary/gobbledygook)). Here are the first 1000 characters of document 2157:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")UDX'HWHFWRU\u001d",
      "\u0003$\u0003*UDSK\u00100LQLQJ\u0010EDVHG\u0003)UDPHZRUN\u0003\u0003\n",
      "\n",
      "IRU\u0003)UDXGXOHQW\u00033KRQH\u0003&DOO\u0003'HWHFWLRQ\u0003\n",
      "\n",
      "9LQFHQW\u00036\u0011\u00037VHQJ\u0014\r",
      "\u000f\u0003-RVK\u0003-LD\u0010&KLQJ\u0003<LQJ\u0014\u000f\u0003&KH\u0010:HL\u0003+XDQJ\u0015\u000f\u0003<LPLQ\u0003.DR\u0016\u000f\u0003DQG\u0003.XDQ\u00107D\u0003&KHQ\u0017\u0003\n",
      "\n",
      "\u0014\u0003'HSDUWPHQW\u0003RI\u0003&RPSXWHU\u00036FLHQFH\u000f\u00031DWLRQDO\u0003&KLDR\u00037XQJ\u00038QLYHUVLW\\\u000f\u00037DLZDQ\u000f\u000352&\u0003\n",
      "\n",
      "\u0015\u0003'HSDUWPHQW\u0003RI\u0003&RPSXWHU\u00036FLHQFH\u0003DQG\u0003,QIRUPDWLRQ\u0003(QJLQHHULQJ\u000f\u00031DWLRQDO\u0003&KHQJ\u0003.XQJ\u00038QLYHUVLW\\\u000f\u00037DLZDQ\u000f\u000352&\u0003\n",
      "\n",
      "\u0016\u0003*RJRORRN\u0003&R\u0011\u0003/WG\u0011\u000f\u00037DLZDQ\u000f\u000352&\u0003\n",
      "\n",
      "\u0017\u0003,QVWLWXWH\u0003RI\u0003,QIRUPDWLRQ\u00036FLHQFH\u000f\u0003$FDGHPLD\u00036LQLFD\u000f\u00037DLZDQ\u000f\u000352&\u0003\n",
      "\n",
      "MDVK\\LQJ#JPDLO\u0011FRP\u000f\u0003ZHLLER\\#LGE\u0011FVLH\u0011QFNX\u0011HGX\u0011WZ\u000f\u0003\\LPLQNDR#JRJRORRN\u0011FRP\u000f\u0003VZF#LLV\u0011VLQLFD\u0011HGX\u0011WZ\u0003\n",
      "\n",
      "\r",
      "&RUUHVSRQGHQFH\u001d",
      " YWVHQJ#FV\u0011QFWX\u0011HGX\u0011WZ\u000f\u0003\n",
      "\n",
      "$%675$&7\u0003\n",
      ",Q\u0003UHFHQW\u0003\\HDUV\u000f\u0003IUDXG\u0003LV\u0003LQFUHDVLQJ\u0003UDSLGO\\\u0003ZLWK\u0003WKH\u0003GHYHORSPHQW\u0003RI\u0003\n",
      "PRGHUQ\u0003 WHFKQRORJ\\\u0003 DQG\u0003 JOREDO\u0003 FRPPXQLFDWLRQ\u0011\u0003 $OWKRXJK\u0003 PDQ\\\u0003\n",
      "OLWHUDWXUHV\u0003 KDYH\u0003 DGGUHVVHG\u0003 WKH\u0003 IUDXG\u0003 GHWHFWLRQ\u0003 SUREOHP\u000f\u0003 WKHVH\u0003\n",
      "H[LVWLQJ\u0003 ZRUNV\u0003 IRFXV\u0003 RQO\\\u0003 RQ\u0003 IRUPXODWLQJ\u0003 WKH\u0003 IUDXG\u0003 GHWHFWLRQ\u0003\n",
      "SUREOHP\u0003 DV\u0003 D\u0003 ELQDU\\\u0003 FODVVLILFDWLRQ\u0003 SUREOHP\u0011\u0003 'XH\u0003 WR\u0003 OLPLWDWLRQ\u0003 RI\u0003\n",
      "LQIRUPDWLRQ\u0003SURYLGHG\u0003E\\\u0003WHOHFRPPXQL\n"
     ]
    }
   ],
   "source": [
    "print kddcorpus.raw(\"p2157.txt\")[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>The NLTK book has an [excellent section on processing raw text and unicode issues](http://www.nltk.org/book/ch03.html#fig-unicode). I could never figure out what caused the error above but that's a dose of real world data problems.   Let's move on.  To begin our exploration of regular expressions (aka \"regex\"), it's important to point out some good resources to brush up on the topic.  The best resource I ever had was in [Videos 1-3, Week 4, Getting and Cleaning Data, Data Science Specialization Track](https://www.coursera.org/learn/data-cleaning) (At Coursera by Johns Hopkins University).  The instruction and examples in these helped me UNDERSTAND how to use regex vice googling [\"how to match text between two strings python regex\"](https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=how+to+match+text+between+two+strings+python+regex) and hacking away until getting the desired output.  When you understand regex, you will start to use metacharacter expression matches vice using literal matches, and crush any text matching requirment.  Here are some learning resources listed in my own subjective order of usefulness and relevance to python:\n",
    "* http://regexone.com/ (interactive teaching)\n",
    "* https://regex101.com/ (interactive testing; you can paste your text and test expressions)\n",
    "* http://regexr.com/ (interactive testing like above)\n",
    "* http://www.learnpython.org/en/Regular_Expressions (not very intuitive at first glimpse, but useful)\n",
    "* https://docs.python.org/2/library/re.html (default Python library documentation on regex)\n",
    "\n",
    "<br>As a quick test, we extract some \"good enough\" titles from the first 26 documents.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online Controlled Experiments:\n",
      "Mining Frequent Itemsets through Progressive Sampling\n",
      "Why It Happened: Identifying and Modeling the Reasons of\n",
      "Matrix Completion with Queries\n",
      "Stochastic Divergence Minimization\n",
      "Bayesian Poisson Tensor Factorization for Inferring\n",
      "TimeCrunch: Interpretable Dynamic Graph Summarization\n",
      "Inside Jokes: Identifying Humorous Cartoon Captions\n",
      "Community Detection based on Distance Dynamics\n",
      "Discovery of Meaningful Rules in Time Series\n",
      "On the Formation of Circles in Co-authorship Networks\n",
      "An Evaluation of Parallel Eccentricity Estimation\n",
      "Efcient Latent Link Recommendation in\n",
      "Turn Waste into Wealth: On Simultaneous Clustering and\n",
      "Set Cover at Web Scale\n",
      "Exploiting Relevance Feedback in Knowledge Graph\n",
      "LINKAGE: An Approach for Comprehensive Risk\n",
      "Transitive Transfer Learning\n",
      "PTE: Predictive Text Embedding through Large-scale\n",
      "An Effective Marketing Strategy for Revenue Maximization\n",
      "Scaling Up Stochastic Dual Coordinate Ascent\n",
      "Heterogeneous Network Embedding via Deep\n",
      "Discovering Valuable Items from Massive Data\n",
      "Deep Learning Architecture with Dynamically Programmed\n",
      "Incorporating World Knowledge to Document Clustering\n"
     ]
    }
   ],
   "source": [
    "# code uses regular expression to extract text up to the first new line character\n",
    "\n",
    "p=re.compile('^(.*)([\\s]){2}[A-z]+[\\s]+.+')\n",
    "for fileid in kddcorpus.fileids()[:25]:\n",
    "    print p.search(kddcorpus.raw(fileid)).group(1).strip()  # use .strip() to remove whitespace from beginning and end of string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling and computation: Using Regular Expressions to extract specific sections of the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are close to the NERC portion.  But, there's a bit more wrangling to do (remember, PDFs are tough work).  For simplicity, let's focus the NERC on two sections of the paper:\n",
    "* the top section which includes authors and schools\n",
    "* the references section of the paper (keynote speaker abstracts do not have an abstract)\n",
    "\n",
    "The tools of choice to extract sections are the [\"positive lookbehind\" and \"positive lookahead\"](https://docs.python.org/2/library/re.html) expressions. Here is an example of code to extract the abstract only:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The collapsed variational Bayes zero (CVB0) inference is a vari- ational inference improved by marginalizing out parameters, the same as with the collapsed Gibbs sampler. A drawback of the CVB0 inference is the memory requirements. A probability vec- tor must be maintained for latent topics for every token in a corpus. When the total number of tokens is N and the number of topics is K, the CVB0 inference requires O(N K) memory. A stochas- tic approximation of the CVB0 (SCVB0) inference can reduce O(N K) to O(V K), where V denotes the vocabulary size. We re- formulate the existing SCVB0 inference by using the stochastic di- vergence minimization algorithm, with which convergence can be analyzed in terms of Martingale convergence theory. We also reveal the property of the CVB0 inference in terms of the leave-one-out perplexity, which leads to the estimation algorithm of the Dirichlet distribution parameters. The predictive performance of the propose SCVB0 inference is better than that of the original SCVB0 infer- ence in four datasets.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set our regular expression\n",
    "p= re.compile('(?<=ABSTRACT)(.+)(?=Categories and Subject Descriptors)')\n",
    "try:\n",
    "    abstract= p.search(re.sub('[\\s]',\" \",kddcorpus.raw('p1035.txt'))).group(1)\n",
    "except AttributeError:\n",
    "    # include a lowercase regex match incase consistency is a problem\n",
    "    p=re.compile('(?<=abstract)(.+)(?=categories and subject descriptors)')\n",
    "    abstract=p.search(re.sub('[\\s]',\" \",holder.lower())).group(1)\n",
    "else:\n",
    "    pass\n",
    "unicodedata.normalize('NFKD', abstract).encode('ascii','ignore').strip() # convert output from unicode to string and strip leading and trailing whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!  Now, to be \"pythonic\" we build two functions that can extract the top and references section of the documents.  For fun, I also made other function to extract the keywords and abstract sections of the documents.  We could do the same for any section of paper although I must provide a warning.  **Working with natural language is a messy ordeal!**  This is a top notch organization (ACM) and a top notch conference (KDD) but human error sitll makes it way into the picture:\n",
    "\n",
    "![Human Error](http://www.process-improvement-institute.com/wp-content/uploads/2015/05/Accounting-for-Human-Error-Probability-in-SIL-Verification.jpg)\n",
    "\n",
    "Specifically in our case:\n",
    "* paper 1 header section = \"Categories and Subject Descriptors\"\n",
    "* paper 2 header section = \"Categories & Subject Descriptors\"\n",
    "\n",
    "Very small difference but these types of differences cause TONS of headaches.  The result?  You have a decision to make: **account for these differences or ignore them**.  I worked to include AS MUCH of the 253 corpus as possible in the results but it's never perfect.  There are also some documents that will be missing sections altogether (i.e. keynote speaker documents do not have a references section.  Our two functions will:\n",
    "\n",
    "1. Extract only the relevant text for the section we seek\n",
    "2. Extract a character count for the section\n",
    "3. Make additonal calculations or extractions\n",
    "  * the top section extraction also extract emails\n",
    "  * we count the number of references and store that value\n",
    "  * as added benefit, we create a simple \"word per reference\" calculation\n",
    "4. Store all the above data as a nested dictionary with the filename as a key\n",
    "\n",
    "These are loooooong blocks of code to accomplish the task above.  For now, we will only show the code to extract the references and perform the quick analysis mentioned above.  The other functions will be in the appendix.  In fairness, all functions could be reduced down to one function composed of nested function calls.  We will save that for later and get the \"functionality\" working before optimizing the code. See the comments below to follow along or just skip to the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code to pull the ferences section only, store a character count, number of references, and \"word per reference\" calculation\n",
    "\n",
    "def refpull(docnum=None,section='references',full = False):\n",
    "    \n",
    "    # Establish an empty dictionary to hold values\n",
    "    ans={}\n",
    "    \n",
    "    # Establish an empty list to hold document ids that don't make the cut (i.e. missing reference section or different format)\n",
    "    # This comes in handy when you are trying to improve your code to catch outliers\n",
    "    failids = []\n",
    "    \n",
    "    # Eliminate any variations you can; covert everything to lowercase.\n",
    "    # This can cause problems because a titlecase \"Abstract\" could be a section header but \"abstract\" can be an adjective in a sentence\n",
    "    section = section.lower()\n",
    "    \n",
    "    # Admin code to set default values and raise an exception if there's human error on input\n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        # We set our text file here; this is what our regular expression will work on\n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        \n",
    "        # This first condtional is for pulling the target section for ALL documents in the corpus\n",
    "        if full == True:\n",
    "            \n",
    "            # Iterate over the corpus to get the id\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                \n",
    "                # This for loop passes the fileid from above into the nltk format to retrieve the raw text\n",
    "                # again, we eliminate any variations by making this lowercase, and append .lower() to the end\n",
    "                text = kddcorpus.raw(fileid).lower()\n",
    "                \n",
    "                # The default section for this function is references, but we set it again to be sure\n",
    "                if section == \"references\":\n",
    "                    \n",
    "                    # These lines of code build our regular expression.\n",
    "                    # In the other functions for abstract or keywords, you see how I use this technique to create different regex arugments\n",
    "                    section1=\"references \\[\" \n",
    "                    target = \"\"   \n",
    "                    part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "                    # We use our regex on the target file; remember, we're still in the loop so it applies to every file\n",
    "                    for sect in section1:\n",
    "                        \n",
    "                        # Using try/except to avoid pesky errors that stop our process; failed processing will append ids \\nto the \"failedids\" list\n",
    "                        try:\n",
    "                            # our regex\n",
    "                            p=re.compile(part1)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "\n",
    "                            # conditional to make sure we pull in references that have more than 50 characters; we don't want empty references\n",
    "                            if len(target) > 50:\n",
    "\n",
    "                                # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                                try:\n",
    "                                    refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                                except:\n",
    "                                    print \"This file does not appear to have a references section\"\n",
    "                                    pass\n",
    "                                \n",
    "                                # we pass the values into a nested dictionary for retrieval\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"references\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                ans[str(fileid)][\"refcount\"]= refnum\n",
    "                                ans[str(fileid)][\"wordperRef\"]=float(len(nltk.word_tokenize(text)))/float(refnum)\n",
    "                                #print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                                break\n",
    "                            else:\n",
    "\n",
    "                                pass\n",
    "                        \n",
    "                        # Anything that failed, gets appended to a failed list for later testing\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "        \n",
    "            return ans\n",
    "            return failids\n",
    "                              \n",
    "        # The code below is to extract the target section from one document; same functionality as above\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum).lower()\n",
    "        \n",
    "        if section == \"top\":\n",
    "            section1=\"\"\n",
    "            section2=[\"abstract\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if len(target)> 1000:\n",
    "                        if len(target) > 3000 and float(len(target))/float(len(text)) > .22:\n",
    "                            target = target[:2500]\n",
    "                        else:\n",
    "                            target=target\n",
    "                        ans[str(docnum)]={}\n",
    "                        ans[str(docnum)][\"references\"]=target.strip()\n",
    "                        ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                        ans[str(docnum)][\"refcount\"]= refnum\n",
    "                        ans[str(docnum)][\"wordperRef\"]=float(len(nltk.word_tokenize(text)))/float(refnum)\n",
    "\n",
    "                        #print [fileid,len(target),len(text)]\n",
    "                        break\n",
    "                     \n",
    "                    else:\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "                \n",
    "        return ans\n",
    "        return failids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a big block of code!  Don't fret, there are several similar blocks in the appendix to extract the abstract and keywords.  Data is messy; this is what cleaning looks like.  In the code above, we also make use of the *nltk.word_tokenize* tool to create the \"word per reference\" figure.  Let's test our function and some output (the word_tokenize calculation will take some time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# call our function, setting \"full=True\" extracts ALL references in corpus\n",
    "test = refpull(full=True)\n",
    "\n",
    "# To get a quick glimpse, I use the example from this page: http://stackoverflow.com/questions/7971618/python-return-first-n-keyvalue-pairs-from-dict\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "man = collections.OrderedDict(test)\n",
    "\n",
    "x = itertools.islice(man.items(), 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename      Character Count    Number of references    Words per Reference\n",
      "----------  -----------------  ----------------------  ---------------------\n",
      "p835.txt                 5345                      37                326.189\n",
      "p865.txt                 5267                      26                412.962\n",
      "p2089.txt                8732                      44                184.455\n",
      "p815.txt                 7775                      60                195.967\n",
      "p2099.txt                3947                      27                386.889\n",
      "p785.txt                 4279                      36                323.667\n",
      "p725.txt                 5769                      36                311.111\n",
      "p597.txt                 6417                      35                306.143\n",
      "p1789.txt                6742                      35                297.486\n",
      "p577.txt                 6694                      42                203.905\n"
     ]
    }
   ],
   "source": [
    "# Let's use a nifty table module to print this all pretty like: https://pypi.python.org/pypi/tabulate\n",
    "# The joy of Python and open source: someone has created something to do what you want; Google is your friend.  \n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "# A quick list comprehension to follow the example on the tabulate pypi page\n",
    "table = [[key,value['charcount'],value['refcount'], value['wordperRef']] for key,value in x]\n",
    "\n",
    "# print the pretty table; we invoke the \"header\" argument and assign custom header!!!!\n",
    "print tabulate(table,headers=[\"filename\",\"Character Count\", \"Number of references\",\"Words per Reference\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data computation and analyses: Using NERC tools and examining for accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are in the spot where data scientists WANT to live: computation and analyses!!!  In truth, most spend their time ingesting, wrangling, and munging data, as you see above. \n",
    "\n",
    "We are ready to test how well some open source NERC tools extract names, places, and organizations from the top and reference sections of our corpus.  As an added benefit (using the web scraping code from above), we can do a comparison to see how well our pdf-ingest-scrape-regex-NERC pipeline works compared to old-fashioned web scraping.  \n",
    "\n",
    "We start with a few hand labeled documents.  Hand labeling is an expensive and tedious process; the entities for two documents I labeled (yea..it's only 2 but that was 295 cut-and-pastes not counting writing the list names):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 authors\n",
      "\n",
      "There are 3 author organizations\n",
      "\n",
      "There are 7 author locations\n",
      "\n",
      "There are 152 authors in the references\n"
     ]
    }
   ],
   "source": [
    "# filename p19.txt\n",
    "\n",
    "p19pdf_authors=['Tim Althoff*','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang']\n",
    "p19pdf_author_organizations=['Computer Science Department','Stanford University','Google']\n",
    "p19pdf_author_locations=['Stanford, CA','Stanford','CA','Google','1600 Amphitheatre Parkway, Mountain View, CA 94043','1600 Amphitheatre Parkway','Mountain View']\n",
    "\n",
    "p19pdf_references_authors =['A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']\n",
    "\n",
    "p19pdf_allauthors=['Tim Althoff*','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang','A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']\n",
    "\n",
    "print \"There are %r authors\" % len(p19pdf_authors)\n",
    "print  # white space\n",
    "print \"There are %r author organizations\" %len(p19pdf_author_organizations)\n",
    "print \n",
    "print \"There are %r author locations\" % len(p19pdf_author_locations)\n",
    "print  \n",
    "print \"There are %r authors in the references\" %len(p19pdf_references_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 authors\n",
      "\n",
      "There are 6 author organizations\n",
      "\n",
      "There are 8 author locations\n",
      "\n",
      "There are 106 authors in the references\n"
     ]
    }
   ],
   "source": [
    "# filename p29.txt\n",
    "\n",
    "p29pdf_authors=['Laurent Amsaleg','Stéphane Girard','Oussama Chelly','Teddy Furon','Michael E. Houle','Ken-ichi Kawarabayashi',\n",
    "               'Michael Nett']\n",
    "p29pdf_author_organizations=['Equipe LINKMEDIA','Campus Universitaire de Beaulieu','CNRS/IRISA Rennes','National Institute of Informatics',\n",
    "                             'Equipe MISTIS INRIA','Google']\n",
    "p29pdf_author_locations=['Campus Universitaire de Beaulieu','35042 Rennes Cedex, France','France','-1-2 Hitotsubashi, Chiyoda-ku Tokyo 101-8430, Japan',\n",
    "                        'Japan','6-10-1 Roppongi, Minato-ku Tokyo 106-6126','Inovallée, 655, Montbonnot 38334 Saint-Ismier Cedex','Tokyo']\n",
    "\n",
    "p29pdf_references_authors =['A. A. Balkema','L. de Haan','N. Bingham', 'C. Goldie','J. Teugels','N. Boujemaa', 'J. Fauqueur', 'M. Ferecatu', 'F. Fleuret',\n",
    "                            'V. Gouet', 'B. LeSaux','H. Sahbi','C. Bouveyron', 'G. Celeux', 'S. Girard','J. Bruske', 'G. Sommer',\n",
    "                           'F. Camastra','A. Vinciarelli','S. Coles','J. Costa' ,'A. Hero','T. de Vries', 'S. Chawla','M. E. Houle',\n",
    "                           'R. A. Fisher','L. H. C. Tippett','M. I. Fraga Alves', 'L. de Haan','T. Lin','M. I. Fraga Alves', 'M. I. Gomes','L. de Haan',\n",
    "                           'B. V. Gnedenko',' A. Gupta', 'R. Krauthgamer','J. R. Lee','A. Gupta', 'R. Krauthgamer','J. R. Lee','M. Hein','J.-Y. Audibert',\n",
    "                           'B. M. Hill','M. E. Houle','M. E. Houle','M. E. Houle','M. E. Houle', 'H. Kashima', 'M. Nett','M. E. Houle', 'X. Ma', 'M. Nett',\n",
    "                            'V. Oria','M. E. Houle', 'X. Ma', 'V. Oria','J. Sun','M. E. Houle','M. Nett','H. Jegou', 'R. Tavenard', 'M. Douze','L. Amsaleg',\n",
    "                           'I. Jollie','D. R. Karger','M. Ruhl','J. Karhunen','J. Joutsensalo','Y. LeCun', 'L. Bottou', 'Y. Bengio', 'P. Haner',\n",
    "                           'J. Pickands, III','C. R. Rao','S. T. Roweis','L. K. Saul','A. Rozza', 'G. Lombardi', 'C. Ceruti', 'E. Casiraghi', 'P. Campadelli',\n",
    "                           'B. Scholkopf', 'A. J. Smola','K.-R. Muller','U. Shaft','R. Ramakrishnan',' F. Takens','J. Tenenbaum', 'V. D. Silva','J. Langford',\n",
    "                           'J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. Venna','S. Kaski',\n",
    "                           'P. Verveer','R. Duin','J. von Brunken', 'M. E. Houle', 'A. Zimek','J. von Brunken', 'M. E. Houle','A. Zimek']\n",
    "\n",
    "p29pdf_allauthors=['Laurent Amsaleg','Stéphane Girard','Oussama Chelly','Teddy Furon','Michael E. Houle','Ken-ichi Kawarabayashi',\n",
    "               'Michael Nett','A. A. Balkema','L. de Haan','N. Bingham', 'C. Goldie','J. Teugels','N. Boujemaa', 'J. Fauqueur', 'M. Ferecatu', 'F. Fleuret',\n",
    "                            'V. Gouet', 'B. LeSaux','H. Sahbi','C. Bouveyron', 'G. Celeux', 'S. Girard','J. Bruske', 'G. Sommer',\n",
    "                           'F. Camastra','A. Vinciarelli','S. Coles','J. Costa' ,'A. Hero','T. de Vries', 'S. Chawla','M. E. Houle',\n",
    "                           'R. A. Fisher','L. H. C. Tippett','M. I. Fraga Alves', 'L. de Haan','T. Lin','M. I. Fraga Alves', 'M. I. Gomes','L. de Haan',\n",
    "                           'B. V. Gnedenko',' A. Gupta', 'R. Krauthgamer','J. R. Lee','A. Gupta', 'R. Krauthgamer','J. R. Lee','M. Hein','J.-Y. Audibert',\n",
    "                           'B. M. Hill','M. E. Houle','M. E. Houle','M. E. Houle','M. E. Houle', 'H. Kashima', 'M. Nett','M. E. Houle', 'X. Ma', 'M. Nett',\n",
    "                            'V. Oria','M. E. Houle', 'X. Ma', 'V. Oria','J. Sun','M. E. Houle','M. Nett','H. Jegou', 'R. Tavenard', 'M. Douze','L. Amsaleg',\n",
    "                           'I. Jollie','D. R. Karger','M. Ruhl','J. Karhunen','J. Joutsensalo','Y. LeCun', 'L. Bottou', 'Y. Bengio', 'P. Haner',\n",
    "                           'J. Pickands, III','C. R. Rao','S. T. Roweis','L. K. Saul','A. Rozza', 'G. Lombardi', 'C. Ceruti', 'E. Casiraghi', 'P. Campadelli',\n",
    "                           'B. Scholkopf', 'A. J. Smola','K.-R. Muller','U. Shaft','R. Ramakrishnan',' F. Takens','J. Tenenbaum', 'V. D. Silva','J. Langford',\n",
    "                           'J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. Venna','S. Kaski',\n",
    "                           'P. Verveer','R. Duin','J. von Brunken', 'M. E. Houle', 'A. Zimek','J. von Brunken', 'M. E. Houle','A. Zimek']\n",
    "\n",
    "\n",
    "print \"There are %r authors\" % len(p29pdf_authors)\n",
    "print  # white space\n",
    "print \"There are %r author organizations\" %len(p29pdf_author_organizations)\n",
    "print \n",
    "print \"There are %r author locations\" % len(p29pdf_author_locations)\n",
    "print  \n",
    "print \"There are %r authors in the references\" %len(p29pdf_references_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we can programmatically access just about all of the corpus, we are free to hand label as much as we want to do the test.  Our measureable test:\n",
    "\n",
    "* Compare machice extracted list of persons, places, and organizations to hand labeled lists\n",
    "* Compute precision, accuracy and recall\n",
    "* Compare different NERC tool scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the performance of three open source NERC tools.  All of them can be trained to improve performance, but for now we will test \"out of the box\" performance:\n",
    "\n",
    "1.  [NLTK's standard chunker](http://www.nltk.org/api/nltk.chunk.html); read more in [the NLTK book](http://www.nltk.org/book/ch07.html)\n",
    "2. [Standard's Named Entity Recognizer](http://nlp.stanford.edu/software/CRF-NER.shtml), which can be accessed as an API via the NLTK tool\n",
    "3. [Polyglot NER](http://polyglot.readthedocs.org/en/latest/index.html) which is natural language pipeline that supports massive multilingual applications.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin to chunk our data using the benefits of having our texts loaded into NLTK.  We first get the data for our test documents.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need the top and references sections from p19.txt and p29.txt\n",
    "\n",
    "p19top = toppull(\"p19.txt\")\n",
    "p19ref = refpull(\"p19.txt\")\n",
    "\n",
    "p29top=toppull(\"p29.txt\")\n",
    "p29ref=refpull(\"p29.txt\")\n",
    "\n",
    "p19={}\n",
    "p19['top']=p19top['p19.txt']['top']\n",
    "p19['references']=p19ref['p19.txt']['references']\n",
    "\n",
    "\n",
    "p29={}\n",
    "p29['top']=p29top['p29.txt']['top']\n",
    "p29['references']=p29ref['p29.txt']['references']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was easy!  All the munging and wrangling pays off down the road.  Now, we test our first NERC tool, the standard chunker in NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sup id=\"fn1\">1. [(2014). Text Mining and its Business Applications - CodeProject. Retrieved December 26, 2015, from http://www.codeproject.com/Articles/822379/Text-Mining-and-its-Business-Applications.]<a href=\"#ref1\" title=\"Jump back to footnote 1 in the text.\">↩</a></sup>\n",
    "\n",
    "<sup id=\"fn2\">2. [Suchanek, F., & Weikum, G. (2013). Knowledge harvesting in the big-data era. Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data. ACM.]<a href=\"#ref2\" title=\"Jump back to footnote 2 in the text.\">↩</a></sup>\n",
    "\n",
    "\n",
    "<sup id =\"fn3\">3. [Nadeau, D., & Sekine, S. (2007). A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1), 3-26.]<a href=\"#ref3\" title = \"Jump back to footnote 3 in the text\">↩</a></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Parking Lot of links, leftover paragraphs, ideas, etc.</span>\n",
    "\n",
    "Describe the data -> Data available here http://dl.acm.org/citation.cfm?id=2783258# \n",
    "\n",
    "Computer Vision - ECCV 2008 pdf download online free. Retrieved December 31, 2015, from http://pdf12.mono-ebook.org/pdf/computer-vision-eccv-2008_12glgt.pdf.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<href id=\"pipe\"><a href=\"#pipeline\" title=\"Jump back to data science pipeline graphic.\">data science pipeline</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ben's Outline from email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ~~Give a brief introduction to the task, and why it's interesting, important. Then begin to discuss the data set, how you acquired, and where a reader can get access to it.~~ \n",
    "\n",
    "* ~~You then could have a data exploration section where you show the number of documents, perform a word count, show snippets of data (e.g. references) etc that are of interest.~~\n",
    "\n",
    "* ~~You can then go through one or a few of your \"code to get\" sections. These functions all follow basically the same pattern, so you could probably merge them into a single function, that appropriately selects the right regular expression.~~ \n",
    "\n",
    "* ~~The next step is to discuss, demonstrate your \"truth tests\" for text extraction accuracy.~~ \n",
    "\n",
    "* Finally, you can get to an introduction of your three methods for NERC, and show how do do each of them. Then compare (visually) the results of the three according to the evaluation mechanism discussed above. \n",
    "\n",
    "* You could then conclude with a discussion about NLTK chunk vs. hand labelled entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kddcorpus_bigrams=[]\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "for fileid in kddcorpus.fileids():\n",
    "    for l in (BigramCollocationFinder.from_words(kddcorpus.words(fileid)).nbest(bigram_measures.pmi, 10)):\n",
    "        kddcorpus_bigrams.append(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Prototype Holder</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test on normal case extraction\n",
    "failids = []\n",
    "text=kddcorpus.raw('p1055.txt')\n",
    "\n",
    "full = True\n",
    "section = \"abstract\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid)\n",
    "        if section == \"abstract\":\n",
    "            section1=\"abstract\"\n",
    "            target = \"\"   \n",
    "            section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if len(target) > 50:\n",
    "                        \n",
    "                        \n",
    "                        print [fileid,len(target),len(text)]\n",
    "                        break\n",
    "                    else:\n",
    "                        failids.append(fileid)\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    \n",
    "                    pass\n",
    "                              \n",
    "else:\n",
    "    \n",
    "    section = \"abstract\"\n",
    "    text = kddcorpus.raw('p1627.txt').lower()\n",
    "    if section == \"abstract\":\n",
    "        section1=\"abstract\"\n",
    "        target = \"\"   \n",
    "        section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "\n",
    "        part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "        for sect in section2:\n",
    "            try:\n",
    "                part2 = \"(?=\"+str(sect)+\")\"\n",
    "                p=re.compile(part1+part2)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                if target > 50:\n",
    "\n",
    "                    \n",
    "                    break\n",
    "            except:\n",
    "                \n",
    "                pass\n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# completed gold standard for keywords, got all of them..no stragglers\n",
    "text = kddcorpus.raw('p39.txt').lower()\n",
    "failids = []\n",
    "full = True\n",
    "section = \"keywords\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid).lower()\n",
    "        if section == \"keywords\":\n",
    "            section1=\"keywords\"\n",
    "            target = \"\"   \n",
    "            section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"permission to make \",\"1.motivation\" ]\n",
    "        \n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    if len(target) >50:\n",
    "                        if len(target) > 300:\n",
    "                            target = target[:200]\n",
    "                        else:\n",
    "                            target = target\n",
    "                        \n",
    "                        print [fileid,target,len(text)]\n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        failids.append(fileid)\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "else:\n",
    "    section = \"keywords\"\n",
    "    \n",
    "    if section == \"keywords\":\n",
    "        section1=\"keywords\"\n",
    "        target = \"\"   \n",
    "        section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"permission to make \",\"1.motivation\" ]\n",
    "\n",
    "        part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "        for sect in section2:\n",
    "            try:\n",
    "                part2 = \"(?=\"+str(sect)+\")\"\n",
    "                p=re.compile(part1+part2)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                if target > 3:                 \n",
    "                    break                  \n",
    "            except:\n",
    "                pass\n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# completed gold standard for abstract, near zero stragglers\n",
    "failids = []\n",
    "text=kddcorpus.raw('p1055.txt')\n",
    "\n",
    "full = True\n",
    "section = \"abstract\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid).lower()\n",
    "        if section == \"abstract\":\n",
    "            section1=\"abstract\"\n",
    "            target = \"\"   \n",
    "            section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if len(target) > 50:\n",
    "                        \n",
    "                        \n",
    "                        print [fileid,len(target),len(text)]\n",
    "                        break\n",
    "                    else:\n",
    "                        failids.append(fileid)\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    \n",
    "                    pass\n",
    "                              \n",
    "else:\n",
    "    \n",
    "    section = \"abstract\"\n",
    "    text = kddcorpus.raw('p1627.txt').lower()\n",
    "    if section == \"abstract\":\n",
    "        section1=\"abstract\"\n",
    "        target = \"\"   \n",
    "        section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "\n",
    "        part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "        for sect in section2:\n",
    "            try:\n",
    "                part2 = \"(?=\"+str(sect)+\")\"\n",
    "                p=re.compile(part1+part2)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                if target > 50:\n",
    "\n",
    "                    \n",
    "                    break\n",
    "            except:\n",
    "                \n",
    "                pass\n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# completed gold standard for references, counts number of references and does \"word per reference\" score\n",
    "\n",
    "failids = []\n",
    "text=kddcorpus.raw('p29.txt').lower()\n",
    "\n",
    "full = False\n",
    "section = \"references\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid).lower()\n",
    "        if section == \"references\":\n",
    "            section1=\"references \\[\" \n",
    "            target = \"\"   \n",
    "            \n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "            \n",
    "            for sect in section1:\n",
    "                try:\n",
    "                    p=re.compile(part1)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "\n",
    "                    if len(target) > 50:\n",
    "                        \n",
    "                        # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                        try:\n",
    "                            if 'references' in locals():\n",
    "\n",
    "                                refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                        except:\n",
    "                            print \"This file does not appear to have a references section\"\n",
    "                            pass\n",
    "                        print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                        break\n",
    "                    else:\n",
    "                        \n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "                              \n",
    "else:\n",
    "    \n",
    "    section = \"references\"\n",
    "    \n",
    "    if section == \"references\":\n",
    "        section1=\"references \\[\"\n",
    "        target = \"\"   \n",
    "        \n",
    "        part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "        for sect in section1:\n",
    "            try:\n",
    "                \n",
    "                p=re.compile(part1)\n",
    "                target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                if target > 50:\n",
    "                    print len(target)\n",
    "\n",
    "                    \n",
    "                break\n",
    "            except:\n",
    "                \n",
    "                pass\n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(failids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gold standard to get top section\n",
    "\n",
    "from emailextractor import file_to_str, get_emails # paste code to .py file from following link and save within your environment path to call it: https://gist.github.com/dideler/5219706\n",
    "\n",
    "failids = []\n",
    "text=kddcorpus.raw('p1623.txt').lower()\n",
    "\n",
    "full = True\n",
    "section = \"top\"\n",
    "if full == True:\n",
    "    for fileid in kddcorpus.fileids():\n",
    "        text = kddcorpus.raw(fileid).lower()\n",
    "        if section == \"top\":\n",
    "            section1=\"\"\n",
    "            section2=[\"abstract\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if len(target)> 1000:\n",
    "                        if len(target) > 3000 and float(len(target))/float(len(text)) > .22:\n",
    "                            target = target[:2500]\n",
    "                        else:\n",
    "                            target=target\n",
    "                        emails = tuple(get_emails(target))\n",
    "                        print [fileid,len(target),len(text),emails]\n",
    "                        break\n",
    "                    else:\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "    print \"Done\"\n",
    "                              \n",
    "else:\n",
    "    \n",
    "    if section == \"top\":\n",
    "            section1=\"\"\n",
    "            section2=[\"abstract\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if len(target)> 1000:\n",
    "                        if len(target) > 3000 and float(len(target))/float(len(text)) > .22:\n",
    "                            target = target[:2500]\n",
    "                        else:\n",
    "                            target=target\n",
    "                        emails = tuple(get_emails(target))\n",
    "                        print [fileid,len(target),len(text),emails]\n",
    "                        break\n",
    "                     \n",
    "                    else:\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "                \n",
    "print target.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "failids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:violet\">Drawing Board/Assembly Line</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attempting function with gold keywords....WORKS\n",
    "\n",
    "def keypull(docnum=None,section='keywords',full = False):\n",
    "    \n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "\n",
    "        \n",
    "\n",
    "        # to return output from entire corpus\n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid).lower()\n",
    "                if section == \"keywords\":\n",
    "                    section1=\"keywords\"\n",
    "                    target = \"\"   \n",
    "                    section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"(1. tutorial )\",\" permission to make \",\"  permission to make\",\"(  permission to make digital )\",\"    bio  \",\"abstract:  \",\"1.motivation\" ]\n",
    "\n",
    "                    part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "                    for sect in section2:\n",
    "                        try:\n",
    "                            part2 = \"(?=\"+str(sect)+\")\"\n",
    "                            p=re.compile(part1+part2)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                            if len(target) >50:\n",
    "                                if len(target) > 300:\n",
    "                                    target = target[:200]\n",
    "                                else:\n",
    "                                    target = target\n",
    "\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"keywords\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                #print [fileid,len(target),len(text)]\n",
    "                                break\n",
    "                            else:\n",
    "                                if len(target)==0:\n",
    "                                     failids.append(fileid)   \n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "            set(failids)\n",
    "            return ans\n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        if full == False:\n",
    "            if section == \"keywords\":\n",
    "                section1=\"keywords\"\n",
    "                target = \"\"   \n",
    "                section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"permission to make \",\"1.motivation\" ]\n",
    "\n",
    "                part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "                for sect in section2:\n",
    "                    try:\n",
    "                        part2 = \"(?=\"+str(sect)+\")\"\n",
    "                        p=re.compile(part1+part2)\n",
    "                        target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                        if len(target) >50:\n",
    "                            if len(target) > 300:\n",
    "                                target = target[:200]\n",
    "                            else:\n",
    "                                target = target\n",
    "                            ans[docnum]={}\n",
    "                            ans[docnum][\"keywords\"]=target.strip()\n",
    "                            ans[docnum][\"charcount\"]=len(target)\n",
    "                            break                  \n",
    "                    except:\n",
    "                        pass\n",
    "    return ans\n",
    "    return failids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attempting function with gold abstracts...WORKS\n",
    "\n",
    "def abpull(docnum=None,section='abstract',full = False):\n",
    "    \n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        # to return output from entire corpus\n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid).lower()\n",
    "                if section == \"abstract\":\n",
    "                    section1=\"abstract\"\n",
    "                    target = \"\"   \n",
    "                    section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "                    part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "                    for sect in section2:\n",
    "                        try:\n",
    "                            part2 = \"(?=\"+str(sect)+\")\"\n",
    "                            p=re.compile(part1+part2)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "\n",
    "                            if len(target) > 50:\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"abstract\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                \n",
    "                                #print [fileid,len(target),len(text)]\n",
    "                                break\n",
    "                            else:\n",
    "                                failids.append(fileid)\n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            pass\n",
    "            return ans\n",
    "                              \n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum).lower()\n",
    "        if section == \"abstract\":\n",
    "            section1=\"abstract\"\n",
    "            target = \"\"   \n",
    "            section2=[\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"keywords\",\"introduction  1.\",\"introduction\", \"\\\\\\\\n\"]\n",
    "\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if target > 50:\n",
    "                        ans[str(docnum)]={}\n",
    "                        ans[str(docnum)][\"abstract\"]=target.strip()\n",
    "                        ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "        return ans\n",
    "        return failids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attempting function with gold top section...WORKS\n",
    "\n",
    "from emailextractor import file_to_str, get_emails # paste code to .py file from following link and save within your environment path to call it: https://gist.github.com/dideler/5219706\n",
    "def toppull(docnum=None,section='top',full = False):\n",
    "    \n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        # to return output from entire corpus\n",
    "        \n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid).lower()\n",
    "                if section == \"top\":\n",
    "                    section1=\"\"\n",
    "                    section2=[\"abstract\"]\n",
    "                    part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "                    for sect in section2:\n",
    "                        try:\n",
    "                            part2 = \"(?=\"+str(sect)+\")\"\n",
    "                            p=re.compile(part1+part2)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "\n",
    "                            if len(target)> 1000:\n",
    "                                if len(target) > 3000 and float(len(target))/float(len(text)) > .22:\n",
    "                                    target = target[:2500]\n",
    "                                else:\n",
    "                                    target=target\n",
    "                                emails = tuple(get_emails(target))\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"top\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                ans[str(fileid)][\"emails\"]=emails\n",
    "                                \n",
    "                                #print [fileid,len(target),len(text)]\n",
    "                                break\n",
    "                            else:\n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "            return ans\n",
    "            return failids\n",
    "        \n",
    "        \n",
    "        \n",
    "                              \n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum).lower()\n",
    "        \n",
    "        if section == \"top\":\n",
    "            section1=\"\"\n",
    "            section2=[\"abstract\"]\n",
    "            part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "            for sect in section2:\n",
    "                try:\n",
    "                    part2 = \"(?=\"+str(sect)+\")\"\n",
    "                    p=re.compile(part1+part2)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    \n",
    "                    if len(target)> 1000:\n",
    "                        if len(target) > 3000 and float(len(target))/float(len(text)) > .22:\n",
    "                            target = target[:2500]\n",
    "                        else:\n",
    "                            target=target\n",
    "                        emails = tuple(get_emails(target))\n",
    "                        ans[str(docnum)]={}\n",
    "                        ans[str(docnum)][\"top\"]=target.strip()\n",
    "                        ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                        ans[str(docnum)][\"emails\"]=emails\n",
    "                        #print [fileid,len(target),len(text)]\n",
    "                        break\n",
    "                     \n",
    "                    else:\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "                \n",
    "        return ans\n",
    "        return failids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attempting function with gold references section\n",
    "\n",
    "def refpull(docnum=None,section='references',full = False):\n",
    "    \n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        # to return output from entire corpus\n",
    "        \n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid).lower()\n",
    "                if section == \"references\":\n",
    "                    section1=[\"references \\[\",\"references\"] \n",
    "                    target = \"\"   \n",
    "\n",
    "                    part1= \"(?<=\"+str(section1)+\")\"\n",
    "\n",
    "                    for sect in section1:\n",
    "                        try:\n",
    "                            part1= \"(?<=\"+str(sect)+\")\"\n",
    "                            p=re.compile(part1)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "\n",
    "                            if len(target) > 50:\n",
    "\n",
    "                                # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                                try:\n",
    "                                    refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                                except:\n",
    "                                    print \"This file does not appear to have a references section\"\n",
    "                                    pass\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"references\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                ans[str(fileid)][\"refcount\"]= refnum\n",
    "                                ans[str(fileid)][\"wordperRef\"]=round(float(len(nltk.word_tokenize(text)))/float(refnum))\n",
    "                                \n",
    "                                #print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                                break\n",
    "                            else:\n",
    "\n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "        \n",
    "            return ans\n",
    "            return failids\n",
    "                              \n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum).lower()\n",
    "        if section == \"references\":\n",
    "                    section1=\"references \\[\" \n",
    "                    target = \"\"   \n",
    "\n",
    "                    \n",
    "\n",
    "                    for sect in section1:\n",
    "                        try:\n",
    "                            part1= \"(?<=\"+str(sect)+\")(.+)\"\n",
    "                            p=re.compile(part1)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "\n",
    "                            if len(target) > 50:\n",
    "\n",
    "                                # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                                try:\n",
    "                                    refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                                except:\n",
    "                                    print \"This file does not appear to have a references section\"\n",
    "                                    pass\n",
    "                                ans[str(docnum)]={}\n",
    "                                ans[str(docnum)][\"references\"]=target.strip()\n",
    "                                ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                                ans[str(docnum)][\"refcount\"]= refnum\n",
    "                                ans[str(docnum)][\"wordperRef\"]=float(len(nltk.word_tokenize(text)))/float(refnum)\n",
    "                                \n",
    "                                #print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                                break\n",
    "                            else:\n",
    "\n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(docnum)\n",
    "                            pass\n",
    "        \n",
    "        return ans\n",
    "        return failids\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Testing Station</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = toppull(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test= refpull('p29.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(test.keys())\n",
    "print len(set(failids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print set(failids)\n",
    "print\n",
    "print\n",
    "print set(failids) & set(keywords.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?<=keywords)(.+)(?=introduction  )\n"
     ]
    }
   ],
   "source": [
    "print part1+part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key,value in test.iteritems():\n",
    "    print key,value['emails']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "No such file or directory: '/Users/linwood/Desktop/KDD_corpus/f1035.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-262-513a58dcb453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkddcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f1035.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'([\\d]\\.)+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[\\s]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/corpus/reader/plaintext.pyc\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/corpus/reader/api.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \"\"\"\n\u001b[1;32m    209\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, fileid)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/compat.pyc\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linwood/anaconda/envs/py27/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, _path)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No such file or directory: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: No such file or directory: '/Users/linwood/Desktop/KDD_corpus/f1035.txt'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = kddcorpus.raw('f1035.txt')\n",
    "p=re.compile('([\\d]\\.)+')\n",
    "refs = p.search(re.sub('[\\s]',\" \",text)).group()\n",
    "print refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#d = re.findall('[^ ](AB.T{1}.*)[^ ]',kddcorpus.raw(\"p1035.txt\"))\n",
    "for fileid in kddcorpus.fileids():\n",
    "    \n",
    "    d = re.findall('[\\s](AB.T{1}.*)[\\s]',kddcorpus.raw(fileid))\n",
    "    if len(d) > 0:\n",
    "        print fileid,len(d), d\n",
    "        \n",
    "    else:\n",
    "        print fileid,\"Failed\",d\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'ABSTRACT']\n"
     ]
    }
   ],
   "source": [
    "print re.findall('[\\s](AB.T{1}.*)[\\s].*',kddcorpus.raw(\"p1909.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Estimating Local Intrinsic Dimensionality\\n\\nLaurent Amsaleg\\nEquipe LINKMEDIA,\\n\\nCNRS/IRISA Rennes, France\\n\\nCampus Universitaire de\\n\\nBeaulieu\\n\\n35042 Rennes Cedex, France\\nlaurent.amsaleg@irisa.fr\\n\\nStphane Girard\\nEquipe MISTIS, INRIA\\n\\nGrenoble, France\\n\\nInovalle, 655, Montbonnot\\n38334 Saint-Ismier Cedex,\\nstephane.girard@inria.fr\\n\\nFrance\\n\\nTeddy Furon\\n\\nEquipe LINKMEDIA,\\n\\nINRIA/IRISA Rennes, France\\n\\nCampus Universitaire de\\n\\nBeaulieu\\n\\n35042 Rennes Cedex, France\\n\\nteddy.furon@inria.fr\\n\\nKen-ichi Kawarabayashi\\n\\nNational Institute of\\nInformatics, Japan\\n2-1-2 Hitotsubashi,\\n\\nChiyoda-ku\\n\\nTokyo 101-8430, Japan\\nk_keniti@nii.ac.jp\\n\\nOussama Chelly\\nNational Institute of\\nInformatics, Japan\\n2-1-2 Hitotsubashi,\\n\\nChiyoda-ku\\n\\nTokyo 101-8430, Japan\\n\\nchelly@nii.ac.jp\\nMichael E. Houle\\nNational Institute of\\nInformatics, Japan\\n2-1-2 Hitotsubashi,\\n\\nChiyoda-ku\\n\\nTokyo 101-8430, Japan\\n\\nmeh@nii.ac.jp\\nMichael Nett\\nGoogle, Japan\\n\\n6-10-1 Roppongi, Minato-ku\\n\\nTokyo 106-6126, Japan\\nmnett@google.com\\n\\nABSTRACT\\nThis paper is concerned with the estimation of a local mea-\\nsure of intrinsic dimensionality (ID) recently proposed by\\nHoule. The local model can be regarded as an extension of\\nKarger and Ruhls expansion dimension to a statistical set-\\nting in which the distribution of distances to a query point\\nis modeled in terms of a continuous random variable. This\\nform of intrinsic dimensionality can be particularly useful in\\nsearch, classication, outlier detection, and other contexts in\\nmachine learning, databases, and data mining, as it has been\\nshown to be equivalent to a measure of the discriminative\\npower of similarity functions. Several estimators of local ID\\nare proposed and analyzed based on extreme value theory,\\nusing maximum likelihood estimation (MLE), the method\\nof moments (MoM), probability weighted moments (PWM),\\nand regularly varying functions (RV). An experimental eval-\\nuation is also provided, using both real and articial data.\\n\\nCategories and Subject Descriptors\\nG.3 [Mathematics of Computing]: Prob'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['itting at starbucks.  I wonder what we all are doing in this god-forsaken place.  Can you under']\n"
     ]
    }
   ],
   "source": [
    "print re.findall('^S(.*)s', \"Sitting at starbucks.  I wonder what we all are doing in this god-forsaken place.  Can you understand what I'm doing?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?<=Abstract)(.+)(?=Categories and Subject Descriptors)\n",
      "(?<=Abstract)(.+)(?=Categories & Subject Descriptors)\n",
      "(?<=Abstract)(.+)(?=CATEGORIES AND SUBJECT DESCRIPTORS)\n",
      "(?<=Abstract)(.+)(?=CATEGORIES & SUBJECT DESCRIPTORS)\n",
      "(?<=Abstract)(.+)(?=categories and subject descriptors)\n",
      "(?<=Abstract)(.+)(?=categories & subject descriptors)\n",
      "(?<=Abstract)(.+)(?=permission to make)\n",
      "(?<=Abstract)(.+)(?=Keywords)\n",
      "(?<=Abstract)(.+)(?=KEYWORDS)\n",
      "(?<=Abstract)(.+)(?=keywords)\n",
      "(?<=Abstract)(.+)(?=Introduction  1.)\n",
      "(?<=Abstract)(.+)(?=INTRODUCTION  1.)\n",
      "(?<=Abstract)(.+)(?=introduction  1.)\n",
      "(?<=Abstract)(.+)(?=Introduction)\n",
      "(?<=Abstract)(.+)(?=INTRODUCTION)\n",
      "(?<=Abstract)(.+)(?=introduction)\n",
      "(?<=Abstract)(.+)(?=\\\\n)\n",
      "(?<=ABSTRACT)(.+)(?=Categories and Subject Descriptors)\n",
      "(?<=ABSTRACT)(.+)(?=Categories & Subject Descriptors)\n",
      "(?<=ABSTRACT)(.+)(?=CATEGORIES AND SUBJECT DESCRIPTORS)\n",
      "(?<=ABSTRACT)(.+)(?=CATEGORIES & SUBJECT DESCRIPTORS)\n",
      "(?<=ABSTRACT)(.+)(?=categories and subject descriptors)\n",
      "(?<=ABSTRACT)(.+)(?=categories & subject descriptors)\n",
      "(?<=ABSTRACT)(.+)(?=permission to make)\n",
      "(?<=ABSTRACT)(.+)(?=Keywords)\n",
      "(?<=ABSTRACT)(.+)(?=KEYWORDS)\n",
      "(?<=ABSTRACT)(.+)(?=keywords)\n",
      "(?<=ABSTRACT)(.+)(?=Introduction  1.)\n",
      "(?<=ABSTRACT)(.+)(?=INTRODUCTION  1.)\n",
      "(?<=ABSTRACT)(.+)(?=introduction  1.)\n",
      "(?<=ABSTRACT)(.+)(?=Introduction)\n",
      "(?<=ABSTRACT)(.+)(?=INTRODUCTION)\n",
      "(?<=ABSTRACT)(.+)(?=introduction)\n",
      "(?<=ABSTRACT)(.+)(?=\\\\n)\n",
      "(?<=abstract)(.+)(?=Categories and Subject Descriptors)\n",
      "(?<=abstract)(.+)(?=Categories & Subject Descriptors)\n",
      "(?<=abstract)(.+)(?=CATEGORIES AND SUBJECT DESCRIPTORS)\n",
      "(?<=abstract)(.+)(?=CATEGORIES & SUBJECT DESCRIPTORS)\n",
      "(?<=abstract)(.+)(?=categories and subject descriptors)\n",
      "(?<=abstract)(.+)(?=categories & subject descriptors)\n",
      "(?<=abstract)(.+)(?=permission to make)\n",
      "(?<=abstract)(.+)(?=Keywords)\n",
      "(?<=abstract)(.+)(?=KEYWORDS)\n",
      "(?<=abstract)(.+)(?=keywords)\n",
      "(?<=abstract)(.+)(?=Introduction  1.)\n",
      "(?<=abstract)(.+)(?=INTRODUCTION  1.)\n",
      "(?<=abstract)(.+)(?=introduction  1.)\n",
      "(?<=abstract)(.+)(?=Introduction)\n",
      "(?<=abstract)(.+)(?=INTRODUCTION)\n",
      "(?<=abstract)(.+)(?=introduction)\n",
      "(?<=abstract)(.+)(?=\\\\n)\n"
     ]
    }
   ],
   "source": [
    "section1=[\"Abstract\",\"ABSTRACT\",\"abstract\"]\n",
    "target = \"\"   \n",
    "section2=[\"Categories and Subject Descriptors\",\"Categories & Subject Descriptors\",\"Categories and Subject Descriptors\".upper(),\"Categories & Subject Descriptors\".upper(),\"categories and subject descriptors\",\"categories & subject descriptors\",\"permission to make\",\"Keywords\",\"keywords\".upper(),\"keywords\",\"introduction  1.\".title(),\"introduction  1.\".upper(),\"introduction  1.\",\"introduction\".title(),\"introduction\".upper(),\"introduction\", \"\\\\\\\\n\"]\n",
    "for sect1 in section1:\n",
    "    for sect2 in section2:\n",
    "        part1= \"(?<=\"+str(sect1)+\")(.+)\"\n",
    "        part2 = \"(?=\"+str(sect2)+\")\"\n",
    "        print part1+part2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part1= \"(?<=\"+str(section1)+\")(.+)\"'(?<=references)(.+)'\n",
    "\n",
    "                    for sect in section1:\n",
    "                        try:\n",
    "                            p=re.compile(part1)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Why It Happened: Identifying and Modeling the Reasons of\\n\\nthe Happening of Social Events\\n\\nYu Rong\\n\\nHong Kong\\n\\nThe Chinese University of\\n\\nThe Chinese University of\\n\\nThe Chinese University of\\n\\nyrong@se.cuhk.edu.hk\\n\\nhcheng@se.cuhk.edu.hk\\n\\nzymo@se.cuhk.edu.hk\\n\\nHong Cheng\\n\\nHong Kong\\n\\nZhiyu Mo\\n\\nHong Kong\\n\\nABSTRACT\\nIn nowadays social networks, a huge volume of content con-\\ntaining rich information, such as reviews, ratings, microblogs,\\netc., is being generated, consumed and diused by users all\\nthe time. Given the temporal information, we can obtain\\nthe event cascade which indicates the time sequence of the\\narrival of information to users. Many models have been pro-\\nposed to explain how information '"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kddcorpus.raw(\"p1015.txt\")[:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " p1.txt Online Controlled Experiments:\n",
      "p1005.txt Mining Frequent Itemsets through Progressive Sampling\n",
      "p1015.txt Why It Happened: Identifying and Modeling the Reasons of\n",
      "p1025.txt Matrix Completion with Queries\n",
      "p1035.txt Stochastic Divergence Minimization\n",
      "p1045.txt Bayesian Poisson Tensor Factorization for Inferring\n",
      "p1055.txt TimeCrunch: Interpretable Dynamic Graph Summarization\n",
      "p1065.txt Inside Jokes: Identifying Humorous Cartoon Captions\n",
      "p1075.txt Community Detection based on Distance Dynamics\n",
      "p1085.txt Discovery of Meaningful Rules in Time Series\n",
      "p109.txt On the Formation of Circles in Co-authorship Networks\n",
      "p1095.txt An Evaluation of Parallel Eccentricity Estimation\n",
      "p1105.txt Efcient Latent Link Recommendation in\n",
      "p1115.txt Turn Waste into Wealth: On Simultaneous Clustering and\n",
      "p1125.txt Set Cover at Web Scale\n",
      "p1135.txt Exploiting Relevance Feedback in Knowledge Graph\n",
      "p1145.txt LINKAGE: An Approach for Comprehensive Risk\n",
      "p1155.txt Transitive Transfer Learning\n",
      "p1165.txt PTE: Predictive Text Embedding through Large-scale\n",
      "p1175.txt An Effective Marketing Strategy for Revenue Maximization\n",
      "p1185.txt Scaling Up Stochastic Dual Coordinate Ascent\n",
      "p119.txt Heterogeneous Network Embedding via Deep\n",
      "p1195.txt Discovering Valuable Items from Massive Data\n",
      "p1205.txt Deep Learning Architecture with Dynamically Programmed\n",
      "p1215.txt Incorporating World Knowledge to Document Clustering\n"
     ]
    }
   ],
   "source": [
    "# code uses regular expression to extract text up to the first new line character\n",
    "\n",
    "p=re.compile('^(.*)[\\s]+[a-z]* *[\\s]')\n",
    "for fileid in kddcorpus.fileids()[:25]:\n",
    "    print fileid, p.search(kddcorpus.raw(fileid)).group(1).strip()  # use .strip() to remove whitespace from beginning and end of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Online Controlled Experiments: ']\n",
      "[u'Mining Frequent Itemsets through Progressive Sampling']\n",
      "[u'Why It Happened: Identifying and Modeling the Reasons of']\n",
      "[u'Matrix Completion with Queries']\n",
      "[u'Stochastic Divergence Minimization']\n",
      "[u'Bayesian Poisson Tensor Factorization for Inferring']\n",
      "[u'TimeCrunch: Interpretable Dynamic Graph Summarization']\n",
      "[u'Inside Jokes: Identifying Humorous Cartoon Captions']\n",
      "[u'Community Detection based on Distance Dynamics']\n",
      "[u' Discovery of Meaningful Rules in Time Series']\n",
      "[u'On the Formation of Circles in Co-authorship Networks']\n",
      "[u'An Evaluation of Parallel Eccentricity Estimation']\n",
      "[u'Efcient Latent Link Recommendation in']\n",
      "[u'Turn Waste into Wealth: On Simultaneous Clustering and']\n",
      "[u'Set Cover at Web Scale']\n",
      "[u'Exploiting Relevance Feedback in Knowledge Graph']\n",
      "[u'LINKAGE: An Approach for Comprehensive Risk']\n",
      "[u'Transitive Transfer Learning']\n",
      "[u'PTE: Predictive Text Embedding through Large-scale']\n",
      "[u'An Effective Marketing Strategy for Revenue Maximization']\n",
      "[u'Scaling Up Stochastic Dual Coordinate Ascent']\n",
      "[u'Heterogeneous Network Embedding via Deep']\n",
      "[u'Discovering Valuable Items from Massive Data']\n",
      "[u'Deep Learning Architecture with Dynamically Programmed']\n",
      "[u'Incorporating World Knowledge to Document Clustering']\n"
     ]
    }
   ],
   "source": [
    "for fileid in kddcorpus.fileids()[:25]:\n",
    "    print re.findall('^(.*)[\\s]{2,}[A-Z]*',kddcorpus.raw(fileid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
